{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HyperQueue lets you build a computation plan consisting of a large amount of tasks and then execute it transparently over a system like SLURM/PBS. It dynamically groups jobs into SLURM/PBS jobs and distributes them to fully utilize allocated notes. You thus do not have to manually aggregate your tasks into SLURM/PBS jobs. Project repository: https://github.com/It4innovations/hyperqueue Submiting a simple task # Start server (e.g. on a login node or in a cluster partition) $ hq server start & Submit a job (command echo 'Hello world' in this case) $ hq submit echo 'Hello world' Ask for computing resource Start worker manually $ hq worker start & Automatic resource request [Not implemented yet] Manual request in PBS $ qsub <your-params-of-qsub> -- hq worker start Manual request in SLURM sbatch <your-params-of-sbatch> -- hq worker start Monitor the state of jobs $ hq jobs","title":"Overview"},{"location":"#submiting-a-simple-task","text":"Start server (e.g. on a login node or in a cluster partition) $ hq server start & Submit a job (command echo 'Hello world' in this case) $ hq submit echo 'Hello world' Ask for computing resource Start worker manually $ hq worker start & Automatic resource request [Not implemented yet] Manual request in PBS $ qsub <your-params-of-qsub> -- hq worker start Manual request in SLURM sbatch <your-params-of-sbatch> -- hq worker start Monitor the state of jobs $ hq jobs","title":"Submiting a simple task"},{"location":"arrays/","text":"Task arrays # [Added in 0.2] Task arrays is a mechanism for submitting many tasks at once. It allows to create a job with many tasks in a single submit and monitor and manage them as a single group. Note: From the user perspective, it is functionally equivalent for \"job arrays\" in SLURM/PBS; however, HQ does not use SLURM/PBS job arrays for providing this feature. HyperQueue's task arrays are handled as any other tasks, e.g. it may happen that two tasks from one array may run simultaneously in the same worker if there are enough resources. Submitting task array # The following submits 100 tasks in a single job with task ids 1-100: $ hq submit --array 1-100 <program> <args1> ... Generally, task ids may be specified with the following syntax (X and Y are unsigned integers): X-Y - Include range from X to Y X - An array with a single element X Env variables # When a task is started then the following environment variables are created: HQ_JOB_ID - Job id HQ_TASK_ID - Task id Tasks states # Each task has its own individual state as defined in the previous chapter. The number of tasks with each state can be seen by: $ hq job <job_id> State of each task can be seen: $ hq job <job_id> --tasks A global job state for summary outputs is derived as following (when a rule is matched, the rest is ignored): If at least one task is in state \"Running\" then job state is \"Running\". If at least one task is in state \"Canceled\" then job state is \"Canceled\". If at least one task is in state \"Failed\" then job state is \"Failed\". If at least all tasks are in state \"Finished\" then job state is \"Finished\". Otherwise the job state is \"Waiting\". Task fail in array job # By default, when a task fails the computation of job continues. TODO - fail policy Job canceling # When a job with more tasks is canceled then all non-finished tasks is canceled.","title":"Task Arrays"},{"location":"arrays/#task-arrays","text":"[Added in 0.2] Task arrays is a mechanism for submitting many tasks at once. It allows to create a job with many tasks in a single submit and monitor and manage them as a single group. Note: From the user perspective, it is functionally equivalent for \"job arrays\" in SLURM/PBS; however, HQ does not use SLURM/PBS job arrays for providing this feature. HyperQueue's task arrays are handled as any other tasks, e.g. it may happen that two tasks from one array may run simultaneously in the same worker if there are enough resources.","title":"Task arrays"},{"location":"arrays/#submitting-task-array","text":"The following submits 100 tasks in a single job with task ids 1-100: $ hq submit --array 1-100 <program> <args1> ... Generally, task ids may be specified with the following syntax (X and Y are unsigned integers): X-Y - Include range from X to Y X - An array with a single element X","title":"Submitting task array"},{"location":"arrays/#env-variables","text":"When a task is started then the following environment variables are created: HQ_JOB_ID - Job id HQ_TASK_ID - Task id","title":"Env variables"},{"location":"arrays/#tasks-states","text":"Each task has its own individual state as defined in the previous chapter. The number of tasks with each state can be seen by: $ hq job <job_id> State of each task can be seen: $ hq job <job_id> --tasks A global job state for summary outputs is derived as following (when a rule is matched, the rest is ignored): If at least one task is in state \"Running\" then job state is \"Running\". If at least one task is in state \"Canceled\" then job state is \"Canceled\". If at least one task is in state \"Failed\" then job state is \"Failed\". If at least all tasks are in state \"Finished\" then job state is \"Finished\". Otherwise the job state is \"Waiting\".","title":"Tasks states"},{"location":"arrays/#task-fail-in-array-job","text":"By default, when a task fails the computation of job continues. TODO - fail policy","title":"Task fail in array job"},{"location":"arrays/#job-canceling","text":"When a job with more tasks is canceled then all non-finished tasks is canceled.","title":"Job canceling"},{"location":"cheatsheet/","text":"Cheatsheet #","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"cpus/","text":"CPU management # [Added in 0.2] Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU. Requesting more CPUs # By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus. Requesting all CPUs # Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...> Pinning # By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically. Automatic pinning # If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process. Manual pinning # If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> NUMA allocation policy # HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\" CPU requests and job arrays # Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU management"},{"location":"cpus/#cpu-management","text":"[Added in 0.2] Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU.","title":"CPU management"},{"location":"cpus/#requesting-more-cpus","text":"By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus.","title":"Requesting more CPUs"},{"location":"cpus/#requesting-all-cpus","text":"Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...>","title":"Requesting all CPUs"},{"location":"cpus/#pinning","text":"By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically.","title":"Pinning"},{"location":"cpus/#automatic-pinning","text":"If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process.","title":"Automatic pinning"},{"location":"cpus/#manual-pinning","text":"If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...>","title":"Manual pinning"},{"location":"cpus/#numa-allocation-policy","text":"HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\"","title":"NUMA allocation policy"},{"location":"cpus/#cpu-requests-and-job-arrays","text":"Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU requests and job arrays"},{"location":"deployment/","text":"Deployment # This section describes, how to HyperQueue server Starting server # Server may run on any computer as long as computing nodes are able to connect to these machine. It is not necessary to be able to connect from server to computing nodes. In the most simple scenario, we expect that the user starts its own instance of HyperQueue directly on login of a HPC system. The server can be simply started by the following command: hq server start Note: The server opens two TCP/IP ports: one for submitting jobs and one for connecting workers. By default, these ports are automatically assigned by the operation system. A user does not remmber them, they are stored in the \"server directory\". Other components automatically reads these settings. Server directory # When a HQ server is started, it creates a server directory where it stores informations needed for submiting jobs and connecting workers. Important: Encryption keys are stored in the server directory. Who has access to server directory may submit jobs, connect workers to HyperQueue instance, and decrypt communication between HyperQueue components. By default, server directory is stored in $HOME/.hq-server . It may be changed via option --server-dir=<PATH> . In such case, all commands need to use the --server-dir settings. You can run more instances of HyperQueue under the same user. All you need is to set a different server directories for each instance. Stopping server # A server can be stopped by command: hq server stop Starting worker # A worker can be started by command. It reads server directory and connectes to the server. hq worker start Starting worker in PBS # qsub <qsub-settings> -- hq worker start Starting worker in SLURM # sbatch <qsub-settings> -- hq worker start List of workers # hq worker list Stopping worker # hq worker stop <id> CPUs configuration # Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout. Manual specification of CPU configration # If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12","title":"Deployment"},{"location":"deployment/#deployment","text":"This section describes, how to HyperQueue server","title":"Deployment"},{"location":"deployment/#starting-server","text":"Server may run on any computer as long as computing nodes are able to connect to these machine. It is not necessary to be able to connect from server to computing nodes. In the most simple scenario, we expect that the user starts its own instance of HyperQueue directly on login of a HPC system. The server can be simply started by the following command: hq server start Note: The server opens two TCP/IP ports: one for submitting jobs and one for connecting workers. By default, these ports are automatically assigned by the operation system. A user does not remmber them, they are stored in the \"server directory\". Other components automatically reads these settings.","title":"Starting server"},{"location":"deployment/#server-directory","text":"When a HQ server is started, it creates a server directory where it stores informations needed for submiting jobs and connecting workers. Important: Encryption keys are stored in the server directory. Who has access to server directory may submit jobs, connect workers to HyperQueue instance, and decrypt communication between HyperQueue components. By default, server directory is stored in $HOME/.hq-server . It may be changed via option --server-dir=<PATH> . In such case, all commands need to use the --server-dir settings. You can run more instances of HyperQueue under the same user. All you need is to set a different server directories for each instance.","title":"Server directory"},{"location":"deployment/#stopping-server","text":"A server can be stopped by command: hq server stop","title":"Stopping server"},{"location":"deployment/#starting-worker","text":"A worker can be started by command. It reads server directory and connectes to the server. hq worker start","title":"Starting worker"},{"location":"deployment/#starting-worker-in-pbs","text":"qsub <qsub-settings> -- hq worker start","title":"Starting worker in PBS"},{"location":"deployment/#starting-worker-in-slurm","text":"sbatch <qsub-settings> -- hq worker start","title":"Starting worker in SLURM"},{"location":"deployment/#list-of-workers","text":"hq worker list","title":"List of workers"},{"location":"deployment/#stopping-worker","text":"hq worker stop <id>","title":"Stopping worker"},{"location":"deployment/#cpus-configuration","text":"Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout.","title":"CPUs configuration"},{"location":"deployment/#manual-specification-of-cpu-configration","text":"If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12","title":"Manual specification of CPU configration"},{"location":"install/","text":"Installation # Binary distribution # Download latest binary distribution from https://github.com/It4innovations/hyperqueue/releases/latest Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-x64.tar.gz Compilation from source codes # Requirements: Git, Rust Clone HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build project: $ cargo build --release Final executable file will in ./target/release/hq","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#binary-distribution","text":"Download latest binary distribution from https://github.com/It4innovations/hyperqueue/releases/latest Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-x64.tar.gz","title":"Binary distribution"},{"location":"install/#compilation-from-source-codes","text":"Requirements: Git, Rust Clone HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build project: $ cargo build --release Final executable file will in ./target/release/hq","title":"Compilation from source codes"},{"location":"jobs/","text":"Jobs # A job is a portion of work that can be submited into a HyperQueue. Each job may have one or tasks . In the current version, a is a single execution of a program. In this section, we are introducing simple jobs, where each job has exactly one tasks. See section about Job arrays for submiting more tasks in one job. Identification numbers # Each job is identified by a positive integer that is assigned by HyperQueue server when the job is submitted, we refer to it as job id . Each task is identified by an unsigned 32b integer called task id . Task id can be assigned by a user and same task id may be used in two different jobs. In simple jobs, task id is set to 0. Submiting jobs # hq submit <program> <args1> ... HyperQueue assigns a unique job id when a job is submitted. When your command contains its own switches, you need to use -- after submit: hq submit -- /bin/bash -c 'echo $PPID' Name of a job # Each job has assigned a name. It has only an informative character for the user. By default, the name is extracted from the job's program name. You can set a job name explicitly by: hq submit --name=<NAME> ... Information about jobs # List of all jobs: hq jobs Detailed information about a job: hq job <job-id> Output of the job # By default, job produces two files named stdout.<job-id>.<task-id> and stderr.<job-id>.<task-id> that contains standard output and standard error output in the. The files are by default placed in the directory where the job was submitted. This can be changed via options --stdout=<path> and --stderr=<path> . These set paths where stdout/stderr files are created. Placeholders %{JOB_ID} and %{TASK_ID} in a path will be replaced to the JOB_ID/TASK_ID. You can disable creating stdout/stderr completely by setting value none . ( --stdout=none / --stderr=none ). Task states # Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted - Only an informative state that a submission was successfull; it is immediately changed into \"Waiting\" state. Waiting - The task is waiting for start Running - The task is running in a worker. It may become \"waiting\" again when a worker (where the task is running) is lost. Finished - The task was sucessfully finished. Failed - The task failed. The error can be shown by hq job <job-id> . Canceled - The task was canceled by a user. Job states # In simple jobs, job state correspondes directly to the state of its a single task. In case of task arrays, see the chapter about task arrays. Canceling jobs # hq cancel <job-id> A job cannot be canceled if it is already finished, failed, or canceled. Priorities # Not released yet, scheduled for release v0.3","title":"Jobs (Basics)"},{"location":"jobs/#jobs","text":"A job is a portion of work that can be submited into a HyperQueue. Each job may have one or tasks . In the current version, a is a single execution of a program. In this section, we are introducing simple jobs, where each job has exactly one tasks. See section about Job arrays for submiting more tasks in one job.","title":"Jobs"},{"location":"jobs/#identification-numbers","text":"Each job is identified by a positive integer that is assigned by HyperQueue server when the job is submitted, we refer to it as job id . Each task is identified by an unsigned 32b integer called task id . Task id can be assigned by a user and same task id may be used in two different jobs. In simple jobs, task id is set to 0.","title":"Identification numbers"},{"location":"jobs/#submiting-jobs","text":"hq submit <program> <args1> ... HyperQueue assigns a unique job id when a job is submitted. When your command contains its own switches, you need to use -- after submit: hq submit -- /bin/bash -c 'echo $PPID'","title":"Submiting jobs"},{"location":"jobs/#name-of-a-job","text":"Each job has assigned a name. It has only an informative character for the user. By default, the name is extracted from the job's program name. You can set a job name explicitly by: hq submit --name=<NAME> ...","title":"Name of a job"},{"location":"jobs/#information-about-jobs","text":"List of all jobs: hq jobs Detailed information about a job: hq job <job-id>","title":"Information about jobs"},{"location":"jobs/#output-of-the-job","text":"By default, job produces two files named stdout.<job-id>.<task-id> and stderr.<job-id>.<task-id> that contains standard output and standard error output in the. The files are by default placed in the directory where the job was submitted. This can be changed via options --stdout=<path> and --stderr=<path> . These set paths where stdout/stderr files are created. Placeholders %{JOB_ID} and %{TASK_ID} in a path will be replaced to the JOB_ID/TASK_ID. You can disable creating stdout/stderr completely by setting value none . ( --stdout=none / --stderr=none ).","title":"Output of the job"},{"location":"jobs/#task-states","text":"Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted - Only an informative state that a submission was successfull; it is immediately changed into \"Waiting\" state. Waiting - The task is waiting for start Running - The task is running in a worker. It may become \"waiting\" again when a worker (where the task is running) is lost. Finished - The task was sucessfully finished. Failed - The task failed. The error can be shown by hq job <job-id> . Canceled - The task was canceled by a user.","title":"Task states"},{"location":"jobs/#job-states","text":"In simple jobs, job state correspondes directly to the state of its a single task. In case of task arrays, see the chapter about task arrays.","title":"Job states"},{"location":"jobs/#canceling-jobs","text":"hq cancel <job-id> A job cannot be canceled if it is already finished, failed, or canceled.","title":"Canceling jobs"},{"location":"jobs/#priorities","text":"Not released yet, scheduled for release v0.3","title":"Priorities"}]}