{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HyperQueue is a tool designed to simplify execution of large workflows on HPC clusters. It allows you to execute a large number of tasks in a simple way, without having to manually submit jobs into batch schedulers like PBS or Slurm. You just specify what you want to compute \u2013 HyperQueue will automatically ask for computational resources and dynamically load-balance tasks across all allocated nodes and cores. Useful links # Installation Quick start Repository Discussion forum Zulip (chat platform) Features # Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Overview"},{"location":"#useful-links","text":"Installation Quick start Repository Discussion forum Zulip (chat platform)","title":"Useful links"},{"location":"#features","text":"Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Features"},{"location":"cheatsheet/","text":"Cheatsheet # Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"events/","text":"Events # HyperQueue internally records various events that describe what has happened during the lifetime of the HQ cluster (worker has connected, task was finished, an allocation was submitted to PBS, etc.). These events might be useful for some power-users, for example to analyze task execution statistics or allocation durations. To access these events, you have to start the HyperQueue server with the --event-log-path option: $ hq server start --event-log-path = events.bin If you use this flag, HQ will continuously stream its events into a log file at the provided path. The events are serialized using a compressed binary encoding. To access the event data from the log file, you first have to export them. JSON export # To export data from the log file to JSON, you can use the following command: $ hq event-log export <event-log-path> The events will be read from the provided log file and printed to stdout encoded in JSON, one event per line (this corresponds to line-delimited JSON, i.e. NDJSON ). Warning The JSON format of the events and their definition is currently unstable and can change with a new HyperQueue version.","title":"Events"},{"location":"events/#events","text":"HyperQueue internally records various events that describe what has happened during the lifetime of the HQ cluster (worker has connected, task was finished, an allocation was submitted to PBS, etc.). These events might be useful for some power-users, for example to analyze task execution statistics or allocation durations. To access these events, you have to start the HyperQueue server with the --event-log-path option: $ hq server start --event-log-path = events.bin If you use this flag, HQ will continuously stream its events into a log file at the provided path. The events are serialized using a compressed binary encoding. To access the event data from the log file, you first have to export them.","title":"Events"},{"location":"events/#json-export","text":"To export data from the log file to JSON, you can use the following command: $ hq event-log export <event-log-path> The events will be read from the provided log file and printed to stdout encoded in JSON, one event per line (this corresponds to line-delimited JSON, i.e. NDJSON ). Warning The JSON format of the events and their definition is currently unstable and can change with a new HyperQueue version.","title":"JSON export"},{"location":"faq/","text":"FAQ # HQ fundamentals # How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs containing tasks to the server. You may have hundreds of thousands of tasks; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign tasks to them. Workers are fully and dynamically controlled by server; you do not need to specify what tasks are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a task in HQ? Task is a unit of computation. Currently, it is either the execution of an arbitrary external program (specified via CLI) or the execution of a single Python function (specified via our Python API). What is a job in HQ? Job is a collection of tasks (a task graph). You can display and manage jobs using the CLI. How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. How many jobs/tasks may I submit into HQ? Our preliminary benchmarks show that the overhead of HQ is around 0.1 ms per task. It should be thus possible to submit a job with tens or hundreds of thousands tasks into HQ. Note that HQ is designed for a large number of tasks, not jobs. If you want to perform a lot of computations, use task arrays , i.e. create a job with many tasks, not many jobs each with a single task. HQ also supports streaming of task outputs into a single file. This avoids creating many small files for each task on a distributed file system, which improves scaling. Does HQ support multi-CPU tasks? Yes. You can define an arbitrary amount of CPUs for each task. HQ is also NUMA aware and you can select the NUMA allocation strategy. Does HQ support job/task arrays? Yes, see task arrays . Does HQ support tasks with dependencies? Yes, although it is currently only implemented in the Python API, which is experimental. It is currently not possible to specify dependencies using the CLI. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python. Relation to HPC technologies # Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to work smoothly on systems using SLURM/PBS, they are not required in order for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely not. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on an HPC system each user (or a group of users that trust each other) may run their own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally on your personal computer. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to that file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running. Relation to other task runtimes # What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one HPC job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of missing load balancing. In the case of HQ, you do not have to aggregate tasks. You can submit millions of small tasks to HQ and it will take care of assigning them dynamically to individual workers (and SLURM/PBS jobs, if automatic allocation is used).","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#hq-fundamentals","text":"How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs containing tasks to the server. You may have hundreds of thousands of tasks; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign tasks to them. Workers are fully and dynamically controlled by server; you do not need to specify what tasks are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a task in HQ? Task is a unit of computation. Currently, it is either the execution of an arbitrary external program (specified via CLI) or the execution of a single Python function (specified via our Python API). What is a job in HQ? Job is a collection of tasks (a task graph). You can display and manage jobs using the CLI. How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. How many jobs/tasks may I submit into HQ? Our preliminary benchmarks show that the overhead of HQ is around 0.1 ms per task. It should be thus possible to submit a job with tens or hundreds of thousands tasks into HQ. Note that HQ is designed for a large number of tasks, not jobs. If you want to perform a lot of computations, use task arrays , i.e. create a job with many tasks, not many jobs each with a single task. HQ also supports streaming of task outputs into a single file. This avoids creating many small files for each task on a distributed file system, which improves scaling. Does HQ support multi-CPU tasks? Yes. You can define an arbitrary amount of CPUs for each task. HQ is also NUMA aware and you can select the NUMA allocation strategy. Does HQ support job/task arrays? Yes, see task arrays . Does HQ support tasks with dependencies? Yes, although it is currently only implemented in the Python API, which is experimental. It is currently not possible to specify dependencies using the CLI. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python.","title":"HQ fundamentals"},{"location":"faq/#relation-to-hpc-technologies","text":"Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to work smoothly on systems using SLURM/PBS, they are not required in order for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely not. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on an HPC system each user (or a group of users that trust each other) may run their own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally on your personal computer. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to that file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running.","title":"Relation to HPC technologies"},{"location":"faq/#relation-to-other-task-runtimes","text":"What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one HPC job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of missing load balancing. In the case of HQ, you do not have to aggregate tasks. You can submit millions of small tasks to HQ and it will take care of assigning them dynamically to individual workers (and SLURM/PBS jobs, if automatic allocation is used).","title":"Relation to other task runtimes"},{"location":"installation/","text":"Binary distribution (recommended) # The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation. Compilation from source code # You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Installation"},{"location":"installation/#binary-distribution-recommended","text":"The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation.","title":"Binary distribution (recommended)"},{"location":"installation/#compilation-from-source-code","text":"You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Compilation from source code"},{"location":"other-tools/","text":"Comparison with other task runtimes # There is a large number of task runtimes, so we cannot list all of them here. Below you can find a selection of other task runtimes that we have experience with and/or that are somehow relevant for HyperQueue. Dask # Dask is a task runtime that is very popular within the Python community, which allows executing arbitrary task graphs composed of Python functions on a distributed cluster. It also supports distributing code using numpy or pandas compatible API. While Dask by itself does not interact with PBS or Slurm, you can use Dask-JobQueue to make it operate in a similar fashion as HyperQueue - with the centralized server running on a login node and the workers running on compute nodes. Dask does not support arbitrary resource requirements and since it is written in Python, it can have problems with scaling to very large task graphs. If your use-case is primarily Python-based, you should definitely give Dask a try, it's a great tool. SnakeMake # SnakeMake is a workflow execution system that focuses on scientific reproducibility. It lets users specify computational workflows using a DSL that combined configuration files and Python. It can operate both as a meta-scheduler (outside of PBS/Slurm) and also as a classical task runtime within a PBS/Slurm job. With SnakeMake, you can submit a workflow either using a task-per-job model (which has high overhead) or you can partition the workflow into several jobs, but in that case SnakeMake will not provide load balancing across these partitions. HyperQueue allows you to submit large workflows without partitioning them manually in any way, as the server will dynamically load balance the tasks onto workers from different PBS/Slurm allocations. Since SnakeMake workflows are defined in configuration files, it's a bit more involved to run computations in SnakeMake than in HyperQueue. On the other hand, SnakeMake lets you define more complex workflows with improved traceability and reproducibility. Merlin # Merlin is a workflow execution system focused on HPC-scale machine learning workflows. It has a relatively similar architecture to HyperQueue, although it uses configuration files rather than CLI for specifying jobs.","title":"Comparison With Other Tools"},{"location":"other-tools/#comparison-with-other-task-runtimes","text":"There is a large number of task runtimes, so we cannot list all of them here. Below you can find a selection of other task runtimes that we have experience with and/or that are somehow relevant for HyperQueue.","title":"Comparison with other task runtimes"},{"location":"other-tools/#dask","text":"Dask is a task runtime that is very popular within the Python community, which allows executing arbitrary task graphs composed of Python functions on a distributed cluster. It also supports distributing code using numpy or pandas compatible API. While Dask by itself does not interact with PBS or Slurm, you can use Dask-JobQueue to make it operate in a similar fashion as HyperQueue - with the centralized server running on a login node and the workers running on compute nodes. Dask does not support arbitrary resource requirements and since it is written in Python, it can have problems with scaling to very large task graphs. If your use-case is primarily Python-based, you should definitely give Dask a try, it's a great tool.","title":"Dask"},{"location":"other-tools/#snakemake","text":"SnakeMake is a workflow execution system that focuses on scientific reproducibility. It lets users specify computational workflows using a DSL that combined configuration files and Python. It can operate both as a meta-scheduler (outside of PBS/Slurm) and also as a classical task runtime within a PBS/Slurm job. With SnakeMake, you can submit a workflow either using a task-per-job model (which has high overhead) or you can partition the workflow into several jobs, but in that case SnakeMake will not provide load balancing across these partitions. HyperQueue allows you to submit large workflows without partitioning them manually in any way, as the server will dynamically load balance the tasks onto workers from different PBS/Slurm allocations. Since SnakeMake workflows are defined in configuration files, it's a bit more involved to run computations in SnakeMake than in HyperQueue. On the other hand, SnakeMake lets you define more complex workflows with improved traceability and reproducibility.","title":"SnakeMake"},{"location":"other-tools/#merlin","text":"Merlin is a workflow execution system focused on HPC-scale machine learning workflows. It has a relatively similar architecture to HyperQueue, although it uses configuration files rather than CLI for specifying jobs.","title":"Merlin"},{"location":"quickstart/","text":"Here we provide an example of deploying HyperQueue on a local computer and running a simple \"Hello world\" script. Run each of the following three commands in separate terminals. Start the HyperQueue server $ hq server start The server will manage computing resources (workers) and distribute submitted tasks amongst them. Start a HyperQueue worker $ hq worker start The worker will connect to the server and execute submitted tasks. Submit a simple computation $ hq submit echo \"Hello world\" This command will submit a job with a single task that will execute echo \"Hello world\" on a worker. You can find the output of the task in job-1/0.stdout . That's it! For a more in-depth explanation of how HyperQueue works and what it can do, check the Deployment and Jobs sections.","title":"Quickstart"},{"location":"cli/output-mode/","text":"By default, HyperQueue CLI commands output information in a human-readable way, usually in the form of a table. If you want to use the CLI commands programmatically, HyperQueue offers two additional output modes that are designed to be machine-readable. You can change the output type of any HyperQueue CLI command either by using the --output-mode flag or by setting the HQ_OUTPUT_MODE environment variable. Flag Environment variable $ hq --output-mode = json job list $ HQ_OUTPUT_MODE = json hq job list Currently, there are three output modes available. The default, human-readable cli mode, and then two machine-readable modes, JSON and Quiet . Important Each machine-readable mode supports a set of commands. You can also use commands that are not listed here, but their output might be unstable, or they might not output anything for a given output mode. JSON # The json output mode is intended to provide very detailed information in the form of a JSON value. With this mode, HyperQueue will always output exactly one JSON value, either an array or an object. Error handling # When an error occurs during the execution of a command, the program will exit with exit code 1 and the program will output a JSON object with a single error key containing a human-readable description of the error. Date formatting # Time-based items are formatted in the following way: Duration - formatted as a floating point number of seconds. Datetime (timestamp) - formatted as a ISO8601 date in UTC Supported commands # Server info: hq server info Example { \"host\" : \"my-machine\" , \"hq_port\" : 42189 , \"pid\" : 32586 , \"server_dir\" : \"/foo/bar/.hq-server\" , \"start_date\" : \"2021-12-20T08:45:41.775753188Z\" , \"version\" : \"0.7.0\" , \"worker_port\" : 38627 } Worker list: hq worker list Example [{ \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 }] Worker info: hq worker info <worker-id> Example { \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 } Submit a job: hq submit <command> Example { \"id\" : 1 } Job list: hq job list Example [{ \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }] Job info: hq job info <job-id> --tasks Example { \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"info\" : { \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }, \"max_fails\" : null , \"pin\" : null , \"priority\" : 0 , \"program\" : { \"args\" : [ \"ls\" ], \"cwd\" : \"%{SUBMIT_DIR}\" , \"env\" : { \"FOO\" : \"BAR\" }, \"stderr\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stderr\" }, \"stdout\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stdout\" } }, \"started_at\" : \"2021-12-20T08:45:53.458919345Z\" , \"tasks\" : [{ \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"id\" : 0 , \"started_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"state\" : \"finished\" , \"worker\" : 1 , \"cwd\" : \"/tmp/foo\" , \"stderr\" : { \"File\" : \"job-1/0.stderr\" }, \"stdout\" : { \"File\" : \"job-1/0.stdout\" } }], \"time_limit\" : null , \"submit_dir\" : \"/foo/bar/submit\" } Automatic allocation queue list: hq alloc list Example [{ \"additional_args\" : [], \"backlog\" : 4 , \"id\" : 1 , \"manager\" : \"PBS\" , \"max_worker_count\" : null , \"name\" : null , \"timelimit\" : 1800.0 , \"worker_cpu_args\" : null , \"worker_resource_args\" : [], \"workers_per_alloc\" : 1 }] Automatic allocation queue info: hq alloc info <allocation-queue-id> Example [{ \"id\" : \"pbs-1\" , \"worker_count\" : 4 , \"queue_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"started_at\" : \"2021-12-20T08:58:25.538001256Z\" , \"ended_at\" : null , \"status\" : \"running\" , \"workdir\" : \"/foo/bar\" }] Automatic allocation queue events: hq alloc events <allocation-queue-id> Example [{ \"date\" : \"2021-12-20T08:56:16.437123396Z\" , \"event\" : \"allocation-finished\" , \"params\" : { \"id\" : \"pbs-1\" } }, { \"date\" : \"2021-12-20T08:58:16.437123396Z\" , \"event\" : \"status-fail\" , \"params\" : { \"error\" : \"qstat failed\" } }] Quiet # The quiet output mode will cause HyperQueue to output only the most important information that should be parseable without any complex parsing logic, e.g. using only Bash scripts. Error handling # When an error occurs during the execution of a command, the program will exit with exit code 1 and the error will be printed to the standard error output. Supported commands # Submit a job: hq submit <command> Schema Outputs a single line containing the ID of the created job. Example $ hq --output-mode = quiet submit ls 1","title":"Output mode"},{"location":"cli/output-mode/#json","text":"The json output mode is intended to provide very detailed information in the form of a JSON value. With this mode, HyperQueue will always output exactly one JSON value, either an array or an object.","title":"JSON"},{"location":"cli/output-mode/#error-handling","text":"When an error occurs during the execution of a command, the program will exit with exit code 1 and the program will output a JSON object with a single error key containing a human-readable description of the error.","title":"Error handling"},{"location":"cli/output-mode/#date-formatting","text":"Time-based items are formatted in the following way: Duration - formatted as a floating point number of seconds. Datetime (timestamp) - formatted as a ISO8601 date in UTC","title":"Date formatting"},{"location":"cli/output-mode/#supported-commands","text":"Server info: hq server info Example { \"host\" : \"my-machine\" , \"hq_port\" : 42189 , \"pid\" : 32586 , \"server_dir\" : \"/foo/bar/.hq-server\" , \"start_date\" : \"2021-12-20T08:45:41.775753188Z\" , \"version\" : \"0.7.0\" , \"worker_port\" : 38627 } Worker list: hq worker list Example [{ \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 }] Worker info: hq worker info <worker-id> Example { \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 } Submit a job: hq submit <command> Example { \"id\" : 1 } Job list: hq job list Example [{ \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }] Job info: hq job info <job-id> --tasks Example { \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"info\" : { \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }, \"max_fails\" : null , \"pin\" : null , \"priority\" : 0 , \"program\" : { \"args\" : [ \"ls\" ], \"cwd\" : \"%{SUBMIT_DIR}\" , \"env\" : { \"FOO\" : \"BAR\" }, \"stderr\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stderr\" }, \"stdout\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stdout\" } }, \"started_at\" : \"2021-12-20T08:45:53.458919345Z\" , \"tasks\" : [{ \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"id\" : 0 , \"started_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"state\" : \"finished\" , \"worker\" : 1 , \"cwd\" : \"/tmp/foo\" , \"stderr\" : { \"File\" : \"job-1/0.stderr\" }, \"stdout\" : { \"File\" : \"job-1/0.stdout\" } }], \"time_limit\" : null , \"submit_dir\" : \"/foo/bar/submit\" } Automatic allocation queue list: hq alloc list Example [{ \"additional_args\" : [], \"backlog\" : 4 , \"id\" : 1 , \"manager\" : \"PBS\" , \"max_worker_count\" : null , \"name\" : null , \"timelimit\" : 1800.0 , \"worker_cpu_args\" : null , \"worker_resource_args\" : [], \"workers_per_alloc\" : 1 }] Automatic allocation queue info: hq alloc info <allocation-queue-id> Example [{ \"id\" : \"pbs-1\" , \"worker_count\" : 4 , \"queue_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"started_at\" : \"2021-12-20T08:58:25.538001256Z\" , \"ended_at\" : null , \"status\" : \"running\" , \"workdir\" : \"/foo/bar\" }] Automatic allocation queue events: hq alloc events <allocation-queue-id> Example [{ \"date\" : \"2021-12-20T08:56:16.437123396Z\" , \"event\" : \"allocation-finished\" , \"params\" : { \"id\" : \"pbs-1\" } }, { \"date\" : \"2021-12-20T08:58:16.437123396Z\" , \"event\" : \"status-fail\" , \"params\" : { \"error\" : \"qstat failed\" } }]","title":"Supported commands"},{"location":"cli/output-mode/#quiet","text":"The quiet output mode will cause HyperQueue to output only the most important information that should be parseable without any complex parsing logic, e.g. using only Bash scripts.","title":"Quiet"},{"location":"cli/output-mode/#error-handling_1","text":"When an error occurs during the execution of a command, the program will exit with exit code 1 and the error will be printed to the standard error output.","title":"Error handling"},{"location":"cli/output-mode/#supported-commands_1","text":"Submit a job: hq submit <command> Schema Outputs a single line containing the ID of the created job. Example $ hq --output-mode = quiet submit ls 1","title":"Supported commands"},{"location":"cli/shortcuts/","text":"Various HyperQueue CLI command options let you enter some value in a specific syntactical format for convenience. Here you can find a list of such shortcuts. ID selector # When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq job cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq job cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq job cancel all - cancel all jobs last The most recently created ID hq worker stop last - stop most recently connected worker hq job cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 . Tip You can add underscore ( _ ) separators to any of the entered numeric values to improve readability: $ hq submit --array = 1 -1000_000 ... Supported commands and options # hq submit --array=<selector> hq worker stop <selector> hq job info <selector> does not support all (use hq job list instead) hq job cancel <selector> hq job wait <selector> hq job progress <selector> Duration # You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here . Supported commands and options # hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ... Tip For increased compatibility with PBS and Slurm , you can also specify the --time-limit option of hq alloc add using the HH:MM:SS format.","title":"Shortcuts"},{"location":"cli/shortcuts/#id-selector","text":"When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq job cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq job cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq job cancel all - cancel all jobs last The most recently created ID hq worker stop last - stop most recently connected worker hq job cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 . Tip You can add underscore ( _ ) separators to any of the entered numeric values to improve readability: $ hq submit --array = 1 -1000_000 ...","title":"ID selector"},{"location":"cli/shortcuts/#supported-commands-and-options","text":"hq submit --array=<selector> hq worker stop <selector> hq job info <selector> does not support all (use hq job list instead) hq job cancel <selector> hq job wait <selector> hq job progress <selector>","title":"Supported commands and options"},{"location":"cli/shortcuts/#duration","text":"You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here .","title":"Duration"},{"location":"cli/shortcuts/#supported-commands-and-options_1","text":"hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ... Tip For increased compatibility with PBS and Slurm , you can also specify the --time-limit option of hq alloc add using the HH:MM:SS format.","title":"Supported commands and options"},{"location":"deployment/","text":"Architecture # HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/#architecture","text":"HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/allocation/","text":"Automatic allocation is one of the core features of HyperQueue. When you run HyperQueue on an HPC cluster, it allows you to autonomously ask the job manager (PBS/Slurm) for computing resources and spawn HyperQueue workers on the provided nodes. Using this mechanism, you can submit computations into HyperQueue without caring about the underlying PBS/Slurm jobs. Job terminology It is common to use the term \"job\" for jobs created by an HPC job manager, such as PBS or Slurm, which are used to perform computations on HPC clusters. However, HyperQueue also uses the term \"job\" for ensembles of tasks . To differentiate between these two, we will refer to jobs created by PBS or Slurm as allocations . We will also refer to PBS/Slurm as a job manager . Allocation queue # To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs --time-limit 1h -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues. Parameters # In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue: --time-limit <duration> Sets the walltime of created allocations 1 . This parameter is required , as HyperQueue must know the duration of the individual allocations. Make sure that you pass a time limit that does not exceed the limit of the PBS/Slurm queue that you intend to use, otherwise the allocation submissions will fail. You can use the dry-run command to debug this. --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. --workers-per-alloc <count> How many workers should be requested in each allocation. This corresponds to the number of requested nodes, as the allocator will always create a single worker per node. --max-worker-count <count> Maximum number of workers that can be queued or running in the created allocation queue. The amount of workers will be limited by the manager (PBS/Slurm), but you can use this parameter to make the limit smaller, for example if you also want to create manager allocations outside HyperQueue. Worker resources You can specify CPU and generic resources of workers spawned in the created allocation queue. The name and syntax of these parameters is the same as when you create a worker manually: PBS Slurm $ hq alloc add pbs --time-limit 1h --cpus 4x4 --resource \"gpu=indices(1-2)\" -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h --cpus 4x4 --resource \"gpu=indices(1-2)\" -- --partition = p1 If you do not pass any resources, they will be detected automatically (same as it works with hq worker start ). --name <name> Name of the allocation queue. Will be used to name allocations. Serves for debug purposes only. Behavior # The automatic allocator will submit allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if **backlog** was set to `4` and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. !!! note The **backlog** value does not limit the number of running allocations, only the number of queued allocations. !!! warning Do not set the `backlog` to a large number to avoid overloading the job manager. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes. Stopping automatic allocation # If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> When an allocation queue is removed, all its corresponding queued and running allocations will be canceled immediately. By default, HQ will not allow you to remove an allocation queue that contains a running allocation. If you want to force its removal, use the --force flag. When the HQ server stops, it will automatically remove all allocation queues and cleanup all allocations. Debugging automatic allocation # Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides a \"dry-run\" command that you can use to test allocation parameters. HyperQueue also provides various sources of information that can help you find out what is going on. To mitigate the case of incorrectly entered allocation parameters, HQ will also try to submit a test allocation (do a \"dry run\") into the target HPC job manager when you add a new allocation queue. If the test allocation fails, the queue will not be created. You can avoid this behaviour by passing the --no-dry-run flag to hq alloc add . There are also additional safety limits. If 10 allocations in a succession fail to be submitted, or if 3 allocations that were submitted fail during runtime in a succession, the corresponding allocation queue will be automatically removed. Dry-run command # To test whether PBS/Slurm will accept the submit parameters that you provide to the auto allocator without creating an allocation queue, you can use the dry-run command. It accepts the same parameters as hq alloc add , which it will use to immediately submit an allocation and print any encountered errors. $ hq alloc dry-run pbs --timelimit 2h -- q qexp -A Project1 If the allocation was submitted successfully, it will be canceled immediately to avoid wasting resources. Finding information about allocations # Basic queue information This command will show you details about allocations created by the automatic allocator. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq-submit.sh Useful autoalloc commands # Here is a list of useful commands to manage automatic allocation: Display a list of all allocation queues # $ hq alloc list Display information about an allocation queue # $ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option. You can use various shortcuts for the duration value. \u21a9","title":"Automatic Allocation"},{"location":"deployment/allocation/#allocation-queue","text":"To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs --time-limit 1h -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues.","title":"Allocation queue"},{"location":"deployment/allocation/#parameters","text":"In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue: --time-limit <duration> Sets the walltime of created allocations 1 . This parameter is required , as HyperQueue must know the duration of the individual allocations. Make sure that you pass a time limit that does not exceed the limit of the PBS/Slurm queue that you intend to use, otherwise the allocation submissions will fail. You can use the dry-run command to debug this. --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. --workers-per-alloc <count> How many workers should be requested in each allocation. This corresponds to the number of requested nodes, as the allocator will always create a single worker per node. --max-worker-count <count> Maximum number of workers that can be queued or running in the created allocation queue. The amount of workers will be limited by the manager (PBS/Slurm), but you can use this parameter to make the limit smaller, for example if you also want to create manager allocations outside HyperQueue. Worker resources You can specify CPU and generic resources of workers spawned in the created allocation queue. The name and syntax of these parameters is the same as when you create a worker manually: PBS Slurm $ hq alloc add pbs --time-limit 1h --cpus 4x4 --resource \"gpu=indices(1-2)\" -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h --cpus 4x4 --resource \"gpu=indices(1-2)\" -- --partition = p1 If you do not pass any resources, they will be detected automatically (same as it works with hq worker start ). --name <name> Name of the allocation queue. Will be used to name allocations. Serves for debug purposes only.","title":"Parameters"},{"location":"deployment/allocation/#behavior","text":"The automatic allocator will submit allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if **backlog** was set to `4` and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. !!! note The **backlog** value does not limit the number of running allocations, only the number of queued allocations. !!! warning Do not set the `backlog` to a large number to avoid overloading the job manager. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes.","title":"Behavior"},{"location":"deployment/allocation/#stopping-automatic-allocation","text":"If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> When an allocation queue is removed, all its corresponding queued and running allocations will be canceled immediately. By default, HQ will not allow you to remove an allocation queue that contains a running allocation. If you want to force its removal, use the --force flag. When the HQ server stops, it will automatically remove all allocation queues and cleanup all allocations.","title":"Stopping automatic allocation"},{"location":"deployment/allocation/#debugging-automatic-allocation","text":"Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides a \"dry-run\" command that you can use to test allocation parameters. HyperQueue also provides various sources of information that can help you find out what is going on. To mitigate the case of incorrectly entered allocation parameters, HQ will also try to submit a test allocation (do a \"dry run\") into the target HPC job manager when you add a new allocation queue. If the test allocation fails, the queue will not be created. You can avoid this behaviour by passing the --no-dry-run flag to hq alloc add . There are also additional safety limits. If 10 allocations in a succession fail to be submitted, or if 3 allocations that were submitted fail during runtime in a succession, the corresponding allocation queue will be automatically removed.","title":"Debugging automatic allocation"},{"location":"deployment/allocation/#dry-run-command","text":"To test whether PBS/Slurm will accept the submit parameters that you provide to the auto allocator without creating an allocation queue, you can use the dry-run command. It accepts the same parameters as hq alloc add , which it will use to immediately submit an allocation and print any encountered errors. $ hq alloc dry-run pbs --timelimit 2h -- q qexp -A Project1 If the allocation was submitted successfully, it will be canceled immediately to avoid wasting resources.","title":"Dry-run command"},{"location":"deployment/allocation/#finding-information-about-allocations","text":"Basic queue information This command will show you details about allocations created by the automatic allocator. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq-submit.sh","title":"Finding information about allocations"},{"location":"deployment/allocation/#useful-autoalloc-commands","text":"Here is a list of useful commands to manage automatic allocation:","title":"Useful autoalloc commands"},{"location":"deployment/allocation/#display-a-list-of-all-allocation-queues","text":"$ hq alloc list","title":"Display a list of all allocation queues"},{"location":"deployment/allocation/#display-information-about-an-allocation-queue","text":"$ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option. You can use various shortcuts for the duration value. \u21a9","title":"Display information about an allocation queue"},{"location":"deployment/server/","text":"The server is a crucial component of HyperQueue which manages workers and jobs . Before running any computations or deploying workers, you must first start the server. Starting the server # The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST Server directory # When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server. Keeping the server alive # The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux . Stopping server # You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Server"},{"location":"deployment/server/#starting-the-server","text":"The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST","title":"Starting the server"},{"location":"deployment/server/#server-directory","text":"When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server.","title":"Server directory"},{"location":"deployment/server/#keeping-the-server-alive","text":"The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux .","title":"Keeping the server alive"},{"location":"deployment/server/#stopping-server","text":"You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Stopping server"},{"location":"deployment/worker/","text":"Workers connect to a running instance of a HyperQueue server and wait for task assignments. Once some task is assigned to them, they will compute it and notify the server of its completion. Starting workers # Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually. Automatic worker deployment (recommended) # If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers. Manual worker deployment # If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though. Deploying a worker using PBS/Slurm # If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment. Stopping workers # If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : $ hq worker stop <selector> Time limit # HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit manually for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated. Idle timeout # When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout. Worker state # Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout . Lost connection to the server # The behavior of what should happen with a worker that lost its connection to the server is configured via hq worker start --on-server-lost=<policy> . You can select from two policies: stop - The worker immediately terminates and kills all currently running tasks. finish-running - The worker does not start to execute any new tasks, but it tries to finish tasks that are already running. When all such tasks finish, the worker will terminate. stop is the default policy when a worker is manually started by hq worker start . When a worker is started by the automatic allocator , then finish-running is used as the default value. Useful worker commands # Here is a list of useful worker commands: Display worker list # This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command. Display information about a specific worker # $ hq worker info <worker-id> You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Workers"},{"location":"deployment/worker/#starting-workers","text":"Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually.","title":"Starting workers"},{"location":"deployment/worker/#automatic-worker-deployment-recommended","text":"If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers.","title":"Automatic worker deployment (recommended)"},{"location":"deployment/worker/#manual-worker-deployment","text":"If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though.","title":"Manual worker deployment"},{"location":"deployment/worker/#deploying-a-worker-using-pbsslurm","text":"If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment.","title":"Deploying a worker using PBS/Slurm"},{"location":"deployment/worker/#stopping-workers","text":"If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : $ hq worker stop <selector>","title":"Stopping workers"},{"location":"deployment/worker/#time-limit","text":"HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit manually for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated.","title":"Time limit"},{"location":"deployment/worker/#idle-timeout","text":"When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout.","title":"Idle timeout"},{"location":"deployment/worker/#worker-state","text":"Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout .","title":"Worker state"},{"location":"deployment/worker/#lost-connection-to-the-server","text":"The behavior of what should happen with a worker that lost its connection to the server is configured via hq worker start --on-server-lost=<policy> . You can select from two policies: stop - The worker immediately terminates and kills all currently running tasks. finish-running - The worker does not start to execute any new tasks, but it tries to finish tasks that are already running. When all such tasks finish, the worker will terminate. stop is the default policy when a worker is manually started by hq worker start . When a worker is started by the automatic allocator , then finish-running is used as the default value.","title":"Lost connection to the server"},{"location":"deployment/worker/#useful-worker-commands","text":"Here is a list of useful worker commands:","title":"Useful worker commands"},{"location":"deployment/worker/#display-worker-list","text":"This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command.","title":"Display worker list"},{"location":"deployment/worker/#display-information-about-a-specific-worker","text":"$ hq worker info <worker-id> You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Display information about a specific worker"},{"location":"jobs/arrays/","text":"It is a common use case to execute the same command for multiple input parameters, for example: Perform a simulation for each input file in a directory or for each line in a CSV file. Train many machine learning models using hyperparameter search for each model configuration. HyperQueue allows you to do this using a job that contains many tasks. We call such jobs Task arrays . You can create a task array with a single submit command and then manage all created tasks as a single group using its containing job. Note Task arrays are somewhat similar to \"job arrays\" used by PBS and Slurm. However, HQ does not use PBS/Slurm job arrays for implementing this feature. Therefore, the limits that are commonly enforced on job arrays on HPC clusters do not apply to HyperQueue task arrays. Creating task arrays # To create a task array, you must provide some source that will determine how many tasks should be created and what inputs (environment variables) should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming . Integer range # The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ... Lines of a file # Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ... JSON array # You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"Task Arrays"},{"location":"jobs/arrays/#creating-task-arrays","text":"To create a task array, you must provide some source that will determine how many tasks should be created and what inputs (environment variables) should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming .","title":"Creating task arrays"},{"location":"jobs/arrays/#integer-range","text":"The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ...","title":"Integer range"},{"location":"jobs/arrays/#lines-of-a-file","text":"Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ...","title":"Lines of a file"},{"location":"jobs/arrays/#json-array","text":"You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"JSON array"},{"location":"jobs/cresources/","text":"Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU. Requesting more CPUs # By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus. Requesting all CPUs # Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...> CPU related environment variables # HQ_CPUS - List of cores assigned to task HQ_PIN - Is set to taskset or omp (depending on the used pin mode) if the task was pinned by HyperQueue (see below). NUM_OMP_THREADS -- Set to number of cores assigned for task. (For compatibility with OpenMP). Pinning # By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in the case of NUMA, processes could be pinned, either manually or automatically. Automatic pinning # HyperQueue can pin threads using two ways: with taskset or by setting OpenMP environment variables. You can use the --pin flag to choose between these two modes. taskset OpenMP $ hq submit --pin taskset --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: taskset -c \"<allocated-cores>\" <your-program> <args> ` $ hq submit --pin omp --cpus=8 <your-program> <args> will cause HyperQueue to execute your program like this: OMP_PROC_BIND = close OMP_PLACES = \"{<allocated-cores>}\" <your-program> <args> If any automatic pinning mode is enabled, the environment variable HQ_PIN will be set. Manual pinning # If you want to gain a full control over pinning processes, you may pin the process by yourself. The assigned CPUs are stored in the environment variable HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities such as taskset or numactl and pass them HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example, you can create the following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In the case of numactl , the equivalent script would be: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> NUMA allocation policy # HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more than one socket (physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq submit --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq submit --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq submit --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\" CPU requests and job arrays # Resource requests are applied to each task of job. For example, if you submit the following: hq submit --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs. CPUs configuration # Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout. Manual specification of CPU configuration # If automatic detection fails, or you want to manually configure set a CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker start --cpus=2x12 Automatic detection of CPUs but ignores HyperThreading (it will detect only the first virtual core of each physical core) $ hq worker start --cpus=\"no-ht\" Manually specify that worker should use the following core ids and how they are organized into sockets. In this example, two sockets are defined, one with 3 cores and one with 2 cores. $ hq worker start --cpus=[[2, 3, 4], [10, 14]]","title":"CPU Resources"},{"location":"jobs/cresources/#requesting-more-cpus","text":"By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus.","title":"Requesting more CPUs"},{"location":"jobs/cresources/#requesting-all-cpus","text":"Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...>","title":"Requesting all CPUs"},{"location":"jobs/cresources/#cpu-related-environment-variables","text":"HQ_CPUS - List of cores assigned to task HQ_PIN - Is set to taskset or omp (depending on the used pin mode) if the task was pinned by HyperQueue (see below). NUM_OMP_THREADS -- Set to number of cores assigned for task. (For compatibility with OpenMP).","title":"CPU related environment variables"},{"location":"jobs/cresources/#pinning","text":"By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in the case of NUMA, processes could be pinned, either manually or automatically.","title":"Pinning"},{"location":"jobs/cresources/#automatic-pinning","text":"HyperQueue can pin threads using two ways: with taskset or by setting OpenMP environment variables. You can use the --pin flag to choose between these two modes. taskset OpenMP $ hq submit --pin taskset --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: taskset -c \"<allocated-cores>\" <your-program> <args> ` $ hq submit --pin omp --cpus=8 <your-program> <args> will cause HyperQueue to execute your program like this: OMP_PROC_BIND = close OMP_PLACES = \"{<allocated-cores>}\" <your-program> <args> If any automatic pinning mode is enabled, the environment variable HQ_PIN will be set.","title":"Automatic pinning"},{"location":"jobs/cresources/#manual-pinning","text":"If you want to gain a full control over pinning processes, you may pin the process by yourself. The assigned CPUs are stored in the environment variable HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities such as taskset or numactl and pass them HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example, you can create the following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In the case of numactl , the equivalent script would be: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...>","title":"Manual pinning"},{"location":"jobs/cresources/#numa-allocation-policy","text":"HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more than one socket (physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq submit --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq submit --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq submit --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\"","title":"NUMA allocation policy"},{"location":"jobs/cresources/#cpu-requests-and-job-arrays","text":"Resource requests are applied to each task of job. For example, if you submit the following: hq submit --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU requests and job arrays"},{"location":"jobs/cresources/#cpus-configuration","text":"Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout.","title":"CPUs configuration"},{"location":"jobs/cresources/#manual-specification-of-cpu-configuration","text":"If automatic detection fails, or you want to manually configure set a CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker start --cpus=2x12 Automatic detection of CPUs but ignores HyperThreading (it will detect only the first virtual core of each physical core) $ hq worker start --cpus=\"no-ht\" Manually specify that worker should use the following core ids and how they are organized into sockets. In this example, two sockets are defined, one with 3 cores and one with 2 cores. $ hq worker start --cpus=[[2, 3, 4], [10, 14]]","title":"Manual specification of CPU configuration"},{"location":"jobs/directives/","text":"Directives # You can specify job parameters using special comments ( directives ) specified in a submitted shell script. Directives are lines that begin with the #HQ prefix. Any text following this prefix will be interpreted as a command line argument for hq submit . Example directive file # Suppose that script.sh has the following content: #!/bin/bash #HQ --name=Example #HQ --cpus=\"2 compact\" --pin taskset ./my-program If you execute $ hq submit script.sh it will behave as if you have executed $ hq submit --name = Example --cpus = \"2 compact\" --pin taskset script.sh Directives mode # You can select three modes using the --directives flag of hq submit . The mode will determine when should HyperQueue attempt to parse directives from the provided command. auto (default) - Directives will be parsed if the first command passed to hq submit has the .sh extension. file - Directives will be parsed from the first command passed to hq submit . stdin - Directives will be parsed from stdin (see --stdin ) off - Directives will not be parsed. Tip When HQ parses directives from a file, it will also try to parse a shebang line from the script and use it to select an interpreter for running the script. Notes # Directives have to be defined at the beginning of the file. Only comments or empty lines are allowed to precede the directives. Directives have to be defined in the first 32KiB of the file, the rest of the file is ignored. Parameters set via CLI have precedence over parameters set via direectives: Parameters that cannot occur multiple times (like --name ) will be overriden by values set from CLI. Parameters that can occur multiple times (like --resource ) will be combined from CLI and from directives. A script may contain more lines with the #HQ prefix, such lines are combined and evaluated as a continuous list of parameters.","title":"Directives"},{"location":"jobs/directives/#directives","text":"You can specify job parameters using special comments ( directives ) specified in a submitted shell script. Directives are lines that begin with the #HQ prefix. Any text following this prefix will be interpreted as a command line argument for hq submit .","title":"Directives"},{"location":"jobs/directives/#example-directive-file","text":"Suppose that script.sh has the following content: #!/bin/bash #HQ --name=Example #HQ --cpus=\"2 compact\" --pin taskset ./my-program If you execute $ hq submit script.sh it will behave as if you have executed $ hq submit --name = Example --cpus = \"2 compact\" --pin taskset script.sh","title":"Example directive file"},{"location":"jobs/directives/#directives-mode","text":"You can select three modes using the --directives flag of hq submit . The mode will determine when should HyperQueue attempt to parse directives from the provided command. auto (default) - Directives will be parsed if the first command passed to hq submit has the .sh extension. file - Directives will be parsed from the first command passed to hq submit . stdin - Directives will be parsed from stdin (see --stdin ) off - Directives will not be parsed. Tip When HQ parses directives from a file, it will also try to parse a shebang line from the script and use it to select an interpreter for running the script.","title":"Directives mode"},{"location":"jobs/directives/#notes","text":"Directives have to be defined at the beginning of the file. Only comments or empty lines are allowed to precede the directives. Directives have to be defined in the first 32KiB of the file, the rest of the file is ignored. Parameters set via CLI have precedence over parameters set via direectives: Parameters that cannot occur multiple times (like --name ) will be overriden by values set from CLI. Parameters that can occur multiple times (like --resource ) will be combined from CLI and from directives. A script may contain more lines with the #HQ prefix, such lines are combined and evaluated as a continuous list of parameters.","title":"Notes"},{"location":"jobs/failure/","text":"In distributed systems, failure is inevitable. This sections describes how HyperQueue handles various types of failures and how can you affect its behavior. Resubmitting jobs # When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq job resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq job resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array. Task restart # Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id. Task array failures # By default, when a single task of a task array fails, the computation of the job will continue. You can change this behavior with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Handling Failure"},{"location":"jobs/failure/#resubmitting-jobs","text":"When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq job resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq job resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array.","title":"Resubmitting jobs"},{"location":"jobs/failure/#task-restart","text":"Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id.","title":"Task restart"},{"location":"jobs/failure/#task-array-failures","text":"By default, when a single task of a task array fails, the computation of the job will continue. You can change this behavior with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Task array failures"},{"location":"jobs/gresources/","text":"Generic resource management # Generic resource management serves for defining arbitrary resources provided by workers and also corresponding resource requests required by tasks. HyperQueue will take care of matching task resource requests so that only workers that can fulfill them will be able to execute such tasks. Some generic resources are automatically detected ; however, users may also define their own resources. Important Generic resources in HyperQueue exist on a purely logical level. They can correspond to physical things (like GPUs), but it is the responsibility of the user to make sure that this correspondence makes sense. HyperQueue by itself does not attach any semantics to generic resources, they are just numbers used for scheduling. Worker resources # Each worker can have several generic resources attached. Each generic resource is a resource pool identified by a name. A resource pool represents some resources provided by a worker; each task can then ask for a part of resources contained in that pool. There are two kind of resource pools: Indexed pool : This pool represents an enumerated set of resources represented by integers. Each resource has its own identity. Tasks do not ask for specific values from the set, they just specify how many resources do they require and HyperQueue will allocate the specified amount of resources from the pool for each task. This pool is useful for resources that have their own identity, for example individual GPU or FPGA accelerators. HyperQueue guarantees that no individual resource from the indexed pool is allocated to more than a single task at any given time and that a task will not be executed on a worker if it does not currently have enough individual resources to fulfill the resource request of the task. Sum pool : This pool represents a resource that has a certain size which be split into individual tasks. A typical example is memory; if a worker has 2000 bytes of memory, it can serve e.g. four tasks, if each task asks for 500 bytes of memory. HyperQueue guarantees that the sum of resource request sizes of running tasks on a worker does not exceed the total size of the sum pool. Specifying worker resources # You can specify the resource pools of a worker when you start it: $ hq worker start --resource \"<NAME1>=<DEF1>\" --resource \"<NAME2>=<DEF2>\" ... where NAMEi is a name (string ) of the i -th resource pool and DEFi is a definition of the i-th resource pool. You can define resource pools using one of the following formats: list(<VALUE0>,<VALUE1>,...,<VALUEN>) where VALUEi is a non-negative integer. This will create an indexed pool containing the specified values. range(<START>-<END>) where START and END are non-negative integers. This will create an indexed pool with numbers in the inclusive range [START, END] . sum(<SIZE>) where SIZE is a positive integer. This will create a sum pool with the given size. Tip You might encounter a problem in your shell when you try to specify worker resources, because the definition contains parentheses ( () ). In that case just wrap the resource definition in quotes, like this: $ hq worker start --resources \"foo=sum(5)\" Automatically detected resources # Nvidia GPUs that are available when a worker is started are automatically detected under the resource name gpus . You can use the environment variable CUDA_VISIBLE_DEVICES when starting a worker to override the list of available GPUs: $ CUDA_VISIBLE_DEVICES = 2 ,3 hq worker start Resource request # When you submit a job, you can define a resource requests with the --resource flag: $ hq submit --resource <NAME1> = <AMOUNT1> --resources <NAME2> = <AMOUNT2> ... Where NAME is a name of the requested resource and the AMOUNT is a positive integer defining the size of the request. Tasks with such resource requests will only be executed on workers that fulfill all the specified task requests. Important Notice that task resource requests always ask for an amount of resources required by a task, regardless whether that resource corresponds to an indexed or a sum pool on workers. For example, let's say that a worker has an indexed pool of GPUs: $ hq worker start --resource \"gpus=range(1-3)\" And we create two jobs, each with a single task. The first job wants 1 GPU, the second one wants two GPUs. $ hq submit --resource gpus = 1 ... $ hq submit --resource gpus = 2 ... Then the first job can be allocated e.g. the GPU 2 and the second job can be allocated the GPUs 1 and 3 . Resource environment variables # When a task that has resource requests is executed, the following variables are passed to it for each resource request named <NAME> : HQ_RESOURCE_REQUEST_<NAME> contains the amount of requested resources. HQ_RESOURCE_VALUES_<NAME> contains the specific resource values allocated for the task as a comma-separated list. This variable is only filled for indexed resource pool. Tip HQ has a special case for a resource named gpus . For that resource, it will also pass the following environment variables to the spawned task: CUDA_DEVICE_ORDER set to the value PCI_BUS_ID CUDA_VISIBLE_DEVICES set to the same value as HQ_RESOURCE_VALUES_gpus","title":"Generic Resources"},{"location":"jobs/gresources/#generic-resource-management","text":"Generic resource management serves for defining arbitrary resources provided by workers and also corresponding resource requests required by tasks. HyperQueue will take care of matching task resource requests so that only workers that can fulfill them will be able to execute such tasks. Some generic resources are automatically detected ; however, users may also define their own resources. Important Generic resources in HyperQueue exist on a purely logical level. They can correspond to physical things (like GPUs), but it is the responsibility of the user to make sure that this correspondence makes sense. HyperQueue by itself does not attach any semantics to generic resources, they are just numbers used for scheduling.","title":"Generic resource management"},{"location":"jobs/gresources/#worker-resources","text":"Each worker can have several generic resources attached. Each generic resource is a resource pool identified by a name. A resource pool represents some resources provided by a worker; each task can then ask for a part of resources contained in that pool. There are two kind of resource pools: Indexed pool : This pool represents an enumerated set of resources represented by integers. Each resource has its own identity. Tasks do not ask for specific values from the set, they just specify how many resources do they require and HyperQueue will allocate the specified amount of resources from the pool for each task. This pool is useful for resources that have their own identity, for example individual GPU or FPGA accelerators. HyperQueue guarantees that no individual resource from the indexed pool is allocated to more than a single task at any given time and that a task will not be executed on a worker if it does not currently have enough individual resources to fulfill the resource request of the task. Sum pool : This pool represents a resource that has a certain size which be split into individual tasks. A typical example is memory; if a worker has 2000 bytes of memory, it can serve e.g. four tasks, if each task asks for 500 bytes of memory. HyperQueue guarantees that the sum of resource request sizes of running tasks on a worker does not exceed the total size of the sum pool.","title":"Worker resources"},{"location":"jobs/gresources/#specifying-worker-resources","text":"You can specify the resource pools of a worker when you start it: $ hq worker start --resource \"<NAME1>=<DEF1>\" --resource \"<NAME2>=<DEF2>\" ... where NAMEi is a name (string ) of the i -th resource pool and DEFi is a definition of the i-th resource pool. You can define resource pools using one of the following formats: list(<VALUE0>,<VALUE1>,...,<VALUEN>) where VALUEi is a non-negative integer. This will create an indexed pool containing the specified values. range(<START>-<END>) where START and END are non-negative integers. This will create an indexed pool with numbers in the inclusive range [START, END] . sum(<SIZE>) where SIZE is a positive integer. This will create a sum pool with the given size. Tip You might encounter a problem in your shell when you try to specify worker resources, because the definition contains parentheses ( () ). In that case just wrap the resource definition in quotes, like this: $ hq worker start --resources \"foo=sum(5)\"","title":"Specifying worker resources"},{"location":"jobs/gresources/#automatically-detected-resources","text":"Nvidia GPUs that are available when a worker is started are automatically detected under the resource name gpus . You can use the environment variable CUDA_VISIBLE_DEVICES when starting a worker to override the list of available GPUs: $ CUDA_VISIBLE_DEVICES = 2 ,3 hq worker start","title":"Automatically detected resources"},{"location":"jobs/gresources/#resource-request","text":"When you submit a job, you can define a resource requests with the --resource flag: $ hq submit --resource <NAME1> = <AMOUNT1> --resources <NAME2> = <AMOUNT2> ... Where NAME is a name of the requested resource and the AMOUNT is a positive integer defining the size of the request. Tasks with such resource requests will only be executed on workers that fulfill all the specified task requests. Important Notice that task resource requests always ask for an amount of resources required by a task, regardless whether that resource corresponds to an indexed or a sum pool on workers. For example, let's say that a worker has an indexed pool of GPUs: $ hq worker start --resource \"gpus=range(1-3)\" And we create two jobs, each with a single task. The first job wants 1 GPU, the second one wants two GPUs. $ hq submit --resource gpus = 1 ... $ hq submit --resource gpus = 2 ... Then the first job can be allocated e.g. the GPU 2 and the second job can be allocated the GPUs 1 and 3 .","title":"Resource request"},{"location":"jobs/gresources/#resource-environment-variables","text":"When a task that has resource requests is executed, the following variables are passed to it for each resource request named <NAME> : HQ_RESOURCE_REQUEST_<NAME> contains the amount of requested resources. HQ_RESOURCE_VALUES_<NAME> contains the specific resource values allocated for the task as a comma-separated list. This variable is only filled for indexed resource pool. Tip HQ has a special case for a resource named gpus . For that resource, it will also pass the following environment variables to the spawned task: CUDA_DEVICE_ORDER set to the value PCI_BUS_ID CUDA_VISIBLE_DEVICES set to the same value as HQ_RESOURCE_VALUES_gpus","title":"Resource environment variables"},{"location":"jobs/jobs/","text":"The main unit of computation within HyperQueue is called a Task . It represents a single computation (currently, a single execution of some program) that is scheduled and executed on a worker. To actually compute something, you have to create a Job , which is a collection of tasks (a task graph). Jobs are units of computation management - you can submit, query or cancel jobs using the CLI. Note This section focuses on simple jobs , where each job contains exactly one task. See Task arrays to find out how to create jobs with multiple tasks. Identification numbers # Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 . Submitting jobs # To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below. Name # Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ... Working directory # By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path. Output # By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are %{SUBMIT_DIR}/job-%{JOB_ID}/%{TASK_ID}.stdout for stdout %{SUBMIT_DIR}/job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder . Environment variables # You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id Time management # You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes. Priority # You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 . Placeholders # You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by Available for %{JOB_ID} Job ID stdout , stderr , cwd , log %{TASK_ID} Task ID stdout , stderr , cwd %{INSTANCE_ID} Instance ID stdout , stderr , cwd %{SUBMIT_DIR} Directory from which the job was submitted. stdout , stderr , cwd , log %{CWD} Working directory of the task. stdout , stderr %{SERVER_UID} Server unique ID (a string of length 6)[^uid] stdout , stderr , cwd , log [^uid] Server generates a random SERVER_UID string every time a new server is started ( hq server start ). State # At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following command 2 : $ hq job info <job-id> Task state # Each task starts in the Waiting state and can end up in one of the terminal states: Finished , Failed or Canceled . Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Waiting The task was submitted and is now waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed . Job state # The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Failed , then job state is Failed . If at least one task is Canceled , then job state is Canceled . All tasks have to be Finished , therefore the job state will also be Finished . Cancelling jobs # You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq job cancel command 2 : $ hq job cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed. Waiting for jobs # There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq job wait command that can be used to wait until an existing job completes 2 : $ hq job wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq job progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq job progress <selector> Attaching standard input # When --stdin flag is used, HQ captures standard input and attaches it to each task of a job. When a task is started then the attached data is written into the standard input of the task. This can be used to submitting scripts without creating file. The following command will capture stdin and executes it in Bash $ hq submit --stdin bash If you want to parse #HQ directives from standard input, you can use --directives=stdin . Task directory # When a job is submitted with --task-dir then a temporary directory is created for each task and passed via environment variable HQ_TASK_DIR . This directory is automatically deleted when the task is completed (for any reason). Providing own error message # A task may pass its own error message into the HyperQueue. HyperQueue provides a filename via environment variable HQ_ERROR_FILENAME , if a task creates this file and terminates with a non-zero return code, then the content of this file is taken as an error message. HQ_ERROR_FILENAME is provided only if task directory is set on. The filename is always placed inside the task directory. If the message is longer than 2KiB, then it is truncated to 2KiB. If task terminates with zero return code, then the error file is ignored. Useful job commands # Here is a list of useful job commands: Display job table # List queued and running jobs List all jobs List jobs by status $ hq job list $ hq job list --all You can display only jobs having the selected states by using the --filter flag: $ hq job list --filter running,waiting Valid filter values are: waiting running finished failed canceled Display information about a specific job # $ hq job info <job-selector> Display information about individual tasks (potentially across multiple jobs) # $ hq task list <job-selector> [ --task-status <status> ] [ --tasks <task-selector> ] Display job stdout / stderr # $ hq job cat <job-id> [ --tasks <task-selector> ] <stdout/stderr> You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9","title":"Jobs and Tasks"},{"location":"jobs/jobs/#identification-numbers","text":"Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 .","title":"Identification numbers"},{"location":"jobs/jobs/#submitting-jobs","text":"To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below.","title":"Submitting jobs"},{"location":"jobs/jobs/#name","text":"Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ...","title":"Name"},{"location":"jobs/jobs/#working-directory","text":"By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path.","title":"Working directory"},{"location":"jobs/jobs/#output","text":"By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are %{SUBMIT_DIR}/job-%{JOB_ID}/%{TASK_ID}.stdout for stdout %{SUBMIT_DIR}/job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder .","title":"Output"},{"location":"jobs/jobs/#environment-variables","text":"You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id","title":"Environment variables"},{"location":"jobs/jobs/#time-management","text":"You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes.","title":"Time management"},{"location":"jobs/jobs/#priority","text":"You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 .","title":"Priority"},{"location":"jobs/jobs/#placeholders","text":"You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by Available for %{JOB_ID} Job ID stdout , stderr , cwd , log %{TASK_ID} Task ID stdout , stderr , cwd %{INSTANCE_ID} Instance ID stdout , stderr , cwd %{SUBMIT_DIR} Directory from which the job was submitted. stdout , stderr , cwd , log %{CWD} Working directory of the task. stdout , stderr %{SERVER_UID} Server unique ID (a string of length 6)[^uid] stdout , stderr , cwd , log [^uid] Server generates a random SERVER_UID string every time a new server is started ( hq server start ).","title":"Placeholders"},{"location":"jobs/jobs/#state","text":"At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following command 2 : $ hq job info <job-id>","title":"State"},{"location":"jobs/jobs/#task-state","text":"Each task starts in the Waiting state and can end up in one of the terminal states: Finished , Failed or Canceled . Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Waiting The task was submitted and is now waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed .","title":"Task state"},{"location":"jobs/jobs/#job-state","text":"The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Failed , then job state is Failed . If at least one task is Canceled , then job state is Canceled . All tasks have to be Finished , therefore the job state will also be Finished .","title":"Job state"},{"location":"jobs/jobs/#cancelling-jobs","text":"You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq job cancel command 2 : $ hq job cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed.","title":"Cancelling jobs"},{"location":"jobs/jobs/#waiting-for-jobs","text":"There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq job wait command that can be used to wait until an existing job completes 2 : $ hq job wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq job progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq job progress <selector>","title":"Waiting for jobs"},{"location":"jobs/jobs/#attaching-standard-input","text":"When --stdin flag is used, HQ captures standard input and attaches it to each task of a job. When a task is started then the attached data is written into the standard input of the task. This can be used to submitting scripts without creating file. The following command will capture stdin and executes it in Bash $ hq submit --stdin bash If you want to parse #HQ directives from standard input, you can use --directives=stdin .","title":"Attaching standard input"},{"location":"jobs/jobs/#task-directory","text":"When a job is submitted with --task-dir then a temporary directory is created for each task and passed via environment variable HQ_TASK_DIR . This directory is automatically deleted when the task is completed (for any reason).","title":"Task directory"},{"location":"jobs/jobs/#providing-own-error-message","text":"A task may pass its own error message into the HyperQueue. HyperQueue provides a filename via environment variable HQ_ERROR_FILENAME , if a task creates this file and terminates with a non-zero return code, then the content of this file is taken as an error message. HQ_ERROR_FILENAME is provided only if task directory is set on. The filename is always placed inside the task directory. If the message is longer than 2KiB, then it is truncated to 2KiB. If task terminates with zero return code, then the error file is ignored.","title":"Providing own error message"},{"location":"jobs/jobs/#useful-job-commands","text":"Here is a list of useful job commands:","title":"Useful job commands"},{"location":"jobs/jobs/#display-job-table","text":"List queued and running jobs List all jobs List jobs by status $ hq job list $ hq job list --all You can display only jobs having the selected states by using the --filter flag: $ hq job list --filter running,waiting Valid filter values are: waiting running finished failed canceled","title":"Display job table"},{"location":"jobs/jobs/#display-information-about-a-specific-job","text":"$ hq job info <job-selector>","title":"Display information about a specific job"},{"location":"jobs/jobs/#display-information-about-individual-tasks-potentially-across-multiple-jobs","text":"$ hq task list <job-selector> [ --task-status <status> ] [ --tasks <task-selector> ]","title":"Display information about individual tasks (potentially across multiple jobs)"},{"location":"jobs/jobs/#display-job-stdoutstderr","text":"$ hq job cat <job-id> [ --tasks <task-selector> ] <stdout/stderr> You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9","title":"Display job stdout/stderr"},{"location":"jobs/multinode/","text":"Warning Multi-node support is now in the experimental stage. The core functionality is working, but some features may be limited and quality of scheduling may vary. Also auto allocation feature is not yet fully prepared for multi-node tasks. Multi-node tasks are tasks that spreads across multiple nodes. Each node reserved for such task is exclusively reserved, i.e. no other tasks may run on such nodes. A job with multi-node task can be specified by --nodes=X option. An example of a job with multi-node task asking for 4 nodes: $ hq submit --nodes 4 test.sh When the task is started, four nodes are assigned to this task. One of them is chosen as \"root\" node where test.sh is started. Hostnames of all assigned nodes can be found in file which path is in environmental variable HQ_NODE_FILE . Each line is now host name. The first line is always the root node. Note: Multi-node tasks always enables task directory ( --task-dir ). Running MPI tasks # A script that starts an MPI program in multi-node task may look like as follows: #!/bin/sh mpirun --node-list = $HQ_NODE_FILE ./a-program","title":"Multinode Tasks"},{"location":"jobs/multinode/#running-mpi-tasks","text":"A script that starts an MPI program in multi-node task may look like as follows: #!/bin/sh mpirun --node-list = $HQ_NODE_FILE ./a-program","title":"Running MPI tasks"},{"location":"jobs/streaming/","text":"Jobs containing many tasks will generate a large amount of stdout and stderr files, which can be problematic, especially on network-based shared filesystems, such as Lustre. For example, when you submit the following task array: $ hq submit --array = 1 -10000 my-computation.sh 20000 files ( 10000 for stdout and 10000 for stderr) will be created on the disk. To avoid this situation, HyperQueue can optionally stream the stdout and stderr output of all tasks of a job over a network to the server, which will continuously append it to a single file called the Log . Note In this section, we refer to stdout and stderr as channels . Redirecting output to the log # You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path. Partial redirection # By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ... Guarantees # HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld. Superseded streams # When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default. Current limitations # The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten. Inspecting the log file # HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args> Log summary # You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary Printing log content # If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand. Log metadata # If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand. Exporting log # Log can be exported into JSON by the following command: $ hq log <log-file-path> export This prints the log file into a JSON format on standard output.","title":"Output Streaming"},{"location":"jobs/streaming/#redirecting-output-to-the-log","text":"You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path.","title":"Redirecting output to the log"},{"location":"jobs/streaming/#partial-redirection","text":"By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ...","title":"Partial redirection"},{"location":"jobs/streaming/#guarantees","text":"HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld.","title":"Guarantees"},{"location":"jobs/streaming/#superseded-streams","text":"When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default.","title":"Superseded streams"},{"location":"jobs/streaming/#current-limitations","text":"The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten.","title":"Current limitations"},{"location":"jobs/streaming/#inspecting-the-log-file","text":"HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args>","title":"Inspecting the log file"},{"location":"jobs/streaming/#log-summary","text":"You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary","title":"Log summary"},{"location":"jobs/streaming/#printing-log-content","text":"If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand.","title":"Printing log content"},{"location":"jobs/streaming/#log-metadata","text":"If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand.","title":"Log metadata"},{"location":"jobs/streaming/#exporting-log","text":"Log can be exported into JSON by the following command: $ hq log <log-file-path> export This prints the log file into a JSON format on standard output.","title":"Exporting log"}]}