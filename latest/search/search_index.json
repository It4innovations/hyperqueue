{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>HyperQueue is a tool designed to simplify execution of large workflows (task graphs) on HPC clusters. It allows you to execute a large number of tasks in a simple way, without having to manually submit jobs into batch schedulers like Slurm or PBS. You just specify what you want to compute \u2013 HyperQueue will automatically ask for computational resources and dynamically load-balance tasks across all allocated nodes and cores. HyperQueue can also work without Slurm/PBS as a general task executor.</p> <p>If you use HyperQueue in your research, please consider citing it.</p>"},{"location":"#useful-links","title":"Useful links","text":"<ul> <li>Installation</li> <li>Quick start</li> <li>Python API</li> <li>Command-line interface reference</li> <li>Repository</li> <li>Discussion forum</li> <li>Zulip (chat platform)</li> </ul>"},{"location":"#features","title":"Features","text":"<p>Resource management</p> <ul> <li>Batch jobs are submitted and managed automatically</li> <li>Computation is distributed amongst all allocated nodes and cores</li> <li>Tasks can specify complex resource requirements<ul> <li>Non-fungible resources (tasks are assigned specific resources, e.g. a GPU with ID <code>1</code>)</li> <li>Fractional resources (tasks can require e.g. <code>0.5</code> of a GPU)</li> <li>Resource variants (tasks can require e.g. <code>1 GPU and 4 CPU cores</code> OR <code>16 CPU cores</code>)</li> <li>Related resources (tasks can require e.g. <code>4 CPU cores in the same NUMA node</code>)</li> </ul> </li> </ul> <p>Performance</p> <ul> <li>Scales to millions of tasks and hundreds of nodes</li> <li>Overhead per task is around 0.1 ms</li> <li>Task output can be streamed to a single file to avoid overloading distributed filesystems</li> </ul> <p>Simple user interface</p> <ul> <li>Task graphs can be defined via a CLI, TOML workflow files or a Python API</li> <li>Cluster utilization can be monitored with a real-time dashboard</li> </ul> <p>Easy deployment</p> <ul> <li>Provided as a single, statically linked binary without any runtime dependencies</li> <li>No admin access to a cluster is needed for its usage</li> </ul>"},{"location":"changelog/","title":"Changelog","text":"<p>This page contains the historical record of changes in various version of HyperQueue. You can use the select box in the top left corner of the page to view the documentation of a specific HyperQueue version.</p>"},{"location":"changelog/#unreleased","title":"Unreleased","text":""},{"location":"changelog/#changes","title":"Changes","text":"<ul> <li>Allocation policy <code>compact</code> is update.   It still tries to find the minimal number of the resource groups,   but when they are found, resources are evenly taken from the minimal groups.   For old behavior, use new policy <code>tight</code>.</li> </ul>"},{"location":"changelog/#new-features","title":"New features","text":"<ul> <li>Implement resource \"coupling\".   You may specify that some resources are coupled, e.g. cpus and gpus.   That means that cpus are gpus are organized in numa nodes, and allocation strategy   will respect that, i.e., it tries to find cpus and gpus from the same numa nodes.   Note: The current implementation does not detect coupling automatically,   you have to specify it manually.</li> <li>New policy <code>tight</code> (and <code>tight!</code>) that is the original implementation of <code>compact</code>.   The policy <code>compact</code> now behaves as is described in the section \"Changes\".</li> <li>Resource policy <code>compact!</code> is now allowed to take fractional resource request.</li> <li>There is a new command <code>hq alloc cat &lt;alloc-id&gt; &lt;stdout/stderr&gt;</code>, which can be used   to debug the output of allocations submitted by the automatic allocator.</li> </ul>"},{"location":"changelog/#fixes","title":"Fixes","text":"<ul> <li>Fixed the issue of possible ignoring idle timeout when time request is used.</li> <li>Worker process terminated because of idle timeout now returns zero exit code.</li> <li>Fixes broken streaming when job file is used.</li> </ul>"},{"location":"changelog/#0230","title":"0.23.0","text":""},{"location":"changelog/#breaking-change","title":"Breaking change","text":"<ul> <li>In <code>--crash-limit</code> value 0 is no longer allowed, use <code>--crash-limit=unlimited</code>.</li> <li>The <code>--workers-per-alloc</code> flag of the <code>hq alloc add</code> command has been replaced with <code>--max-workers-per-alloc</code>,   which determines the maximum number of workers to spawn in each allocation. Previously, the flag caused the   allocator to (almost) always spawn the determined number of workers per allocation, regardless of actual   computational load.</li> </ul>"},{"location":"changelog/#changes_1","title":"Changes","text":"<p>The automatic allocator has been finally reimplemented, and is now much better:</p> <ul> <li>It now uses information from the scheduler to determine how many allocations to spawn, and thus it can react to the   current computational load much more accurately. It should also be less \"eager\".</li> <li>It properly supports multi-node tasks.</li> <li>It considers computational load across all allocation queues (before, each queue was treated separately, which led to   creating too many submissions).</li> <li>It now exposes a <code>min-utilization</code> parameter, which can be used to avoid spawning an allocation that couldn't be   utilized   enough.</li> </ul> <p>As this is a large behavioral change, we would be happy to hear your feedback!</p>"},{"location":"changelog/#new-features_1","title":"New features","text":"<ul> <li>New command <code>hq task explain &lt;job_id&gt; &lt;task_id&gt;</code> explains why a task cannot be run on a given worker.</li> <li>The server scheduler now slightly prioritizes tasks from older jobs and finishing partially-computed task graphs</li> <li>New values for <code>--crash-limit</code>:<ul> <li><code>never-restart</code> - task is never restarted, even if it \"crashes\" on a worker that was explicitly terminated.</li> <li><code>unlimited</code> - unlimited crash limit</li> </ul> </li> <li><code>hq worker info</code> contains more information</li> <li><code>hq job forget</code> tries to free more memory</li> <li>You can now configure Job name in the Python API.</li> <li><code>hq job progress</code> now displays all jobs and tasks that you wait for, rather than those that were unfinished at the   time when the command was executed.</li> </ul>"},{"location":"changelog/#fixes_1","title":"Fixes","text":"<ul> <li>Fixed a problem with journal loading when task dependencies are used</li> <li>Fixed restoring crash counters and instance ids from journal</li> <li>Fixed some corner cases of load balancing in server scheduler</li> </ul>"},{"location":"changelog/#docs","title":"Docs","text":"<ul> <li>CLI documentation (when <code>--help</code> is used) was cleaned up and improved</li> <li>Our documentation now contains an automatically generated reference of all available HQ CLI commands and options</li> <li>The <code>hq doc</code> and <code>hq generate-completion</code> commands have been documented</li> </ul>"},{"location":"changelog/#experimental","title":"Experimental","text":"<ul> <li>Added direct data transfers between tasks. User API not stabilized</li> </ul>"},{"location":"changelog/#0220","title":"0.22.0","text":""},{"location":"changelog/#new-features_2","title":"New features","text":"<ul> <li>Added <code>hq worker deploy-ssh</code> to deploy workers to a set of nodes using SSH.</li> <li>Added <code>hq doc</code> command for accessing documentation about various HQ features from the command-line.</li> <li><code>hq journal replay</code> added. It similar to <code>hq journal stream</code> but it will not wait for new events.</li> <li>More robust initialization of dashboard</li> <li>Authentication and encryption of client/worker connection can be disabled. It is mostly for testing   and benchmarking purpose. Do not use if you are not in 100% safe environment.</li> </ul>"},{"location":"changelog/#breaking-change_1","title":"Breaking change","text":"<ul> <li>The Python API now requires Python 3.9, up from Python 3.6.</li> </ul>"},{"location":"changelog/#fixes_2","title":"Fixes","text":"<ul> <li>Fixes #848, inefficient scheduling of tasks with priorities</li> <li>HyperQueue will no longer allocate extreme amounts of memory when loading a corrupted journal</li> </ul>"},{"location":"changelog/#v0211","title":"v0.21.1","text":""},{"location":"changelog/#fixes_3","title":"Fixes","text":"<ul> <li>Fixes random task crashes. Details in #823.</li> </ul>"},{"location":"changelog/#v0210","title":"v0.21.0","text":""},{"location":"changelog/#breaking-change_2","title":"Breaking change","text":"<ul> <li>Pre-built HyperQueue releases available from our GitHub repository are now built with GLIBC <code>2.28</code>, instead of <code>2.17</code>.   If you need to run HyperQueue on a system with an older GLIBC version, you might need to recompile it from source on   your system. If you encounter any issues, please let us know.</li> </ul>"},{"location":"changelog/#changes_2","title":"Changes","text":"<ul> <li><code>hq event-log</code> command renamed to <code>hq journal</code></li> <li><code>hq dashboard</code> has been re-enabled by default.</li> </ul>"},{"location":"changelog/#new-features_3","title":"New features","text":"<ul> <li>Added <code>hq journal prune</code> for pruning journal file.</li> <li>Added <code>hq journal flush</code> for forcing server to flush the journal.</li> </ul>"},{"location":"changelog/#v0200","title":"v0.20.0","text":""},{"location":"changelog/#new-features_4","title":"New features","text":"<ul> <li> <p>It is now possible to dynamically submit new tasks into an existing job (we call this concept \"Open jobs\").   See Open jobs documentation</p> </li> <li> <p>Worker streaming. Before, you could stream task stderr/stdout to the server over the network using the <code>--log</code>   parameter of <code>hq submit</code>.   This approach had various issues and was not scalable. Therefore, we have replaced this functionality with worker   streaming,   where the streaming of task output to a set of files on disk is performed by workers instead.   This new streaming approach creates more files than original solution (where it was always one file per job),   but the number of files stays small and independent on the number of executed tasks.   The new architecture also allows parallel I/O writing and storing of multiple job streams in one stream handle.   You can use worker streaming using the <code>--stream</code> parameter of <code>hq submit</code>. Check out the documentation for more   information.</p> </li> <li> <p>Optimization of journal size</p> </li> <li> <p>Tasks' crash counters are not increased when worker is stopped by <code>hq worker stop</code> or by time limit.</p> </li> </ul>"},{"location":"changelog/#removed","title":"Removed","text":"<ul> <li>Because worker streaming fully replaces original streaming, the original server streaming was removed.   For most cases, you can rename <code>--log</code> to <code>--stream</code> and <code>hq log</code> to <code>hq output-log</code>. See the docs for more details.</li> </ul>"},{"location":"changelog/#fixes_4","title":"Fixes","text":"<ul> <li>HQ should no longer crash while printing job info when a failed task does not have any workers   attached (https://github.com/It4innovations/hyperqueue/issues/731).</li> </ul>"},{"location":"changelog/#note","title":"Note","text":"<ul> <li>Dashboard still not enabled in this version</li> </ul>"},{"location":"changelog/#v0190","title":"v0.19.0","text":""},{"location":"changelog/#new-features_5","title":"New features","text":"<ul> <li> <p>Server resilience. Server state can be loaded back from a journal when it crashes. This will restore the state of   submitted jobs and also autoallocator queues. Find out   more here.</p> </li> <li> <p><code>HQ_NUM_NODES</code> for multi-node tasks introduced. It contains the number of nodes assigned to task.   You do not need to manually count lines in <code>HQ_NODE_FILE</code> anymore.</p> </li> </ul>"},{"location":"changelog/#changes_3","title":"Changes","text":"<ul> <li> <p>Dashboard is disabled in this version. We expect to reneeble it in 1-2 release cycles</p> </li> <li> <p>Node file generated for multi-node tasks now contains only short hostnames   (e.g. if hostname is \"cn690.karolina.it4i.cz\", only \"cn690\" is written into node list)   You can read <code>HQ_HOST_FILE</code> if you need to get full hostnames without stripping.</p> </li> </ul>"},{"location":"changelog/#fixes_5","title":"Fixes","text":"<ul> <li>Enable passing of empty <code>stdout</code>/<code>stderr</code> to Python function tasks in the Python   API (https://github.com/It4innovations/hyperqueue/issues/691).</li> <li><code>hq alloc add --name &lt;name&gt;</code> will now correctly use the passed <code>&lt;name&gt;</code> to name allocations submitted to Slurm/PBS.</li> </ul>"},{"location":"changelog/#v0180","title":"v0.18.0","text":""},{"location":"changelog/#breaking-change_3","title":"Breaking change","text":"<ul> <li> <p>Mechanism for resubmitting tasks was changed. Command <code>resubmit</code> was removed,   see https://it4innovations.github.io/hyperqueue/latest/jobs/failure/ for replacement.</p> </li> <li> <p>The output format of the <code>job info</code> command with JSON output mode has been changed. Note that   the JSON output mode is still unstable.</p> </li> </ul>"},{"location":"changelog/#new-features_6","title":"New features","text":"<ul> <li> <p>Combination of --time-request and --nodes is now allowed</p> </li> <li> <p>Allow setting a time request for a task (<code>min_time</code> resource value) using the Python API.</p> </li> <li> <p>Optimizations related to job submit &amp; long term memory saving</p> </li> <li> <p>The CLI dashboard is now enabled by default. You can try it with the <code>hq dashboard</code> command. Note that it is still   very experimental and a lot of useful features are missing.</p> </li> </ul>"},{"location":"changelog/#v0170","title":"v0.17.0","text":""},{"location":"changelog/#breaking-change_4","title":"Breaking change","text":""},{"location":"changelog/#memory-resource-in-megabytes","title":"Memory resource in megabytes","text":"<ul> <li>Automatically detected resource \"mem\" that is the size of RAM of a worker is now using megabytes as a unit.   i.e. <code>--resource mem=100</code> asks now for 100 MiB (previously 100 bytes).</li> </ul>"},{"location":"changelog/#new-features_7","title":"New features","text":""},{"location":"changelog/#non-integer-resource-requests","title":"Non-integer resource requests","text":"<ul> <li>You may now ask of non-integer amount of a resource. e.g. for 0.5 of GPU.   This enables resource sharing on the logical level of HyperQueue scheduler and allows to utilize remaining part the   resource   by another tasks.</li> </ul>"},{"location":"changelog/#job-submission","title":"Job submission","text":"<ul> <li>You can now specify <code>cleanup modes</code> when passing <code>stdout</code>/<code>stderr</code> paths to tasks. Cleanup mode decides what should   happen with the file once the task has finished executing. Currently, a single cleanup mode is implemented, which   removes   the file if the task has finished successfully:</li> </ul> <pre><code>$ hq submit --stdout=out.txt:rm-if-finished /my-program\n</code></pre>"},{"location":"changelog/#fixes_6","title":"Fixes","text":"<ul> <li>Fixed crash when task fails during its initialization</li> </ul>"},{"location":"changelog/#v0160","title":"v0.16.0","text":""},{"location":"changelog/#new-features_8","title":"New features","text":""},{"location":"changelog/#pregenerating-access-files","title":"Pregenerating access files","text":"<ul> <li>Via command <code>hq server generate-access</code> you can precreate an access file that can be later used for staring server,   and connecting workers, and clients. This is useful in cloud environments.</li> </ul>"},{"location":"changelog/#job-submission_1","title":"Job submission","text":"<ul> <li>A new command <code>hq job forget &lt;job-selector&gt;</code> has been introduced. It can be used to completely forget a job, and thus   reduce the memory usage of the HQ server. It is useful especially if you submit a large amount of jobs and keep the   server running for a long time.</li> </ul>"},{"location":"changelog/#automatic-allocation","title":"Automatic allocation","text":"<ul> <li>Autoalloc can now execute a custom shell command/script on each worker node before the worker starts and after the   worker stops. You can use this feature e.g. to initialize some data or load software modules for each worker node.</li> </ul> <p><pre><code>$ hq alloc add pbs --time-limit 30m \\\n  --worker-start-cmd \"/project/xxx/init-node.sh\" \\\n  --worker-stop-cmd \"/project/xxx/cleanup-node.sh\"\n</code></pre> * You can now set a time limit for workers spawned in allocations with the <code>--worker-time-limit</code> flag. You can use this   command to make workers stop sooner, so that you e.g. give more headroom for a <code>--worker-stop-cmd</code> command to execute   before the allocation is terminated. If you do not use this parameter, worker time limit will be set to the time limit   of the allocation.</p> <p>Example:   <pre><code>$ hq alloc add pbs --time-limit 1h --worker-time-limit 58m --worker-stop-cmd \"/project/xxxx/slow-command.sh\"\n</code></pre>   In this case, the allocation will run for one hour, but the HQ worker will be stopped after 58 minutes (unless it is   stopped sooner because of idle timeout). The worker stop command will thus have at least two minutes to execute.</p>"},{"location":"changelog/#changes_4","title":"Changes","text":""},{"location":"changelog/#access-file","title":"Access file","text":"<p>The format of the access file is changed. It is mostly internal change but you can experience parsing error when connecting an old client/worker to a new server (Connecting a new client/worker to an old server will given you a proper message).</p>"},{"location":"changelog/#v0150","title":"v0.15.0","text":""},{"location":"changelog/#breaking-changes","title":"Breaking changes","text":"<ul> <li>NVIDIA GPUs are now automatically detected under the resource name <code>gpus/nvidia</code>, instead of   just <code>gpus</code>! If you have been using the <code>gpus</code> resource name, you should update your scripts.   See more details below.</li> </ul>"},{"location":"changelog/#new-features_9","title":"New features","text":""},{"location":"changelog/#resource-management","title":"Resource management","text":"<ul> <li> <p>You can now specify more resources for one task, e.g.: 1 cpu and 1 gpu OR 4 cpus. The scheduler considers both   configurations in task planning.   For example let us assume that we have many tasks with the mentioned configuration and worker with 16 cpus and 4 gpus.   The tasks will fully utilize the node, 4 tasks will run in the configuration with gpu and 3 tasks will run in the cpu   only mode.</p> </li> <li> <p>Job Definition File is a TOML file that can define a job.   It allows to submit complex jobs without using Python API (dependencies, resource variants, ...).</p> </li> </ul> <pre><code>$ hq job submit-file myfile.toml\n</code></pre> <ul> <li>You can now specify (indexed) resource values provided by workers as strings (previously only   integers were allowed). Notably, automatic detection of Nvidia GPUs specified with string UUIDs   now works.</li> </ul> <pre><code>$ hq worker start --resource=\"res1=[foo, bar]\"\n</code></pre> <ul> <li> <p>HyperQueue now provides built-in support for AMD GPUs. For this reason, the default name of GPU   resources that are automatically detected on a worker has been changed from <code>gpus</code> to <code>gpus/nvidia</code>   for NVIDIA GPUs. AMD GPUs are now autodetected as <code>gpus/amd</code>. In the future, we intend to create a way   to ask for any GPU resource (e.g. <code>--resource=gpus=2</code>), regardless of its type.</p> </li> <li> <p>AMD GPUs are now automatically detected in workers from the environment variable <code>ROCR_VISIBLE_DEVICES</code>.</p> </li> <li> <p>Allowed characters for resource names has been changed. The name now has to begin with an ASCII letter,   and it can only contain ASCII letters, ASCII digits and the slash (<code>/</code>) symbol. This restriction is   introduced for better alignment with shells, which typically do not support complicated variable names.   HQ passes the resource names to executed tasks through environment variables, so it has to take this   into account. Note that the <code>/</code> symbol in resource name will be normalized to <code>_</code> when being passed   to a task.</p> </li> <li> <p><code>hq task info</code> now shows more information</p> </li> </ul>"},{"location":"changelog/#changes_5","title":"Changes","text":""},{"location":"changelog/#job-submission_2","title":"Job submission","text":"<ul> <li>The default path for <code>stdout</code> and <code>stderr</code> files has been changed   from <code>%{SUBMIT_DIR}/job-%{JOB_ID}/%{TASK_ID}.[stdout/stderr]</code>   to <code>%{CWD}/job-%{JOB_ID}/%{TASK_ID}.[stdout/stderr]</code>. Note that the default value for the working   directory (<code>%{CWD}</code>) is set to the submission directory, so if you have used the defaults before,   nothing will change for you. Stdout and stderr paths are now also resolved relative to the working   directory of the given task, not to the submit directory.</li> </ul>"},{"location":"changelog/#v0140","title":"v0.14.0","text":""},{"location":"changelog/#new-features_10","title":"New features","text":""},{"location":"changelog/#cli","title":"CLI","text":"<ul> <li>#545 Add a new command <code>hq job summary</code>,   which displays the amount of jobs per each job state.</li> </ul>"},{"location":"changelog/#platforms","title":"Platforms","text":"<ul> <li>HQ can be now compiled for Raspberry Pi</li> </ul>"},{"location":"changelog/#fixes_7","title":"Fixes","text":""},{"location":"changelog/#worker","title":"Worker","text":"<ul> <li>#539 Fix connection of worker to server   in the presence of both IPv4 and IPv6 addresses.</li> </ul>"},{"location":"changelog/#job-submission_3","title":"Job submission","text":"<ul> <li>#540 Parse all arguments from shebang   in a directives file (e.g. <code>#!/bin/bash -l</code>).</li> </ul>"},{"location":"changelog/#streaming","title":"Streaming","text":"<ul> <li>Fixed a bug in closing streaming when tasks are very short and synchronized.</li> </ul>"},{"location":"changelog/#v0130","title":"v0.13.0","text":""},{"location":"changelog/#new-features_11","title":"New features","text":""},{"location":"changelog/#resource-management_1","title":"Resource management","text":"<ul> <li> <p>Almost complete rewrite of resource management.   CPU and other resources were unified: the most visible change is that you can define \"cpus\" and other resource;   and other resources can now be defined in groups (NUMA-like resources).</p> </li> <li> <p>Many improvements in scheduler: Improved schedules for multi-resource requests;   better behavior on non-heterogeneous clusters;   better interaction between resources and priorities.</p> </li> </ul>"},{"location":"changelog/#automatic-allocation_1","title":"Automatic allocation","text":"<ul> <li>#467 You can now pause (and resume)   autoalloc queues using <code>hq alloc pause</code> and <code>hq alloc resume</code>.   Paused queues will not submit new allocations into the selected job manager. They can be later resumed.   When an autoalloc queue hits too many submission or worker execution errors, it will now be paused   instead of removed.</li> </ul>"},{"location":"changelog/#tasks","title":"Tasks","text":"<ul> <li> <p>HQ allows to limit how many times a task may be in a running state while worker is lost   (such a task may be a potential source of worker's crash).   If the limit is reached, the task is marked as failed.   The limit can be configured by <code>--crash-limit</code> in submit.</p> </li> <li> <p>Groups of workers are introduced. A multi-node task is now started only on workers from the same group.   By default, workers are grouped by PBS/Slurm allocations, but it can be configured manually.</p> </li> </ul>"},{"location":"changelog/#changes_6","title":"Changes","text":""},{"location":"changelog/#resource-management_2","title":"Resource management","text":"<ul> <li><code>--cpus=no-ht</code> is now changed to a flag <code>--no-hyper-threading</code>.</li> <li>Explicit list definition of a resource was changed from <code>--resource xxx=list(1,2,3)</code> to <code>--resource xxx=[1,2,3]</code>.   (this is the result of unification of CPUs with other resources).</li> <li>Python API: Attribute <code>generic</code> in <code>ResourceRequest</code> is renamed to <code>resources</code></li> </ul>"},{"location":"changelog/#tasks_1","title":"Tasks","text":"<ul> <li>#461 When a task is cancelled, times out   or its worker is killed, HyperQueue now tries to make sure that both the tasks and any processes that   it has spawned will be also terminated.</li> <li>#480 You can now select multiple tasks in <code>hq task info</code>.</li> </ul>"},{"location":"changelog/#v0120","title":"v0.12.0","text":""},{"location":"changelog/#new-features_12","title":"New features","text":""},{"location":"changelog/#automatic-allocation_2","title":"Automatic allocation","text":"<ul> <li>#457 You can now specify the idle timeout   for workers started by the automatic allocator using the <code>--idle-timeout</code> flag of the <code>hq alloc add</code> command.</li> </ul>"},{"location":"changelog/#resiliency","title":"Resiliency","text":"<ul> <li>#449 Tasks that were present during multiple   crashes of the workers will be canceled.</li> </ul>"},{"location":"changelog/#cli_1","title":"CLI","text":"<ul> <li>#463 You can now wait until <code>N</code> workers   are connected to the clusters with <code>hq worker wait N</code>.</li> </ul>"},{"location":"changelog/#python-api","title":"Python API","text":"<ul> <li>Resource requests improvements in Python API.</li> </ul>"},{"location":"changelog/#changes_7","title":"Changes","text":""},{"location":"changelog/#cli_2","title":"CLI","text":"<ul> <li> <p>#477 Requested resources are now shown while   submitting an <code>array</code> and while viewing information about task <code>TASK_ID</code> of specified   job <code>JOB_ID</code> using <code>hq task info JOB_ID TASK_ID</code></p> </li> <li> <p>#444 The <code>hq task list</code> command will now   hide some details by default, to conserve space in terminal output. To show all details, use the   <code>-v</code> flag to enable verbose output.</p> </li> <li> <p>#455 Improve the quality of error messages   produced when parsing various CLI parameters, like resources.</p> </li> </ul>"},{"location":"changelog/#automatic-allocation_3","title":"Automatic allocation","text":"<ul> <li>#448 The automatic allocator will now start   workers in multi-node Slurm allocations using <code>srun --overlap</code>. This should avoid taking up Slurm   task resources by the started workers (if possible). If you run into any issues with using <code>srun</code>   inside HyperQueue tasks, please let us know.</li> </ul>"},{"location":"changelog/#jobs","title":"Jobs","text":"<ul> <li>#483 There is no longer a length limit   for job names.</li> </ul>"},{"location":"changelog/#fixes_8","title":"Fixes","text":""},{"location":"changelog/#job-submission_4","title":"Job submission","text":"<ul> <li>#450 Attempts to resubmit a job with zero   tasks will now result in an explicit error, rather than a crash of the client.</li> </ul>"},{"location":"changelog/#automatic-allocation_4","title":"Automatic allocation","text":"<ul> <li>#494 Fixed a specific issue where the   auto allocator could submit more allocations than intended.</li> </ul>"},{"location":"changelog/#v0110","title":"v0.11.0","text":""},{"location":"changelog/#new-features_13","title":"New features","text":""},{"location":"changelog/#cli_3","title":"CLI","text":"<ul> <li> <p>#464 New command was added that allows users   to see more detailed info about selected task <code>TASK_ID</code> from a concrete job <code>JOB_ID</code>.     <pre><code>$ hq task info JOB_ID TASK_ID\n</code></pre></p> </li> <li> <p>#423 You can now specify the server   directory using the <code>HQ_SERVER_DIR</code> environment variable.</p> </li> </ul>"},{"location":"changelog/#resource-management_3","title":"Resource management","text":"<ul> <li>#427 A new specifier has been added to   specify indexed pool resources for workers as a set of individual resource indices.     <pre><code>$ hq worker start --resource \"gpus=list(1,3,8)\"\n</code></pre></li> <li>#428 Workers will now attempt to automatically   detect available GPU resources from the <code>CUDA_VISIBLE_DEVICES</code> environment variable.</li> </ul>"},{"location":"changelog/#stream-log","title":"Stream log","text":"<ul> <li>Basic export of stream log into JSON (<code>hq output-log &lt;log_file&gt; export</code>)</li> </ul>"},{"location":"changelog/#server","title":"Server","text":"<ul> <li> <p>Improved scheduling of multi-node tasks.</p> </li> <li> <p>Server now generates a random unique ID (UID) string every time a new server is started (<code>hq server start</code>).   It can be used as a placeholder <code>%{SERVER_ID}</code>.</p> </li> </ul>"},{"location":"changelog/#changes_8","title":"Changes","text":""},{"location":"changelog/#cli_4","title":"CLI","text":"<ul> <li>#464 More detailed task information (Time, Paths)   were moved from <code>hq task list</code> into <code>hq task info</code>.</li> <li>#433 (Backwards incompatible change)   The CLI command <code>hq job tasks</code> has been removed and its functionality has been incorporated into the   <code>hq task list</code> command instead.   resource requests,</li> <li>#420 Shebang (e.g. <code>#!/bin/bash</code>) will   now be read from submitted program based on the provided   directives mode. If a shebang   is found, HQ will execute the program located at the shebang path and pass it the rest of the   submitted arguments.</li> </ul> <p>By default, directives and shebang will be read from the submitted program only if its filename ends   with <code>.sh</code>. If you want to explicitly enable reading the shebang, pass <code>--directives=file</code> to   <code>hq submit</code>.</p> <p>Another change is that the shebang is now read by the client (i.e. it will be read on the node that   submits the job), not on worker nodes as previously. This means that the submitted file has to be   accessible on the client node.</p>"},{"location":"changelog/#resource-management_4","title":"Resource management","text":"<ul> <li>#427 (Backwards incompatible change)   The environment variable <code>HQ_RESOURCE_INDICES_&lt;resource-name&gt;</code>, which is passed to tasks with   resource requests,   has been renamed to <code>HQ_RESOURCE_VALUES_&lt;resource-name&gt;</code>.</li> <li> <p>#427 (Backwards incompatible change)   The specifier for specifying indexed pool resources for workers as a range has been renamed from   <code>indices</code> to <code>range</code>.</p> <p><pre><code># before\n$ hq worker start --resource \"gpus=indices(1-3)\"\n# now\n$ hq worker start --resource \"gpus=range(1-3)\"\n</code></pre> * #427 The   generic resource   documentation has been rewritten and improved.</p> </li> </ul>"},{"location":"changelog/#v0100","title":"v0.10.0","text":""},{"location":"changelog/#new-features_14","title":"New features","text":""},{"location":"changelog/#running-tasks","title":"Running tasks","text":"<ul> <li>HQ will now set the OpenMP <code>OMP_NUM_THREADS</code> environment variable for each task. The amount of threads   will be set according to the number of requested cores. For example, this job submission:</li> </ul> <pre><code>$ hq submit --cpus=4 -- &lt;program&gt;\n</code></pre> <p>would pass <code>OMP_NUM_THREADS=4</code> to the executed <code>&lt;program&gt;</code>.</p> <ul> <li> <p>New task OpenMP pinning mode was added. You can now use <code>--pin=omp</code> when submitting jobs. This   CPU pin mode will generate the corresponding <code>OMP_PLACES</code> and <code>OMP_PROC_BIND</code> environment variables   to make sure that OpenMP pins its threads to the exact cores allocated by HyperQueue.</p> </li> <li> <p>Preview version of multi-node tasks. You may submit multi-node task by <code>hq submit --nodes=X ...</code></p> </li> </ul>"},{"location":"changelog/#cli_5","title":"CLI","text":"<ul> <li>Less verbose log output by default. You can use \"--debug\" to turn on the old behavior.</li> </ul>"},{"location":"changelog/#changes_9","title":"Changes","text":""},{"location":"changelog/#scheduler","title":"Scheduler","text":"<ul> <li>When there is only a few tasks, scheduler tries to fit tasks on fewer workers.   Goal is to enable earlier stopping of workers because of idle timeout.</li> </ul>"},{"location":"changelog/#cli_6","title":"CLI","text":"<ul> <li>The <code>--pin</code> boolean option for submitting jobs has been changed to take a value. You can get the   original behaviour by specifying <code>--pin=taskset</code>.</li> </ul>"},{"location":"changelog/#fixes_9","title":"Fixes","text":""},{"location":"changelog/#automatic-allocation_5","title":"Automatic allocation","text":"<ul> <li>PBS/Slurm allocations using multiple workers will now correctly spawn a HyperQueue worker on all   allocated nodes.</li> </ul>"},{"location":"changelog/#v090","title":"v0.9.0","text":""},{"location":"changelog/#new-features_15","title":"New features","text":""},{"location":"changelog/#tasks_2","title":"Tasks","text":"<ul> <li> <p>Task may be started with a temporary directory that is automatically deleted when the task is finished.   (flag <code>--task-dir</code>).</p> </li> <li> <p>Task may provide its own error message by creating a file with name passed by environment variable   <code>HQ_ERROR_FILENAME</code>.</p> </li> </ul>"},{"location":"changelog/#cli_7","title":"CLI","text":"<ul> <li>You can now use the <code>hq task list &lt;job-selector&gt;</code> command to display a list of tasks across multiple jobs.</li> <li>Add <code>--filter</code> flag to <code>worker list</code> to allow filtering workers by their status.</li> </ul>"},{"location":"changelog/#changes_10","title":"Changes","text":""},{"location":"changelog/#automatic-allocation_6","title":"Automatic allocation","text":"<ul> <li>Automatic allocation has been rewritten from scratch. It will no longer query PBS/Slurm allocation   statuses periodically, instead it will try to derive allocation state from workers that connect   to it from allocations.</li> <li>When adding a new allocation queue, HyperQueue will now try to immediately submit a job into the queue   to quickly test whether the entered configuration is correct. If you want to avoid this behaviour, you   can use the <code>--no-dry-run</code> flag for <code>hq alloc add &lt;pbs/slurm&gt;</code>.</li> <li>If too many submissions (10) or running allocations (3) fail in a succession, the corresponding   allocation queue will be automatically removed to avoid error loops.</li> <li><code>hq alloc events</code> command has been removed.</li> <li>The <code>--max-kept-directories</code> parameter for allocation queues has been removed. HyperQueue will now keep   <code>20</code> last allocation directories amongst all allocation queues.</li> </ul>"},{"location":"changelog/#fixes_10","title":"Fixes","text":"<ul> <li>HQ will no longer warn that <code>stdout</code>/<code>stderr</code> path does not contain the <code>%{TASK_ID}</code> placeholder   when submitting array jobs if the placeholder is contained within the working directory path and   <code>stdout</code>/<code>stderr</code> contains the <code>%{CWD}</code> placeholder.</li> </ul>"},{"location":"changelog/#v080","title":"v0.8.0","text":""},{"location":"changelog/#fixes_11","title":"Fixes","text":""},{"location":"changelog/#automatic-allocation_7","title":"Automatic allocation","text":"<ul> <li>Issue #294: The automatic allocator   leaves behind directories of inactive (failed or finished) allocations on the filesystem. Although   these directories contain useful debugging information, creating too many of them can needlessly   waste disk space. To alleviate this, HyperQueue will now keep only the last <code>20</code> directories of   inactive allocations per each allocation queue and remove the older directories to save space.</li> </ul> <p>You can change this parameter by using the <code>--max-kept-directories</code> flag when creating an allocation   queue:</p> <pre><code>$ hq alloc add pbs --time-limit 1h --max-kept-directories 100\n</code></pre>"},{"location":"changelog/#new-features_16","title":"New features","text":""},{"location":"changelog/#jobs_1","title":"Jobs","text":"<ul> <li>Added new command for outputting <code>stdout</code>/<code>stderr</code> of jobs.</li> </ul> <pre><code># Print stdout of all tasks of job 1\n$ hq job cat 1 stdout\n\n# Print stderr of tasks 1, 2, 3 of job 5\n$ hq job cat 5 stderr --tasks 1-3\n</code></pre> <p>You can find more information in   the documentation * <code>#HQ</code> directives - You can now specify job parameters using a shell script passed to <code>hq submit</code>   by using HQ directives such as <code>#HQ --cpus=4</code>. This feature was inspired by similar functionality   that is present in e.g. PBS or Slurm. You can find more information in the   documentation.</p> <ul> <li> <p>HyperQueue will now attempt to parse shebang (like <code>#!/bin/bash</code>) if you provide a path to a   shell script (<code>.sh</code>) as the first command in <code>hq submit</code>. If the parsing is successful, HyperQueue   will use the parsed interpreter path to execute the shell script. In practice, this means that   you can now submit scripts beginning with a shebang like this:</p> <pre><code>$ hq submit script.sh\n</code></pre> </li> </ul> <p>This previously failed, unless you provided an interpreter, or provided a path starting with   <code>.</code> or an absolute path to the script.</p> <ul> <li>Capturing stdio and attaching it to each task of a job. This can be used to submitting scripts   without creating file. The following command will capture stdin and executes it in Bash</li> </ul> <pre><code>$ hq submit --stdin bash\n</code></pre>"},{"location":"changelog/#worker-configuration","title":"Worker configuration","text":"<ul> <li>You can now select what should happen when a worker loses its connection to the server using the   new <code>--on-worker-lost</code> flag available for <code>worker start</code> and <code>hq alloc add</code> commands. You can find   more information in   the documentation.</li> </ul>"},{"location":"changelog/#cli_8","title":"CLI","text":"<ul> <li> <p>You can now force HyperQueue commands to output machine-readable data using the <code>--output-mode</code> flag   available to all HyperQueue commands. Notably, you can output data of the commands as JSON. You can   find more information in the documentation.</p> </li> <li> <p>You can now generate shell completion using the <code>hq generate-completion &lt;shell&gt;</code> command.</p> </li> </ul>"},{"location":"changelog/#changes_11","title":"Changes","text":""},{"location":"changelog/#cli_9","title":"CLI","text":"<ul> <li>The command line interface for jobs has been changed to be more consistent with the interface for   workers. Commands that have been formerly standalone (like <code>hq jobs</code>, <code>hq resubmit</code>, <code>hq wait</code>) are   not accessed through <code>hq job</code>. The only previous job-related command that remained on the top level   is <code>hq submit</code>, which is now a shortcut for <code>hq job submit</code>. Here is a table of changed commands:</li> </ul> Previous command New command <code>hq jobs</code> <code>hq job list</code> <code>hq job</code> <code>hq job info</code> <code>hq resubmit</code> <code>hq job resubmit</code> <code>hq cancel</code> <code>hq job cancel</code> <code>hq wait</code> <code>hq job wait</code> <code>hq progress</code> <code>hq job progress</code> <code>hq submit</code> <code>hq submit</code> or <code>hq job submit</code> <ul> <li> <p>The <code>--tasks</code> flag of the <code>hq job info &lt;job-id&gt;</code> command has been removed. If you want to display the   individual tasks of a job, please use the <code>hq task list &lt;job-id&gt;</code> command.</p> </li> <li> <p>The command line parsing of <code>hq submit</code> has been changed slightly. All flags and arguments that appear   after the first positional argument will now be considered to belong to the executed program, not to   the submit command. This mimics the behaviour of e.g. <code>docker run</code>. For example:     <pre><code>$ hq submit foo --array 1-4\n# Before: submits a task array with 4 tasks that execute the program `foo`\n# Now: submits a single task that executes `foo --array 1-4`\n</code></pre></p> </li> <li> <p><code>hq job list</code> will now only show queued and running jobs by default. You can use the <code>--all</code> flag   to display all jobs or the <code>--filter</code> flag to filter jobs that are in specified states.</p> </li> <li> <p>The <code>--status</code> flag of <code>hq job resubmit</code> has been renamed to <code>--filter</code>.</p> </li> <li> <p>Tables outputted by various informational commands (like <code>hq job info</code> or <code>hq worker list</code>)   are now more densely packed and should thus better fit on terminal screens.</p> </li> </ul>"},{"location":"changelog/#preview-features","title":"Preview features","text":"<ul> <li>You can now store HyperQueue events into a log file and later export them to JSON for further   processing. You can find more information in the   documentation.</li> </ul> <p>Note that this functionality is quite low-level, and it's designed primarily for   tool builders that use HyperQueue programmatically, not regular users. It is also currently   unstable.</p> <ul> <li>You can now try the preview version of HQ dashboard. It can be started via:</li> </ul> <pre><code>$ hq dashboard\n</code></pre>"},{"location":"changelog/#v070","title":"v0.7.0","text":""},{"location":"changelog/#fixes_12","title":"Fixes","text":"<ul> <li> <p>Fixes an invalid behavior of the scheduler when resources are defined</p> </li> <li> <p>The automatic allocator will no longer keep submitting allocations in situations where the created   workers would not be able to execute currently waiting tasks. Currently, this situation is detected   only for the case when a task has a time request higher than the time limit of the allocation   queue.</p> </li> </ul>"},{"location":"changelog/#new-features_17","title":"New features","text":""},{"location":"changelog/#automatic-allocation_8","title":"Automatic allocation","text":"<ul> <li>You can now specify CPU and generic resources for workers created by the automatic allocator:     <pre><code>$ hq alloc add pbs --time-limit 2h --cpus 4x4 --resource \"gpu=indices(1-2)\" -- -q qexp -A Project1\n</code></pre></li> <li>You can now test auto allocation parameters using a dry-run command:     <pre><code>$ hq alloc dry-run pbs --time-limit 2h -- -q qexp -A Project1\n</code></pre>   Using this command you can quickly test if PBS/Slurm will accept allocations created with   the provided parameters.</li> <li>You can now specify a limit for the number of workers spawned inside a single allocation queue.   You can use the parameter <code>--max-worker-count</code> when creating a queue to make sure that the queue   will not create too many workers.     <pre><code>$ hq alloc add pbs --time-limit 00:10:00 --max-worker-count 10 -- -q qprod -A Project1\n</code></pre></li> <li>You can now specify the timelimit of PBS/Slurm allocations using the <code>HH:MM:SS</code> format:   <code>hq alloc add pbs --time-limit 01:10:30</code>.</li> </ul>"},{"location":"changelog/#resource-management_5","title":"Resource management","text":"<ul> <li>Workers can be now started with the parameter <code>--cpus=\"no-ht\"</code>. When detecting CPUs in this mode,   HyperThreading will be ignored (for each physical core only the first HT virtual core will be chosen).</li> <li>The user may explicitly specify what CPU IDs should be used by a worker   (including arrangement of IDs into sockets).   (E.g. <code>hq worker start --cpus=[[0, 1], [6, 8]]</code>)</li> </ul>"},{"location":"changelog/#cli_10","title":"CLI","text":"<ul> <li>Improve error messages printed when an invalid CLI parameter is entered.</li> </ul>"},{"location":"changelog/#changes_12","title":"Changes","text":"<ul> <li>The <code>--time-limit</code> parameter of <code>hq alloc add</code> command is now required.</li> <li><code>hq alloc remove</code> will no longer let you remove an allocation queue that contains running   allocations by default. If you want to force its removal and cancel the running allocations   immediately, use the <code>--force</code> flag.</li> </ul>"},{"location":"changelog/#v061","title":"v0.6.1","text":""},{"location":"changelog/#fixes_13","title":"Fixes","text":"<ul> <li>Fixed computation of worker load in scheduler</li> <li>Fixed performance problem when canceling more than 100k tasks</li> </ul>"},{"location":"changelog/#changes_13","title":"Changes","text":"<ul> <li>When a job is submitted, it does not show full details in response   but only a short message. Details can be still shown by <code>hq job &lt;id&gt;</code>.</li> </ul>"},{"location":"changelog/#v060","title":"v0.6.0","text":""},{"location":"changelog/#new-features_18","title":"New features","text":"<ul> <li>Generic resource management has been added. You can find out more in   the documentation.<ul> <li>HyperQueue can now automatically detect how many Nvidia GPUs are present on a worker node.</li> </ul> </li> <li>You can now submit a task array where each task will receive one element of a JSON array using   <code>hq submit --from-json</code>. You can find out more in   the documentation.</li> </ul>"},{"location":"changelog/#changes_14","title":"Changes","text":"<ul> <li>There have been a few slight CLI changes:<ul> <li><code>hq worker list</code> no longer has <code>--offline</code> and <code>--online</code> flags. It will now display only running   workers by default. If you want to show also offline workers, use the <code>--all</code> flag.</li> <li><code>hq alloc add</code> no longer has a required <code>--queue/--partition</code> option. The PBS queue/Slurm partition   should now be passed as a trailing argument after <code>--</code>: <code>hq alloc add pbs -- -qqprod</code>.</li> </ul> </li> <li>Server subdirectories generated for each run of the HyperQueue server are now named with a numeric ID instead of   a date.</li> <li>The documentation has been rewritten.</li> </ul>"},{"location":"changelog/#v050","title":"v0.5.0","text":""},{"location":"changelog/#new-features_19","title":"New features","text":"<ul> <li>Time limit and Time request for tasks (options <code>--time-limit</code> and <code>--time-request</code>)</li> <li>Time limit for workers</li> <li>Job and task times are shown in job information tables</li> <li>Integers in command line options can be now written with an underscore separator (e.g. <code>--array=1-1_000</code>)</li> <li>Placeholders in log file paths</li> <li>Preview version of PBS and SLURM auto allocation</li> <li>HyperQueue can be now compiled without <code>jemalloc</code> (this enables PowerPC builds).   To remove dependency on <code>jemalloc</code>, build HyperQueue with <code>--no-default-features</code>.</li> </ul>"},{"location":"changelog/#changes_15","title":"Changes","text":"<ul> <li><code>hq submit --wait</code> and <code>hq wait</code> will no longer display a progress bar while waiting for the job(s) to finish.   The progress bar was moved to <code>hq submit --progress</code> and <code>hq progress</code>.</li> <li>The default path of job stdout and stderr has been changed to <code>job-%{JOB_ID}/%{TASK_ID}.[stdout/stderr]</code></li> <li>Normalization of stream's end behavior when job is canceled</li> <li>Job id is now represented as u32</li> </ul>"},{"location":"changelog/#v040","title":"v0.4.0","text":""},{"location":"changelog/#new-features_20","title":"New features","text":"<ul> <li>Streaming - streaming stdout/stderr of all tasks in a job into one file   to avoid creating many files.</li> <li>Better reporting where job is running.</li> <li>Setting a priority via <code>hq submit --priority &lt;P&gt;</code></li> <li>Option <code>hq submit --wait ...</code> to wait until the submitted job finishes</li> <li>Command <code>hq wait &lt;id&gt; / all / last</code> to wait for a given job(s)</li> <li>Command <code>hq resubmit &lt;job-id&gt;</code> to resubmit a previous job</li> <li>Command <code>hq cancel all</code> / <code>hq cancel last</code> to cancel all jobs / last job</li> <li>Command <code>hq worker stop all</code> to cancel all workers</li> <li>Command <code>hq server info</code> to get an information about server</li> </ul>"},{"location":"changelog/#v030","title":"v0.3.0","text":""},{"location":"changelog/#new-features_21","title":"New features","text":"<ul> <li>Option for automatic closing workers without tasks (Idle timeout)</li> <li>Submit option <code>--max-fails X</code> to cancel an job when more than X tasks fails</li> <li>Submit option <code>--each-line FILE</code> to create a task per a line in a file.</li> <li>Submit option <code>--env VAR=VALUE</code> to specify env variable in a task</li> <li>Submit option <code>--cwd DIR</code> to specify a working dir of a task</li> <li>New placeholders in paths: <code>%{CWD}</code>, <code>%{DATE}</code>, and <code>%{SUBMIT_DIR}</code></li> <li>Added a progressbar in a job array detail.</li> <li><code>hq server start --host=xxx</code> allows to specify hostname/address under which the server is visible</li> </ul>"},{"location":"changelog/#v021","title":"v0.2.1","text":""},{"location":"changelog/#new-features_22","title":"New features","text":"<ul> <li>Filters for command <code>hq jobs &lt;filter&gt;</code>   (e.g. <code>hq jobs running</code>)</li> </ul>"},{"location":"changelog/#fixes_14","title":"Fixes","text":"<ul> <li>NUMA detection on some architectures</li> </ul>"},{"location":"changelog/#v020","title":"v0.2.0","text":""},{"location":"changelog/#new-features_23","title":"New features","text":"<ul> <li>Job arrays</li> <li>Cpu management</li> <li>--stdout/--stderr configuration in submit</li> </ul>"},{"location":"cheatsheet/","title":"Cheatsheet","text":"<p>Here you can find a cheatsheet with the most basic HQ commands. </p>"},{"location":"faq/","title":"FAQ","text":"<p>Here you can find a list of frequently asked questions about HyperQueue. If you'd like to ask about anything related to HyperQueue, feel free to ask on our discussion forum or on our Zulip server. </p>"},{"location":"faq/#hq-fundamentals","title":"HQ fundamentals","text":"How does HQ work? <p>You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs containing tasks to the server. You may have hundreds of thousands of tasks; they may have various CPUs and other resource requirements.</p> <p>Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign tasks to them.</p> <p>Workers are fully and dynamically controlled by server; you do not need to specify what tasks are executed on a particular worker or preconfigure it in any way.</p> <p>HQ provides a command line tool for submitting and controlling jobs.</p> <p><p> </p></p> What is a task in HQ? <p>Task is a unit of computation. Currently, it is either the execution of an arbitrary external program (specified via CLI) or the execution of a single Python function (specified via our Python API).</p> What is a job in HQ? <p>Job is a collection of tasks (a task graph). You can display and manage jobs using the CLI.</p> What operating systems does HQ support? <p>HyperQueue currently only officially supports Linux. It might be possible to compile it for other operating systems, however we do not provide any support nor promise to fix any bugs for other operating systems.</p> How to deploy HQ? <p>HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed.</p> How many jobs/tasks may I submit into HQ? <p>Our preliminary benchmarks show that the overhead of HQ is around 0.1 ms per task. It should be thus possible to submit a job with tens or hundreds of thousands tasks into HQ.</p> <p>Note that HQ is designed for a large number of tasks, not jobs. If you want to perform a lot of computations, use task arrays, i.e. create a job with many tasks, not many jobs each with a single task.</p> <p>HQ also supports streaming of task outputs into a single file. This avoids creating many small files for each task on a distributed filesystem, which improves scaling.</p> Does HQ support multi-CPU tasks? <p>Yes. You can define an arbitrary amount of CPUs for each task. HQ is also NUMA aware and you can select the NUMA allocation strategy.</p> Does HQ support job/task arrays? <p>Yes, see task arrays.</p> Does HQ support tasks with dependencies? <p>Yes, although it is currently only implemented in the Python API, which is experimental. It is currently not possible to specify dependencies using the CLI.</p> How is HQ implemented? <p>HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako, that is derived from our previous work RSDS. Integration tests are written in Python, but HQ itself does not depend on Python.</p>"},{"location":"faq/#relation-to-hpc-technologies","title":"Relation to HPC technologies","text":"Do I need to SLURM or PBS to run HQ? <p>No. Even though HQ is designed to work smoothly on systems using SLURM/PBS, they are not required in order for HQ to work.</p> Is HQ a replacement for SLURM or PBS? <p>Definitely not. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on an HPC system each user (or a group of users that trust each other) may run their own instance of HQ.</p> Do I need an HPC cluster to run HQ? <p>No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally on your personal computer.</p> Is it safe to run HQ on a login node shared by other users? <p>Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to that file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running.</p> <p>Performance should also not be a concern. Our experiments show that the server consumes only ~0.3ms of CPU time every second per a thousand tasks executed.</p>"},{"location":"faq/#relation-to-other-task-runtimes","title":"Relation to other task runtimes","text":"How does HQ differ from SnakeMake/Dask/Merlin/...? <p>You can find a comparison of HQ with similar tools here.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#binary-distribution-recommended","title":"Binary distribution (recommended)","text":"<p>The easiest way to install HyperQueue is to download and unpack the prebuilt <code>hq</code> executable:</p> <ol> <li> <p>Download the latest release archive from this link.</p> <p>Target architecture</p> <p>Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for <code>x86-64</code> and <code>PowerPC</code> architectures.</p> </li> <li> <p>Unpack the downloaded archive:</p> <pre><code>$ tar -xvzf hq-&lt;version&gt;-linux-&lt;architecture&gt;.tar.gz\n</code></pre> <p>The archive contains a single binary <code>hq</code>, which is used both for deploying the HQ cluster and submitting tasks into HQ. You can add <code>hq</code> to your system <code>$PATH</code> to make its usage easier.</p> </li> </ol> <p>See Quickstart for an example \"Hello world\" HyperQueue computation.</p>"},{"location":"installation/#compilation-from-source-code","title":"Compilation from source code","text":"<p>You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases.</p> <ol> <li>Setup a Rust toolchain</li> <li> <p>Clone the HyperQueue repository:</p> <pre><code>$ git clone https://github.com/It4innovations/hyperqueue/\n</code></pre> </li> <li> <p>Build HyperQueue:</p> <pre><code>$ RUSTFLAGS=\"-C target-cpu=native\" cargo build --release\n</code></pre> Jemalloc dependency <p>HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with <code>--no-default-features</code>:</p> <pre><code>$ cargo build --release --no-default-features\n</code></pre> </li> <li> <p>Use the executable located in <code>./target/release/hq</code></p> </li> </ol>"},{"location":"other-tools/","title":"Comparison with other task runtimes","text":"<p>There is a very large number of different task runtimes, with various performance characteristics, feature sets, programming models and trade-offs, and it is of course infeasible to compare HyperQueue with all of them. One of HyperQueue's authors has written a PhD thesis titled <code>Ergonomics and efficiency of workflows on HPC clusters</code>, which includes a section that compares HyperQueue with several other tools. We invite you to examine this section (and the whole thesis) if you want to find out more about the relation of HyperQueue to other task runtimes.</p> <p>The descriptions of other task runtimes presented on this page are actual as of October 2024.</p> <p>Below you can find a table<sup>1</sup>, which compares selected features of twelve task runtimes that we have experience with and/or that we think are relevant for HyperQueue. You can find more information about the table in Section 7.6 of the thesis.</p> <p></p> <p>Below we also provide opinionated<sup>2</sup> descriptions of selected task runtimes that we think can be reasonable compared with HyperQueue.</p> <ul> <li>GNU Parallel</li> <li>HyperShell</li> <li>Dask</li> <li>Ray</li> <li>Parsl</li> <li>PyCOMPSs</li> <li>Pegasus</li> <li>Balsam</li> <li>AutoSubmit</li> <li>FireWorks</li> <li>SnakeMake</li> <li>Merlin</li> </ul>"},{"location":"other-tools/#gnu-parallel","title":"GNU Parallel","text":"<p>GNU Parallel is a command-line utility for executing many tasks in parallel on a set of computational nodes. It does not offer many advanced task runtime features, but it does one thing well; it enables a parallelized and even distributed execution of a set of programs with a single command invocation. HyperQueue takes inspiration from this approach, as it offers a CLI that can be used to execute task graphs with many tasks and complex resource requirements with a single command.</p>"},{"location":"other-tools/#hypershell","title":"HyperShell","text":"<p>HyperShell is primarily designed for executing many homogeneous tasks using the command-line. It is pure Python and supports use as a Python library. It introduces several useful features on top of GNU Parallel, such as automatic task re-execution when a task fails, a database in-the-loop which enables users to observe the history of executed workflows, a user-defined tagging system, and simple autoscaling that allows for dynamic scaling. HyperShell also allows for task aggregation (which it refers to as bundling) for greater throughput of tasks in extreme scales; however, this is a fixed size for a given workflow which does not work well for collections of heterogeneous tasks. HyperShell also does not support task dependencies; therefore, it cannot be used to execute general task graphs.</p>"},{"location":"other-tools/#dask","title":"Dask","text":"<p>Dask is a task runtime that is very popular within the Python community, which allows executing arbitrary task graphs composed of Python functions on a distributed cluster. It also supports distributing code using <code>numpy</code> or <code>pandas</code> compatible API.</p> <p>While Dask by itself does not interact with PBS or Slurm, you can use Dask-JobQueue to make it operate in a similar fashion as HyperQueue - with the centralized server running on a login node and the workers running on compute nodes. Dask does not support arbitrary resource requirements and since it is written in Python, it can have problems with scaling to very large task graphs.</p> <p>If your use-case is primarily Python-based though, you should definitely give Dask a try, it's a great tool.</p>"},{"location":"other-tools/#ray","title":"Ray","text":"<p>Ray is a distributed task runtime primarily aimed at parallelizing the training and inference of machine learning models in Python. It uses a relatively unique architecture that leverages distributed scheduling; not all task submission and scheduling decisions need to go through a central location, unlike most other compared task runtimes including HyperQueue. This allows it to scale to an enormous amount of resources, millions of tasks and thousands of nodes. However, in order to enable this level of scalability, the workflow itself has to be implemented in a way where tasks submit new tasks from worker nodes dynamically. Therefore, batch computing use-cases that simply want to execute a predetermined workflow might be unable to achieve such high performance.</p> <p>Same as Dask, it offers basic resource requirements and it also supports fractional resources and related resource groups. However, it does not allow expressing multi-node tasks. In contrast to Dask, it is internally implemented in C++, which introduces much less overhead than Python. Even though Ray provides some autoscaling functionality, it does not support Slurm or other HPC allocation managers. In general, it is not specialized for HPC idiosyncrasies nor for executing arbitrary task graphs; even though it has a low-level interface for creating tasks through Python functions, it primarily focuses on generating task graphs automatically from high-level descriptions of machine learning pipelines, which are then executed e.g. on cloud resources.</p>"},{"location":"other-tools/#parsl","title":"Parsl","text":"<p>Parsl is another representative of a Python-oriented task runtime. It allows defining tasks that represent either Python function calls or command-line application invocations using Python. Computational resources in Parsl are configured through a block, a set of preconfigured resources (nodes) designed for executing specific kinds of tasks. In addition to blocks, users also have to specify launchers, which determine how will be each task executed (e.g. using a Slurm or an MPI execution command) and also an executor, which controls how will be tasks scheduled and batched into allocations and if the execution will be fault-tolerant. While these options let users specify how will be their task graph executed on a very granular level, it requires them to tune this configuration per task graph or target cluster; the configuration system is also relatively complex. This is in contrast to HyperQueue, which has a fully general resource management model that does not require users to configure anything; tasks are automatically load balanced across all available workers regardless of allocations and workers do not have to be preconfigured for specific tasks.</p> <p>Parsl has basic support for resource requirements, but does not allow creating custom user-specified resource kinds. It also allows specifying the number of nodes assigned to a task; however, such tasks have to be executed within a single block; Parsl does not allow executing multi-node tasks across different blocks or allocations.</p>"},{"location":"other-tools/#pycompss","title":"PyCOMPSs","text":"<p>PyCOMPSs is a Python interface for executing task graphs on top of the COMPSs distributed system. It allows defining arbitrary task graphs and has comprehensive support for multi-node tasks and basic resource requirements, but it does not allow users to define custom resource requirements. It was extended to support configuration of NUMA nodes for individual tasks. In terms of scheduling, it implements several simple scheduling algorithms; users can select which one should be used. Assignment of tasks to allocations is performed in a manual way; users enqueue a task graph (an application), which is then fully executed once that allocation is started.</p> <p>COMPSs provides basic support for automatic allocation that can dynamically react to computational load. However, it can only add or remove nodes from a primary allocation that is always tied to the execution of a single application; it does not provide fully flexible load balancing. PyCOMPSs is slightly more challenging to deploy than most of the other compared task runtimes, since it also requires a Java runtime environment in addition to a Python interpreter.</p>"},{"location":"other-tools/#pegasus","title":"Pegasus","text":"<p>Pegasus is a very general workflow management system that can execute %workflows on a wide range of clusters, from HPC to cloud. It provides support for various additional features that have not been examined in this thesis, such as data provenance or advanced file management and staging. Its workflows are usually defined using workflow files, which enable specifying dependencies both explicitly or by inferring them from input/output files of tasks. It also supports basic resource requirements, but does not allow defining custom resource kinds nor using multi-node tasks. By default, it maps each task to a single allocation, but it also allows users to cluster tasks together using one of several predefined modes. However, users have to configure this clustering manually; it is not performed fully automatically like in HyperQueue.</p> <p>In terms of deployment, it has the most complex set of runtime dependencies out of the compared task runtimes, as it requires not only a Python interpreter and a Java runtime environment, but also the HTCondor workload management system, which can be non-trivial to install on an HPC cluster. Pegasus delegates some of its functionality to HTCondor; it requires a configured instance of HTCondor before it can execute workflows on a cluster.</p>"},{"location":"other-tools/#balsam","title":"Balsam","text":"<p>Balsam is a task runtime for executing workflows defined using Python on HPC clusters. It uses a similar fully flexible method for mapping tasks to allocations as HyperQueue, including automatic allocation; however, it is limited to a single allocation queue, similarly as in Dask. It supports multi-node tasks, although users have to statically preconfigure workers to either execute single-node or multi-node tasks. It does not allow specifying custom resource kinds nor more advanced resource management offered by HyperQueue, such as resource variants. The Balsam server requires access to a PostgreSQL database instance, which makes its deployment slightly more challenging than some other tools that do not need a database or that can use an embedded database like SQLite.</p>"},{"location":"other-tools/#autosubmit","title":"AutoSubmit","text":"<p>AutoSubmit is a high-level tool for executing workflows and experiments. It focuses primarily on experiment tracking, data provenance and workflow automation. In its default mode, each task corresponds to a single allocation, which is not ideal for short running tasks; AutoSubmit is designed primarily for coarse-grained workflows. It provides a way to bundle multiple tasks into the same allocation using wrappers, but same as with e.g. Pegasus, this has to be preconfigured statically by the user; it is not performed automatically. AutoSubmit does not support custom task resource kinds and it also does not support direct data transfers between tasks nor output streaming.</p>"},{"location":"other-tools/#fireworks","title":"FireWorks","text":"<p>FireWorks is a workflow system for managing the execution of workflows on distributed clusters. It allows defining task graphs using either workflow files or through a Python API. It supports fault-tolerant task execution, although failed tasks have to be re-executed manually. FireWorks does not seem to support any task resource requirements; resources can only be configured for individual allocations. Its meta-scheduling approach is relatively complicated; it provides several ways of mapping tasks to allocations and individual workers with different trade-offs rather than providing a unified way that users would not have to worry about. FireWorks requires a MongoDB database to store tasks, which can make its deployment slightly challenging.</p>"},{"location":"other-tools/#snakemake","title":"SnakeMake","text":"<p>SnakeMake is a popular workflow management system for executing coarse-grained workflows defined using workflow files that can be extended with inline Python code. It can operate both as a meta-scheduler (outside of PBS/Slurm) and also as a classical task runtime within a PBS/Slurm job. Its workflows are based on files; tasks are expected to produce and consume files, which are also used to infer dependencies between them. This can pose an issue with a large number of tasks, as the created files can overload distributed filesystems; no output streaming is offered by the task runtime. It enables assigning both known (e.g. CPU or memory) and custom resource kinds to tasks. It also allows specifying the number of nodes required for each task.</p> <p>With SnakeMake, you can submit a workflow either using a task-per-job model (which has high overhead) or you can partition the workflow into several jobs, but in that case SnakeMake will not provide load balancing across these partitions, and partitioning the jobs manually can be quite arduous. HyperQueue allows you to submit large workflows without partitioning them manually in any way, as the server will dynamically load balance the tasks onto workers from different PBS/Slurm allocations.</p> <p>Since SnakeMake workflows are defined in configuration files, it's a bit more involved to run computations in SnakeMake than in HyperQueue. On the other hand, SnakeMake lets you define more complex workflows with improved traceability and reproducibility.</p>"},{"location":"other-tools/#merlin","title":"Merlin","text":"<p>Merlin is a task queueing system that enables execution of large workflows on HPC clusters. It leverages the Celery task queue for distributing tasks to workers and the Maestro workflow specification for defining task graphs. Tasks are submitted into separate Celery queues, whose resources need to be preconfigured; its load balancing is thus not fully flexible and automatic like in HyperQueue.</p> <p>It also does not support automatic allocation and nor does it support custom resource kinds. Failed tasks can be automatically restarted if they end with a specific status code; however, if they fail because of unexpected reasons, users have to mark them for re-execution manually. Merlin requires a message broker backend, such as RabbitMQ or Redis, for its functionality, which makes its deployment non-trivial.</p> <ol> <li> <p>It corresponds to Table 7.2 from the PhD thesis.\u00a0\u21a9</p> </li> <li> <p>If you think that our description is inaccurate or misleading, please file an issue.\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Here we provide an example of deploying HyperQueue on a local computer and running a simple \"Hello world\" script.</p> <p>Run each of the following three commands in separate terminals.</p> <ol> <li> <p>Start the HyperQueue server</p> <pre><code>$ hq server start\n</code></pre> <p>The server will manage computing resources (workers) and distribute submitted tasks amongst them.</p> </li> <li> <p>Start a HyperQueue worker</p> <pre><code>$ hq worker start\n</code></pre> <p>The worker will connect to the server and execute submitted tasks.</p> </li> <li> <p>Submit a simple computation</p> <pre><code>$ hq submit echo \"Hello world\"\n</code></pre> <p>This command will submit a job with a single task that will execute <code>echo \"Hello world\"</code> on a worker. You can find the output of the task in <code>job-1/0.stdout</code>.</p> </li> </ol> <p>That's it! For a more in-depth explanation of how HyperQueue works and what it can do, check the Deployment and Jobs sections.</p>"},{"location":"cli/dashboard/","title":"Dashboard","text":"<p>HyperQueue offers a command-line dashboard that shows information about the state of workers and jobs. It can show which jobs are currently queued or running, which tasks are running on which workers, or what is the current hardware utilization of workers.</p> <p>Warning</p> <p>The dashboard is currently in an experimental stage. Some of its features might not work properly, and important features might be missing. Please let us know if you encounter any issues with it, or if you want us to add new features to it.</p> <p>You can start the dashboard using the <code>hq dashboard</code> command: <pre><code>$ hq dashboard\n</code></pre> The dashboard will try to connect to a running HyperQueue server, and display various information. You can navigate the dashboard using your keyboard.</p> <p>Note</p> <p>You have to enable journalling in order to see any data in the dashboard.</p> <p>Here is an example video that shows how does the dashboard look like: </p>"},{"location":"cli/documentation/","title":"Documentation","text":"<p>You can use the <code>hq doc</code> command to show a URL to a page about a specific topic from this documentation. If you use the <code>--open</code> flag, HyperQueue will directly open a browser tab pointing to that page.</p> <p>The following documentation pages are currently supported:</p> <ul> <li><code>hq doc job</code>: Information about tasks and jobs.</li> <li><code>hq doc taskarray</code>: Information about task arrays.</li> <li><code>hq doc resources</code>: Information about resources.</li> <li><code>hq doc worker</code>: Information about workers.</li> <li><code>hq doc autoalloc</code>: Information about automatic allocation.</li> <li><code>hq doc python-api</code>: Information about the Python API.</li> <li><code>hq doc cheatsheet</code>: Link to the HQ cheatsheet.</li> <li><code>hq doc changelog</code>: Link to the HQ changelog.</li> <li><code>hq doc faq</code>: Link to frequently asked questions.</li> </ul>"},{"location":"cli/output-mode/","title":"Output mode","text":"<p>By default, HyperQueue CLI commands output information in a human-readable way, usually in the form of a table. If you want to use the CLI commands programmatically, HyperQueue offers two additional output modes that are designed to be machine-readable.</p> <p>You can change the output type of any HyperQueue CLI command either by using the <code>--output-mode</code> flag or by setting the <code>HQ_OUTPUT_MODE</code> environment variable.</p> FlagEnvironment variable <pre><code>$ hq --output-mode=json job list\n</code></pre> <pre><code>$ HQ_OUTPUT_MODE=json hq job list\n</code></pre> <p>Currently, there are three output modes available. The default, human-readable <code>cli</code> mode, and then two machine-readable modes, JSON and Quiet.</p> <p>Important</p> <p>Each machine-readable mode supports a set of commands. You can also use commands that are not listed here, but their output might be unstable, or they might not output anything for a given output mode.</p>"},{"location":"cli/output-mode/#json","title":"JSON","text":"<p>The <code>json</code> output mode is intended to provide very detailed information in the form of a JSON value. With this mode, HyperQueue will always output exactly one JSON value, either an array or an object.</p> <p>Warning</p> <p>The JSON output is currently unstable and can change with every new HyperQueue version. It can also be incomplete and might not contain all information outputted by the human-readable <code>cli</code> mode. The documentation below can be inaccurate, please examine source code for more details.</p>"},{"location":"cli/output-mode/#error-handling","title":"Error handling","text":"<p>When an error occurs during the execution of a command, the program will exit with exit code <code>1</code> and the program will output a JSON object with a single <code>error</code> key containing a human-readable description of the error.</p>"},{"location":"cli/output-mode/#date-formatting","title":"Date formatting","text":"<p>Time-based items are formatted in the following way:</p> <ul> <li>Duration - formatted as a floating point number of seconds.</li> <li>Datetime (timestamp) - formatted as a <code>ISO8601</code> date in UTC</li> </ul>"},{"location":"cli/output-mode/#supported-commands","title":"Supported commands","text":"<ul> <li> <p>Server info: <code>hq server info</code></p> Example <pre><code>{\n  \"host\": \"my-machine\",\n  \"hq_port\": 42189,\n  \"pid\": 32586,\n  \"server_dir\": \"/foo/bar/.hq-server\",\n  \"start_date\": \"2021-12-20T08:45:41.775753188Z\",\n  \"version\": \"0.7.0\",\n  \"worker_port\": 38627\n}\n</code></pre> </li> <li> <p>Worker list: <code>hq worker list</code></p> Example <pre><code>[{\n  \"configuration\": {\n    \"heartbeat_interval\": 8.0,\n    \"hostname\": \"my-machine\",\n    \"idle_timeout\": null,\n    \"listen_address\": \"my-machine:45611\",\n    \"log_dir\": \"...\",\n    \"resources\": {\n      \"cpus\": [[0, 1, 2, 3]],\n      \"generic\": [{\n        \"kind\": \"sum\",\n        \"name\": \"resource1\",\n        \"params\": {\n          \"size\": 1000\n        }\n      }]\n    },\n    \"time_limit\": null,\n    \"work_dir\": \"...\"\n  },\n  \"ended\": null,\n  \"id\": 1\n}]\n</code></pre> </li> <li> <p>Worker info: <code>hq worker info</code></p> Example <pre><code>{\n  \"configuration\": {\n    \"heartbeat_interval\": 8.0,\n    \"hostname\": \"my-machine\",\n    \"idle_timeout\": null,\n    \"listen_address\": \"my-machine:45611\",\n    \"log_dir\": \"...\",\n    \"resources\": {\n      \"cpus\": [[0, 1, 2, 3]],\n      \"generic\": [{\n        \"kind\": \"sum\",\n        \"name\": \"resource1\",\n        \"params\": {\n          \"size\": 1000\n        }\n      }]\n    },\n    \"time_limit\": null,\n    \"work_dir\": \"...\"\n  },\n  \"ended\": null,\n  \"id\": 1\n}\n</code></pre> </li> <li> <p>Submit a job: <code>hq submit</code></p> Example <pre><code>{\n  \"id\": 1\n}\n</code></pre> </li> <li> <p>Job list: <code>hq job list</code></p> Example <pre><code>[{\n  \"id\": 1,\n  \"name\": \"ls\",\n  \"resources\": {\n    \"cpus\": {\n      \"cpus\": 1,\n      \"type\": \"compact\"\n    },\n    \"generic\": [],\n      \"min_time\": 0.0\n    },\n  \"task_count\": 1,\n  \"task_stats\": {\n    \"canceled\": 0,\n    \"failed\": 0,\n    \"finished\": 1,\n    \"running\": 0,\n    \"waiting\": 0\n  }\n}]\n</code></pre> </li> <li> <p>Job info: <code>hq job info</code></p> Example <pre><code>{\n  \"finished_at\": \"2021-12-20T08:56:16.438062340Z\",\n  \"info\": {\n    \"id\": 1,\n    \"name\": \"ls\",\n    \"resources\": {\n      \"cpus\": {\n        \"cpus\": 1,\n        \"type\": \"compact\"\n      },\n      \"generic\": [],\n        \"min_time\": 0.0\n      },\n    \"task_count\": 1,\n    \"task_stats\": {\n      \"canceled\": 0,\n      \"failed\": 0,\n      \"finished\": 1,\n      \"running\": 0,\n      \"waiting\": 0\n    }\n  },\n  \"max_fails\": null,\n  \"pin\": null,\n  \"priority\": 0,\n  \"program\": {\n    \"args\": [\n      \"ls\"\n    ],\n    \"cwd\": \"%{SUBMIT_DIR}\",\n    \"env\": {\n      \"FOO\": \"BAR\"\n    },\n    \"stderr\": {\n      \"File\": \"job-%{JOB_ID}/%{TASK_ID}.stderr\"\n    },\n    \"stdout\": {\n      \"File\": \"job-%{JOB_ID}/%{TASK_ID}.stdout\"\n    }\n  },\n  \"started_at\": \"2021-12-20T08:45:53.458919345Z\",\n  \"tasks\": [{\n    \"finished_at\": \"2021-12-20T08:56:16.438062340Z\",\n    \"id\": 0,\n    \"started_at\": \"2021-12-20T08:56:16.437123396Z\",\n    \"state\": \"finished\",\n    \"worker\": 1,\n    \"cwd\": \"/tmp/foo\",\n    \"stderr\": {\n      \"File\": \"job-1/0.stderr\"\n    },\n    \"stdout\": {\n      \"File\": \"job-1/0.stdout\"\n    }\n  }],\n  \"time_limit\": null,\n  \"submit_dir\": \"/foo/bar/submit\"\n}\n</code></pre> </li> <li> <p>Automatic allocation queue list: <code>hq alloc list</code></p> Example <pre><code>[{\n  \"additional_args\": [],\n  \"backlog\": 4,\n  \"id\": 1,\n  \"manager\": \"PBS\",\n  \"max_worker_count\": null,\n  \"name\": null,\n  \"timelimit\": 1800.0,\n  \"worker_cpu_args\": null,\n  \"worker_resource_args\": [],\n  \"workers_per_alloc\": 1\n}]\n</code></pre> </li> <li> <p>Automatic allocation queue info: <code>hq alloc info</code></p> Example <pre><code>[{\n  \"id\": \"pbs-1\",\n  \"worker_count\": 4,\n  \"queue_at\": \"2021-12-20T08:56:16.437123396Z\",\n  \"started_at\": \"2021-12-20T08:58:25.538001256Z\",\n  \"ended_at\": null,\n  \"status\": \"running\",\n  \"workdir\": \"/foo/bar\"\n}]\n</code></pre> </li> </ul>"},{"location":"cli/output-mode/#quiet","title":"Quiet","text":"<p>The <code>quiet</code> output mode will cause HyperQueue to output only the most important information that should be parseable without any complex parsing logic, e.g. using only Bash scripts.</p>"},{"location":"cli/output-mode/#error-handling_1","title":"Error handling","text":"<p>When an error occurs during the execution of a command, the program will exit with exit code <code>1</code> and the error will be printed to the standard error output.</p>"},{"location":"cli/output-mode/#supported-commands_1","title":"Supported commands","text":"<ul> <li> <p>Submit a job: <code>hq submit</code></p> Schema <p>Outputs a single line containing the ID of the created job.</p> Example <pre><code>$ hq --output-mode=quiet submit ls\n1\n</code></pre> </li> </ul>"},{"location":"cli/shell-completion/","title":"Shell completion","text":"<p>You can use the <code>hq generate-completion &lt;shell&gt;</code> command to generate shell completions, so that your shell can auto-complete <code>hq</code> commands.</p> <p>Currently, the following shells are supported:</p> <ul> <li><code>bash</code></li> <li><code>elvish</code></li> <li><code>fish</code></li> <li><code>powershell</code></li> <li><code>zsh</code></li> </ul> <p>Note that you will need to store the generated completions into a certain file/directory on disk, so that your shell can find them. The location of that file is shell-specific.</p>"},{"location":"cli/shortcuts/","title":"Shortcuts","text":"<p>Various HyperQueue CLI command options let you enter some value in a specific syntactical format for convenience. Here you can find a list of such shortcuts.</p>"},{"location":"cli/shortcuts/#id-selector","title":"ID selector","text":"<p>When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID:</p> <ul> <li><code>&lt;id&gt;</code> Single ID<ul> <li><code>hq worker stop 1</code> - stop a worker with ID <code>1</code></li> <li><code>hq job cancel 5</code> - cancel a job with ID <code>5</code></li> </ul> </li> <li><code>&lt;start&gt;-&lt;end&gt;:&lt;step&gt;</code> Inclusive range of IDs, starting at <code>start</code> and ending at <code>end</code> with step <code>step</code><ul> <li><code>hq submit --array=1-10</code> - create a task array with <code>10</code> tasks</li> <li><code>hq worker stop 1-3</code> - stop workers with IDs <code>1</code>, <code>2</code> and <code>3</code></li> <li><code>hq job cancel 2-10:2</code> - cancel jobs with IDs <code>2</code>, <code>4</code>, <code>6</code>, <code>8</code> and <code>10</code></li> </ul> </li> <li><code>all</code> All valid IDs<ul> <li><code>hq worker stop all</code> - stop all workers</li> <li><code>hq job cancel all</code> - cancel all jobs</li> </ul> </li> <li><code>last</code> The most recently created ID<ul> <li><code>hq worker stop last</code> - stop most recently connected worker </li> <li><code>hq job cancel last</code> - cancel most recently submitted job</li> </ul> </li> </ul> <p>You can also combine the first two types of selectors with a comma. For example, the command</p> <pre><code>$ hq worker stop 1,3,5-8\n</code></pre> <p>would stop workers with IDs <code>1</code>, <code>3</code>, <code>5</code>, <code>6</code>, <code>7</code> and <code>8</code>.</p> <p>Tip</p> <p>You can add underscore (<code>_</code>) separators to any of the entered numeric values to improve readability:</p> <pre><code>$ hq submit --array=1-1000_000 ...\n</code></pre>"},{"location":"cli/shortcuts/#supported-commands-and-options","title":"Supported commands and options","text":"<ul> <li><code>hq submit --array=&lt;selector&gt;</code></li> <li><code>hq worker stop &lt;selector&gt;</code></li> <li><code>hq job info &lt;selector&gt;</code><ul> <li>does not support <code>all</code> (use <code>hq job list</code> instead)</li> </ul> </li> <li><code>hq job cancel &lt;selector&gt;</code></li> <li><code>hq job wait &lt;selector&gt;</code></li> <li><code>hq job progress &lt;selector&gt;</code></li> </ul>"},{"location":"cli/shortcuts/#duration","title":"Duration","text":"<p>You can enter durations using various time suffixes, for example:</p> <ul> <li><code>1h</code> - one hour</li> <li><code>3m</code> - three minutes</li> <li><code>14s</code> - fourteen seconds</li> <li><code>15days 2min 2s</code> - fifteen days, two minutes and two seconds</li> </ul> <p>You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here.</p>"},{"location":"cli/shortcuts/#supported-commands-and-options_1","title":"Supported commands and options","text":"<ul> <li><code>hq worker start --time-limit=&lt;duration&gt;</code></li> <li><code>hq worker start --idle-timeout=&lt;duration&gt;</code></li> <li><code>hq alloc add pbs --time-limit=&lt;duration&gt;</code></li> <li><code>hq submit --time-limit=&lt;duration&gt; ...</code></li> <li><code>hq submit --time-request=&lt;duration&gt; ...</code></li> </ul> <p>Tip</p> <p>For increased compatibility with <code>PBS</code> and <code>Slurm</code>, you can also specify the <code>--time-limit</code> option of <code>hq alloc add</code> using the <code>HH:MM:SS</code> format.</p>"},{"location":"deployment/","title":"Architecture","text":"<p>HyperQueue has two runtime components:</p> <ul> <li>Server: a long-lived component which can run e.g. on a login node of a computing cluster. It handles task   submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers.</li> <li>Worker: runs on a computing node and actually executes submitted tasks.</li> </ul> <p>Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system.</p> <p>Learn more about deploying server and the workers.</p> <p>There is also a third component that we call the client, which represents the users of HyperQueue invoking various <code>hq</code> commands to communicate with the server component.</p>"},{"location":"deployment/allocation/","title":"Automatic Allocation","text":"<p>Automatic allocation is one of the core features of HyperQueue. When you run HyperQueue on an HPC cluster, it allows you to autonomously ask the allocation manager (PBS/Slurm) for computing resources and spawn HyperQueue workers on the provided nodes.</p> <p>Using this mechanism, you can submit computations into HyperQueue without caring about the underlying PBS/Slurm allocations.</p> <p>Job vs allocation terminology</p> <p>It is common to use the term \"job\" for allocations created by an HPC allocation manager, such as PBS or Slurm, which are used to perform computations on HPC clusters. However, HyperQueue also uses the term \"job\" for task graphs.</p> <p>To differentiate between these two, we will refer to jobs created by PBS or Slurm as <code>allocations</code>. We will also refer to PBS/Slurm as an <code>allocation manager</code>.</p>"},{"location":"deployment/allocation/#allocation-queue","title":"Allocation queue","text":"<p>To enable automatic allocation, you have to create at least one <code>Allocation queue</code>. The queue describes a specific configuration that will be used by HyperQueue to request computing resources from the allocation manager on your behalf.</p> <p>Each allocation queue has a set of parameters. You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm.</p> <p>To create a new allocation queue, you can use the <code>hq alloc add</code> command. Any trailing arguments (passed after <code>--</code>) will be passed verbatim directly to <code>qsub</code>/<code>sbatch</code> when requesting a new allocation.</p> PBSSlurm <pre><code>$ hq alloc add pbs --time-limit 1h -- -qqprod -AAccount1\n</code></pre> <pre><code>$ hq alloc add slurm --time-limit 1h -- --partition=p1\n</code></pre> <p>Tip</p> <p>Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server (unless you use journaling).</p> <p>You have to specify the <code>--time-limit</code> parameter to tell HyperQueue the walltime of the requested allocations (the upper limit on their duration). The automatic allocator will automatically pass this walltime to PBS/Slurm, along with the number of nodes that should be spawned in each allocation. Other parameters, such as project credentials, account ID, queue/partition name or the number of requested CPU or GPU cores per node have to be specified manually by the user using the trailing arguments.</p> <p>Warning</p> <p>The corollary of the paragraph above is that you should not pass the number of nodes that should be allocated or the allocation walltime in the trailing arguments. Instead, you can specify various parameters that tell the automatic allocator how to configure these options.</p> <p>Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below. You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues.</p> <p>Warning</p> <p>Note that the HQ server needs to have access to <code>qsub</code> or <code>sbatch</code> binaries on the node where it is executed. If you want to submit PBS/Slurm allocations on a remote cluster, you will need to use e.g. a proxy to redirect the commands to that cluster. See this issue for more information. If you have a use-case for such remote PBS/Slurm allocation submission, please let us know, as we could try to make that easier in HyperQueue if there was enough interest in it.</p>"},{"location":"deployment/allocation/#parameters","title":"Parameters","text":"<p>In addition to arguments that are passed to <code>qsub</code>/<code>sbatch</code>, you can also use several other command line options when creating a new allocation queue.</p> <p>Tip</p> <p>We recommend you to specify the worker resources that will be available on workers spawned in the given allocation queue using the <code>--cpus</code> and <code>--resource</code> flags as precisely as possible. It will help the automatic allocator create the first allocation more accurately.</p>"},{"location":"deployment/allocation/#time-limit","title":"Time limit","text":"<ul> <li>Format<sup>1</sup>: <code>--time-limit &lt;duration&gt;</code></li> </ul> <p>Sets the walltime of created allocations.</p> <p>This parameter is required, as HyperQueue must know the duration of the individual allocations. Make sure that you pass a time limit that does not exceed the limit of the PBS/Slurm queue that you intend to use, otherwise the allocation submissions will fail. You can use the <code>dry-run</code> command to debug this.</p> <p>By default, workers will use a time limit equal to the time limit of the queue (unless overridden).</p> <p>Important</p> <p>If you specify a time request for a task, you should be aware that the time limit for the allocation queue should be larger than the time request if you want to run this task on workers created by this allocations queue, because it will always take some time before a worker is fully initialized. For example, if you set <code>--time-request 1h</code> when submitting a task, and <code>--time-limit 1h</code> when creating an allocation queue, this task will never get scheduled on workers from this queue, because the worker will get started with an actual time limit of e.g. <code>59m 58s</code>.</p>"},{"location":"deployment/allocation/#backlog","title":"Backlog","text":"<ul> <li>Format: <code>--backlog &lt;count&gt;</code></li> <li>Default: <code>1</code></li> </ul> <p>Maximum number of allocations that should be queued (waiting to be started) in PBS/Slurm at any given time. Has to be a positive integer.</p> <p>Note</p> <p>The backlog value does not limit the number of running allocations, only the number of queued allocations.</p> <p>Warning</p> <p>Do not set the <code>backlog</code> to a large number to avoid potentially overloading the allocation manager.</p>"},{"location":"deployment/allocation/#maximum-number-of-workers-per-allocation","title":"Maximum number of workers per allocation","text":"<ul> <li>Format: <code>--max-workers-per-alloc &lt;count&gt;</code></li> <li>Default: <code>1</code></li> </ul> <p>The maximum number of workers that will be requested in each allocation. Note that if there is not enough computational demand, the automatic allocator can create allocations with a smaller number of workers.</p> <p>Note</p> <p>If you use multi-node tasks, you will probably want to use this parameter to set the maximum number of workers in a queue to the size of your groups.</p> <p>Note that the number of workers always corresponds to the number of requested nodes, as the allocator always creates a single worker per node in a single allocation request.</p>"},{"location":"deployment/allocation/#maximum-worker-count","title":"Maximum worker count","text":"<ul> <li>Format: <code>--max-worker-count &lt;count&gt;</code></li> <li>Default: unlimited</li> </ul> <p>Maximum number of workers that can be queued or running across all allocations managed by the allocation queue. The total amount of workers will be usually limited by the manager (PBS/Slurm), but you can use this parameter to make the limit smaller, for example if you also want to manage allocations outside HyperQueue.</p>"},{"location":"deployment/allocation/#minimal-utilization","title":"Minimal utilization","text":"<ul> <li>Format: <code>--min-utilization &lt;ratio&gt;</code></li> <li>Default: <code>0.0</code></li> </ul> <p>Minimal utilization can be used to avoid creating allocations if there is not enough computational demand at the moment. If the scheduler thinks that it can currently make use of <code>N</code> of worker resources (on a range of <code>0.0 - 1.0</code>) in a single allocation of this queue, <code>min-utilization</code> has to be at least <code>N</code>, otherwise the allocation will not be created.</p> <p>It has to be a floating point number between 0.0 and 1.0.</p> <p>The default minimal utilization is <code>0</code>, which means that an allocation will be created if the scheduler thinks that it can use any (non-zero) amount of resources of worker(s) in the allocation.</p>"},{"location":"deployment/allocation/#worker-resources","title":"Worker resources","text":"<p>You can specify CPU and generic resources of workers spawned by the allocation queue. The name and syntax of these parameters is the same as when you create a worker manually:</p> PBSSlurm <pre><code>$ hq alloc add pbs --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- -qqprod -AAccount1\n</code></pre> <pre><code>$ hq alloc add slurm --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- --partition=p1\n</code></pre> <p>If you do not pass any resources, they will be detected automatically (same as it works with <code>hq worker start</code>).</p> <p>As noted above, we recommend you to set these resources as precisely as possible, if you know them.</p>"},{"location":"deployment/allocation/#idle-timeout","title":"Idle timeout","text":"<ul> <li>Format<sup>1</sup>: <code>--idle-timeout &lt;duration&gt;</code></li> <li>Default: <code>5m</code></li> </ul> <p>Sets the idle timeout for workers started by the allocation queue. We suggest that you do not use a long duration for this parameter, as it can result in wasting precious allocation time.</p>"},{"location":"deployment/allocation/#worker-start-command","title":"Worker start command","text":"<ul> <li>Format: <code>--worker-start-cmd &lt;cmd&gt;</code></li> </ul> <p>Specifies a shell command that will be executed on each allocated node just before a worker is started on that node. You can use it e.g. to initialize some shared environment for the node, or to load software modules.</p>"},{"location":"deployment/allocation/#worker-stop-command","title":"Worker stop command","text":"<ul> <li>Format: <code>--worker-stop-cmd &lt;cmd&gt;</code></li> </ul> <p>Specifies a shell command that will be executed on each allocated node just after the worker stops on that node. You can use it e.g. to clean up a previously initialized environment for the node.</p> <p>Warning</p> <p>The execution of this command is best-effort! It is not guaranteed that the command will always be executed. For example, PBS/Slurm can kill the allocation without giving HQ a chance to run the command.</p>"},{"location":"deployment/allocation/#worker-time-limit","title":"Worker time limit","text":"<ul> <li>Format<sup>1</sup>: <code>--worker-time-limit &lt;duration&gt;</code></li> </ul> <p>Sets the time limit of workers spawned by the allocation queue. After the time limit expires, the worker will be stopped. By default, the worker time limit is set to the time limit of the allocation queue. But if you want, you can shorten it with this flag to make the worker exit sooner, for example to give more time for a worker stop command to execute.</p> <p>Note</p> <p>This command is not designed to stop workers early if they have nothing to do. This functionality is provided by idle timeout.</p>"},{"location":"deployment/allocation/#name","title":"Name","text":"<ul> <li>Format: <code>--name &lt;name&gt;</code></li> </ul> <p>Name of the allocation queue. It will be used to name allocations submitted to the allocation manager. Serves for debug purposes only.</p>"},{"location":"deployment/allocation/#automatic-dry-run-submission","title":"Automatic dry-run submission","text":"<p>Once you create an allocation queue, HyperQueue will first automatically create a test \"dry run\" allocation, to make sure that PBS/Slurm accepts the arguments that you have passed to the <code>hq alloc add</code> command. This allocation will be created in a suspended mode and it will be then immediately cancelled, so that it does not consume any resources.</p> <p>You can disable this behavior using the <code>--no-dry-run</code> flag when running <code>hq alloc add</code>.</p> <p>You can also create a dry-run submission manually.</p>"},{"location":"deployment/allocation/#behavior","title":"Behavior","text":"<p>The (automatic) allocator submits allocations into PBS/Slurm based on current computational demand. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, so it will terminate if it doesn't receive any new tasks for five minutes.</p> <p>The allocator coordinates with the HyperQueue scheduler to figure out how many workers (and allocations) are needed to execute tasks that are currently waiting for computational resources. To determine that, the allocator needs to know the exact resources provided by workers that connect from allocations spawned by a given allocation queue. Because we cannot know these resources exactly until the first worker connects, the allocator can behave slightly differently before the first worker from a given allocation queue connects.</p> <p>For example, assume that you have specified that workers in a given queue will have <code>4</code> CPU cores (using <code>--cpus 4</code> when running <code>hq alloc add</code>). Here are a few scenarios for which we show how would the allocator behave:</p> <ul> <li>If we have waiting tasks that require <code>8</code> cores, no allocation will be submitted, because workers from this queue   definitely cannot compute these tasks.</li> <li>If we have waiting tasks that require <code>2</code> cores, the allocator knows that workers from this queue will be able to   compute them, so it will submit as many allocations are required to compute these tasks (based on backlog   and other limits).</li> <li>If we have waiting tasks that require <code>2</code> cores and <code>1</code> GPU, the allocator will first submit a single allocation,   because it is possible that the spawned worker will have a GPU (which just wasn't explicitly specified by the user in   <code>hq alloc add</code>). Then:<ul> <li>If the connected worker has a GPU available, the allocator will submit further allocations for these waiting   tasks.</li> <li>If the connected worker doesn't have a GPU available, the allocator will not submit further allocations to compute   tasks that require a GPU.</li> </ul> </li> </ul>"},{"location":"deployment/allocation/#rate-limits","title":"Rate limits","text":"<p>The allocator internally uses rate limiting to avoid overloading the PBS/Slurm allocation manager by spawning too many allocations too quickly.</p> <p>It also uses additional safety limits. If <code>10</code> allocations in a succession fail to be submitted or if <code>3</code> allocations that were submitted fail to start in a succession, the corresponding allocation queue will be automatically paused.</p>"},{"location":"deployment/allocation/#pausing-automatic-allocation","title":"Pausing automatic allocation","text":"<p>If you want to pause the submission of new allocations from a given allocation queue, without removing the queue completely, you can use the <code>hq alloc pause</code> command.</p> <p>If you later want to resume allocation submission, you can use the <code>hq alloc resume</code> command.</p>"},{"location":"deployment/allocation/#stopping-automatic-allocation","title":"Stopping automatic allocation","text":"<p>If you want to remove an allocation queue, use the <code>hq alloc remove</code> command:</p> <pre><code>$ hq alloc remove &lt;queue-id&gt;\n</code></pre> <p>When an allocation queue is removed, its queued and running allocations will be canceled immediately.</p> <p>By default, HQ will not allow you to remove an allocation queue that contains a running allocation. If you want to force its removal, use the <code>--force</code> flag.</p> <p>When the HQ server stops, it will automatically remove all allocation queues and cleanup all allocations.</p>"},{"location":"deployment/allocation/#debugging-automatic-allocation","title":"Debugging automatic allocation","text":"<p>Since the automatic allocator is a \"background\" process that interacts with an external allocation manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides a dry-run allocation test and also various sources of information that can help you figuring out what is going on.</p>"},{"location":"deployment/allocation/#dry-run-command","title":"Dry-run command","text":"<p>To test whether PBS/Slurm will accept the submit parameters that you provide to the auto allocator without creating an allocation queue, you can use the <code>hq alloc dry-run</code> command. It accepts the same parameters as <code>hq alloc add</code>, which it will use to immediately submit an allocation and print any encountered errors.</p> <pre><code>$ hq alloc dry-run pbs --timelimit 2h -- q qexp -A Project1\n</code></pre> <p>If the allocation was submitted successfully, it will be canceled immediately to avoid wasting resources.</p>"},{"location":"deployment/allocation/#finding-information-about-allocations","title":"Finding information about allocations","text":"<ul> <li><code>Basic queue information</code> This command will   show you details about allocations created by the automatic allocator.</li> <li> <p>Extended logging To get more information about what is happening inside the allocator, start the HyperQueue   server with the following environment variable:</p> <pre><code>$ RUST_LOG=hyperqueue::server::autoalloc=debug hq server start\n</code></pre> </li> </ul> <p>The log output of the server will then contain a detailed trace of allocator actions.</p> <ul> <li> <p>Allocation files Each time the allocator submits an allocation into the allocation manager, it will write the   submitted   bash script, allocation ID and <code>stdout</code> and <code>stderr</code> of the allocation to disk. You can find these files inside the   server directory:</p> <pre><code>$ ls &lt;hq-server-dir&gt;/hq-current/autoalloc/&lt;queue-id&gt;/&lt;allocation-num&gt;/\nstderr\nstdout\njob-id\nhq-submit.sh\n</code></pre> </li> </ul> <p>You can output the contents of the <code>stdout</code> and <code>stderr</code> files of an allocation using the <code>hq alloc log</code> command.</p>"},{"location":"deployment/allocation/#useful-autoalloc-commands","title":"Useful autoalloc commands","text":"<p>Below you can find a list of useful automatic allocation commands. The complete <code>hq alloc</code> CLI reference can be found here.</p>"},{"location":"deployment/allocation/#display-a-list-of-all-allocation-queues","title":"Display a list of all allocation queues","text":"<p>The <code>hq alloc list</code> command will show basic information about all allocation queues.</p> <pre><code>$ hq alloc list\n</code></pre>"},{"location":"deployment/allocation/#display-information-about-an-allocation-queue","title":"Display information about an allocation queue","text":"<p>The <code>hq alloc info</code> command will show allocations in the selected allocation queue.</p> <pre><code>$ hq alloc info &lt;queue-id&gt;\n</code></pre> <p>You can filter allocations by their state (<code>queued</code>, <code>running</code>, <code>finished</code>, <code>failed</code>) using the <code>--filter</code> option.</p>"},{"location":"deployment/allocation/#display-stdoutstderr-output-of-an-allocation","title":"Display stdout/stderr output of an allocation","text":"<p>The <code>hq alloc log</code> command will print the stdout/stderr output of a specific allocation. It can be used to debug allocation errors (e.g. some PBS/Slurm problems) or the output of workers spawned in allocations.</p> <pre><code>$ hq alloc log &lt;allocation-id&gt; &lt;stdout/stderr&gt;\n</code></pre> <ol> <li> <p>You can use various shortcuts for the duration value.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"deployment/cloud/","title":"Starting HQ without a shared filesystem","text":"<p>By default, HyperQueue assumes the existence of a shared filesystem, which it uses to exchange metadata required to connect servers and workers and to run various HQ commands.</p> <p>On systems without a shared filesystem, you will have to distribute an access file (<code>access.json</code>) to clients and workers. This file contains the address and port where the server is running, and also secret keys required for encrypted communication.</p>"},{"location":"deployment/cloud/#sharing-the-access-file","title":"Sharing the access file","text":"<p>After you start a server, you can find its <code>access.json</code> file in the <code>$HOME/.hq-server/hq-current</code> directory. You can then copy it to a different filesystem using a method of your choosing, and configure clients and workers to use that file.</p> <p>By default, clients and workers search for the <code>access.json</code> file in the <code>$HOME/.hq-server</code> directory, but you can override that using the <code>--server-dir</code> argument, which is available for all <code>hq</code> CLI commands. If you moved the <code>access.json</code> file into a directory called <code>/home/foo/hq-access</code> on the worker's node, you should start the worker like this:</p> <pre><code>$ hq --server-dir=/home/foo/hq-access worker start\n</code></pre> <p>Tip</p> <p>You can also configure the server directory using an environment variable.</p>"},{"location":"deployment/cloud/#generate-an-access-file-in-advance","title":"Generate an access file in advance","text":"<p>In some cases you might want to generate the access file in advance, before the server is started, and let the server, clients and workers use that access file. This can be useful so that you don't have to redistribute the access file to client/worker nodes everytime the server restarts, which could be cumbersome.</p> <p>To achieve this, an access file can be generated in advance using the <code>hq server generate-access</code> command:</p> <pre><code>$ hq server generate-access myaccess.json --client-port=6789 --worker-port=1234\n</code></pre> <p>This generates a <code>myaccess.json</code> file that contains generates keys and host information.</p> <p>The server can be later started with this configuration as follows:</p> <pre><code>$ hq server start --access-file=myaccess.json\n</code></pre> <p>Clients and workers should load the pre-generated access file in the same way as was described above. However, you will have to rename the generated file to <code>access.json</code>, because clients and workers look it up by its exact name in the provided server directory.</p> <p>Note</p> <p>The server will still generate and manages its \"own\" <code>access.json</code> in the server directory path, even if you provide your own access file. These files are the same, so you can use either when connectiong clients and workers.</p>"},{"location":"deployment/cloud/#splitting-access-for-client-and-workers","title":"Splitting access for client and workers","text":"<p>The default access file contains two secret keys and two TCP/IP addresses, one for clients and one for workers. This metadata can be divided into two separate files, containing only information needed only by clients or only by workers.</p> <pre><code>$ hq server generate-access full.json --client-file=client.json --worker-file=worker.json --client-port=6789 --worker-port=1234\n</code></pre> <p>This command creates three files: <code>full.json</code>, <code>client.json</code>, <code>worker.json</code>.</p> <p>For starting a client you can use <code>client.json</code> as <code>access.json</code> while it does not contain information for workers.</p> <p>For starting a worker you can use <code>worker.json</code> as <code>access.json</code> while it does not contain information for clients.</p> <p>For starting server (<code>hq server start --access-file=...</code>) you have to use <code>full.json</code> as it contains all necessary information.</p>"},{"location":"deployment/cloud/#setting-different-server-hostname-for-workers-and-clients","title":"Setting different server hostname for workers and clients","text":"<p>You can use the following command to configure different hostnames under which the server is visible to workers and clients.</p> <pre><code>hq server generate-access full.json --worker-host=&lt;WORKER_HOST&gt; --client-host=&lt;CLIENT_HOST&gt; ...\n</code></pre>"},{"location":"deployment/server/","title":"Server","text":"<p>The server is a crucial component of HyperQueue which manages workers and jobs. Before running any computations or deploying workers, you must first start the server.</p>"},{"location":"deployment/server/#starting-the-server","title":"Starting the server","text":"<p>The server can be started by running the <code>hq server start</code> command:</p> <pre><code>$ hq server start\n</code></pre> <p>You can change the hostname under which the server is visible to workers with the --host option:</p> <pre><code>$ hq server start --host=HOST\n</code></pre>"},{"location":"deployment/server/#server-directory","title":"Server directory","text":"<p>When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers. This directory is then used to select a running HyperQueue instance.</p> <p>By default, the server directory will be stored in <code>$HOME/.hq-server</code>. This location may be changed with the option <code>--server-dir=&lt;PATH&gt;</code>, which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories.</p> <p>If you use a non-default server directory, make sure to pass the same <code>--server-dir</code> to all HyperQueue commands that should use the selected HyperQueue server:</p> <pre><code>$ hq --server-dir=foo server start &amp;\n$ hq --server-dir=foo worker start\n</code></pre> <p>Tip</p> <p>To avoid having to pass the <code>--server-dir</code> parameter to all <code>hq</code> commands separately, you can also pass it through the<code>HQ_SERVER_DIR</code> environment variable, and export it to share it for all commands in the same terminal session: <pre><code>$ export HQ_SERVER_DIR=bar\n$ hq server start &amp;\n$ hq worker start &amp;\n</code></pre></p> <p>Server directory access</p> <p>Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server.</p>"},{"location":"deployment/server/#running-multiple-servers","title":"Running multiple servers","text":"<p>When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink <code>hq-current</code> which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs.</p>"},{"location":"deployment/server/#keeping-the-server-alive","title":"Keeping the server alive","text":"<p>The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed.</p> <p>For example, if you SSH into a login node of an HPC cluster and then run the server like this:</p> <pre><code>$ hq server start\n</code></pre> <p>The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux.</p>"},{"location":"deployment/server/#resuming-stoppedcrashed-server","title":"Resuming stopped/crashed server","text":"<p>The server supports resilience, which allows it to restore its state after it is stopped or if it crashes. To enable resilience, you can tell the server to log events into a journal file, using the <code>--journal</code> flag:</p> <pre><code>$ hq server start --journal /path/to/journal\n</code></pre> <p>If the server is stopped or it crashes, and you use the same command to start the server (using the same journal file path), it will continue from the last point:</p> <pre><code>$ hq server start --journal /path/to/journal\n</code></pre> <p>This functionality restores the state of jobs and automatic allocation queues. However, it does not restore worker connections; in the current version, new workers have to be connected to the server after it restarts.</p> <p>Warning</p> <p>If the server crashes, the last few seconds of progress may be lost. For example, when a task is finished and the server crashes before the journal is written, then after resuming the server, the task will be recomputed.</p>"},{"location":"deployment/server/#exporting-journal-events","title":"Exporting journal events","text":"<p>If you'd like to programmatically analyze events that are stored in the journal file, you can export them to JSON using the <code>hq journal export</code> command:</p> <pre><code>$ hq journal export &lt;journal-path&gt;\n</code></pre> <p>The events will be read from the provided journal and printed to <code>stdout</code> encoded in JSON, one event per line (this corresponds to line-delimited JSON, i.e. JSON Lines).</p> <p>You can also directly stream events in real-time from the server using the <code>hq journal stream</code> command:</p> <pre><code>$ hq journal stream\n</code></pre> <p>Warning</p> <p>The JSON format of the journal events and their definition is currently unstable and can change with a new HyperQueue version.</p>"},{"location":"deployment/server/#pruning-the-journal","title":"Pruning the journal","text":"<p>The <code>hq journal prune</code> command removes all completed jobs and disconnected workers from the journal file, in order to reduce its size on disk.</p>"},{"location":"deployment/server/#flushing-the-journal","title":"Flushing the journal","text":"<p>The <code>hq journal flush</code> command will force the server to flush the journal, so that the latest state of affairs is persisted to disk. It is mainly useful for testing or if you are going to run <code>hq journal export</code> while a server is running (however, it is usually better to use <code>hq journal stream</code>).</p>"},{"location":"deployment/server/#stopping-the-server","title":"Stopping the server","text":"<p>You can stop a running server with the <code>hq server stop</code> command:</p> <pre><code>$ hq server stop\n</code></pre> <p>When a server is stopped, all running jobs and connected workers will be immediately stopped.</p>"},{"location":"deployment/worker/","title":"Workers","text":"<p>Workers manage the computational resources of a single computer (node) and use them to execute tasks submitted into HyperQueue. They connect to a running instance of a HyperQueue server and wait for task assignments. Once some task is assigned to them, they will compute it and notify the server of its completion.</p>"},{"location":"deployment/worker/#starting-workers","title":"Starting workers","text":"<p>Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually.</p>"},{"location":"deployment/worker/#automatic-worker-deployment-recommended","title":"Automatic worker deployment (recommended)","text":"<p>If you are using an allocation manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation. It is a component of HyperQueue that takes care of submitting PBS/Slurm allocations and spawning HyperQueue workers.</p>"},{"location":"deployment/worker/#manual-worker-deployment","title":"Manual worker deployment","text":"<p>If you want to start a worker manually, you can use the <code>hq worker start</code> command:</p> <pre><code>$ hq worker start\n</code></pre> <p>Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it.</p> <p>By default, the worker will try to connect to a server using the default server directory. If you want to connect to a different server, use the <code>--server-dir</code> option.</p> <p>Sharing the server directory</p> <p>When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem.</p> <p>However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though. See this page for more details.</p>"},{"location":"deployment/worker/#deploying-a-worker-using-pbsslurm","title":"Deploying a worker using PBS/Slurm","text":"<p>If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command (<code>qsub</code> or <code>sbatch</code>) and run the <code>hq worker start</code> command inside the created allocation. If you want to start a worker on each allocated node, you can run this command on each node using e.g. <code>mpirun</code>.</p> <p>Example submission script:</p> PBSSlurm <pre><code>#!/bin/bash\n#PBS -q &lt;queue&gt;\n\n# Run a worker on the main node\n/&lt;path-to-hyperqueue&gt;/hq worker start --manager pbs\n\n# Run a worker on all allocated nodes\nml OpenMPI\npbsdsh /&lt;path-to-hyperqueue&gt;/hq worker start --manager pbs\n</code></pre> <pre><code>#!/bin/bash\n#SBATCH --partition &lt;partition&gt;\n\n# Run a worker on the main node\n/&lt;path-to-hyperqueue&gt;/hq worker start --manager slurm\n\n# Run a worker on all allocated nodes\nml OpenMPI\nsrun --overlap /&lt;path-to-hyperqueue&gt;/hq worker start --manager slurm\n</code></pre> <p>The worker will try to automatically detect that it is started under a PBS/Slurm allocation, but you can also explicitly pass the option <code>--manager &lt;pbs/slurm&gt;</code> to tell the worker that it should expect a specific environment.</p>"},{"location":"deployment/worker/#deploying-a-worker-using-ssh","title":"Deploying a worker using SSH","text":"<p>If you have an OpenSSH-compatible <code>ssh</code> binary available in your environment, HQ can deploy workers to a set of hostnames using the <code>hq worker deploy-ssh</code> command:</p> <pre><code>$ hq worker deploy-ssh &lt;nodefile&gt; &lt;worker-start-args&gt;\n</code></pre> <p>To use this command, you need to prepare a hostfile, which should contain a set of lines describing individual hostnames on which you want to deploy the workers: <pre><code>node1\nnode2:1234\n</code></pre></p> <p>As you can see above, each hostname can optionally have an attached SSH port (to change the default SSH port number <code>22</code>).</p> <p>Assume that the content above is stored in a file called <code>hostfile.txt</code>. If we then execute the following command: <pre><code>$ hq worker deploy-ssh hostfile.txt --time-limit 1h\n</code></pre> HQ would run <code>hq worker start --time-limit 1h</code> on both <code>node1</code> and <code>node2:1234</code>.</p> <p>The nodes have to be accessible using a passwordless SSH connection.</p> <p>You can also use the <code>--show-output</code> flag to display output of the workers. You need to pass this flag before the <code>&lt;nodefile&gt;</code> argument.</p>"},{"location":"deployment/worker/#stopping-workers","title":"Stopping workers","text":"<p>If you have started a worker manually, and you want to stop it, you can use the <code>hq worker stop</code> command<sup>1</sup>:</p> <pre><code>$ hq worker stop &lt;selector&gt;\n</code></pre>"},{"location":"deployment/worker/#time-limit","title":"Time limit","text":"<p>HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm allocations that have a limited duration.</p> <p>It is very useful for the workers to know how much remaining time (\"lifetime\") they have until they will be stopped. This duration is called the <code>Worker time limit</code>.</p> <p>When a worker is started manually inside a PBS or Slurm allocation, it will automatically calculate the time limit from the metadata of the allocation. If you want to set time limit for workers started outside of PBS/Slurm allocations or if you want to override the detected settings, you can use the <code>--time-limit=&lt;DURATION&gt;</code> option<sup>2</sup> when starting the worker.</p> <p>When the time limit is reached, the worker is automatically terminated.</p> <p>The time limit of a worker affects what tasks can be scheduled to it. For example, a task submitted with <code>--time-request 10m</code> will not be scheduled onto a worker that only has a remaining time limit of 5 minutes.</p>"},{"location":"deployment/worker/#idle-timeout","title":"Idle timeout","text":"<p>When you deploy HQ workers inside a PBS or Slurm allocation, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason.</p> <p>You can achieve this using <code>Worker idle timeout</code>. If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes.</p> <p>You can set the idle timeout using the <code>--idle-timeout</code> option<sup>2</sup> when starting the worker.</p> <p>Tip</p> <p>Workers started automatically have the idle timeout set to five minutes.</p> <p>Idle timeout can also be configured globally for all workers using the <code>--idle-timeout</code> option when starting a server:</p> <pre><code>$ hq server start --idle-timeout=&lt;TIMEOUT&gt;\n</code></pre> <p>This value will be then used for each worker that does not explicitly specify its own idle timeout.</p>"},{"location":"deployment/worker/#worker-state","title":"Worker state","text":"<p>Each worker can be in one of the following states:</p> <ul> <li>Running Worker is running and is able to process tasks</li> <li>Connection lost Worker lost connection to the server. Probably someone manually killed the worker or the walltime   of its PBS/Slurm allocation was reached.</li> <li>Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or   a hardware crash of the computational node.</li> <li>Stopped Worker was stopped.</li> <li>Idle timeout Worker was terminated due to Idle timeout.</li> </ul>"},{"location":"deployment/worker/#lost-connection-to-the-server","title":"Lost connection to the server","text":"<p>The behavior of what should happen when a worker loses its connection to the server is configured via <code>hq worker start --on-server-lost=&lt;policy&gt;</code>. You can select from two policies:</p> <ul> <li><code>stop</code> - The worker immediately terminates and kills all currently running tasks.</li> <li><code>finish-running</code> - The worker does not start executing any new tasks, but it tries to finish tasks   that are already running. When all such tasks finish, the worker will terminate.</li> </ul> <p><code>stop</code> is the default policy when a worker is manually started by <code>hq worker start</code>. When a worker is started by the automatic allocator, then <code>finish-running</code> is used as the default value.</p>"},{"location":"deployment/worker/#worker-groups","title":"Worker groups","text":"<p>Each worker is a member of exactly one worker group. Groups are used to determine which workers are eligible to execute multi-node tasks. You can find more information about worker groups here.</p>"},{"location":"deployment/worker/#useful-worker-commands","title":"Useful worker commands","text":"<p>Below you can find a list of useful worker commands. The complete <code>hq worker</code> CLI reference can be found here.</p>"},{"location":"deployment/worker/#display-worker-list","title":"Display worker list","text":"<p>The <code>hq worker list</code> command will display a list of workers that are currently connected to the server: <pre><code>$ hq worker list\n</code></pre></p> <p>If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the <code>--all</code> flag to the <code>list</code> command.</p>"},{"location":"deployment/worker/#display-worker-information","title":"Display worker information","text":"<p>You can display information about a specific worker using the <code>hq worker info</code> command:</p> <pre><code>$ hq worker info &lt;worker-id&gt;\n</code></pre>"},{"location":"deployment/worker/#detect-hardware-resources","title":"Detect hardware resources","text":"<p>You can test which hardware resources would a worker automatically detect on your current computer using the <code>hq worker hwdetect</code> command:</p> <pre><code>$ hq worker hwdetect\n</code></pre> <ol> <li> <p>You can use various shortcuts to select multiple workers at once.\u00a0\u21a9</p> </li> <li> <p>You can use various shortcuts for the duration value.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>Here you can find several examples of how HyperQueue can be used for various use-cases, both with the command-line interface and also with the Python API.</p> <ul> <li>Data arrays</li> <li>Iterative computation</li> </ul>"},{"location":"examples/data-arrays/","title":"Data arrays","text":"<p>Executing the same program for each item of an array of data is a perfect use-case for HyperQueue. It contains built-in support for generating a task array from a file containing a JSON array or from a file where each task input is specified on a separate line.</p>"},{"location":"examples/data-arrays/#processing-many-input-files-with-the-same-program","title":"Processing many input files with the same program","text":"<p>Let's say that we have a directory with 100 data files that we want to process using some program.</p> <p>First, we create an input file (called e.g. <code>inputs.txt</code>) that will store the filepaths of all these data files:</p> <pre><code>/data/input-01.txt\n/data/input-02.txt\n...\n</code></pre> <p>Then we create a bash script called e.g. <code>compute.sh</code> that will be executed by each HyperQueue task. Each such task will receive a single line from <code>inputs.txt</code> in the <code>HQ_ENTRY</code> environment variable. Our bash script will simply forward this line to a program of our choosing:</p> <pre><code>#!/bin/bash\n\n/home/user/my-program --param a=b --input ${HQ_ENTRY}\n</code></pre> <p>And finally, we can submit a task graph where a single task will be spawned for each line in the file above using the following command:</p> <pre><code>$ hq submit --each-line=inputs.txt ./compute.sh\n</code></pre> <p>If the <code>inputs.txt</code> file contained 100 lines, the command above would create a single job with 100 tasks.</p>"},{"location":"examples/iterative-computation/","title":"Iterative computation","text":"<p>It is a common use-case to perform an iterative computation, e.g. run a randomized simulation until the results are stable/accurate enough, or train a machine learning model while the loss keeps dropping.</p> <p>While there is currently no built-in support in HQ for iteratively submitting new tasks to an existing job, you can perform an iterative computation relatively easily with the following approach:</p> <ol> <li>Submit a HQ job that performs a computation</li> <li>Wait for the job to finish</li> <li>Read the output of the job and decide if computation should continue</li> <li>If yes, go to 1.</li> </ol>"},{"location":"examples/iterative-computation/#command-line-interface","title":"Command-line interface","text":"<p>With the command-line interface, you can perform the iterative loop e.g. in Bash.</p> <pre><code>#!/bin/bash\n\nwhile :\ndo\n  # Submit a job and wait for it to complete\n  ./hq submit --wait ./compute.sh\n\n  # Read the output of the job\n  output=$(./hq job cat last stdout)\n\n  # Decide if we should end or continue\n  if [ \"${output}\" -eq 0 ]; then\n      break\n  fi\ndone\n</code></pre>"},{"location":"examples/iterative-computation/#python-api","title":"Python API","text":"<p>With the Python API, we can simply write the outermost iteration loop in Python, and repeatedly submit jobs, until some end criterion has been achieved:</p> <pre><code>from hyperqueue import Job, Client\n\nclient = Client()\n\nwhile True:\n    job = Job()\n    job.program([\"my-program\"], stdout=\"out.txt\")\n\n    # Submit a job\n    submitted = client.submit(job)\n\n    # Wait for it to complete\n    client.wait_for_jobs([submitted])\n\n    # Read the output of the job\n    with open(\"out.txt\") as f:\n        # Check some termination condition and eventually end the loop\n        if f.read().strip() == \"done\":\n            break\n</code></pre>"},{"location":"jobs/arrays/","title":"Task Arrays","text":"<p>It is a common use case to execute the same command for multiple input parameters, for example:</p> <ul> <li>Perform a simulation for each input file in a directory or for each line in a CSV file.</li> <li>Train many machine learning models using hyperparameter search for each model configuration.</li> </ul> <p>HyperQueue allows you to do this using a job that contains many tasks. We call such jobs Task arrays. You can create a task array with a single <code>hq submit</code> command and then manage all created tasks as a single group using its containing job.</p> <p>Note</p> <p>Task arrays are somewhat similar to \"job arrays\" used by PBS and Slurm. However, HQ does not use PBS/Slurm job arrays for implementing this feature. Therefore, the limits that are commonly enforced on job arrays on HPC clusters do not apply to HyperQueue task arrays.</p>"},{"location":"jobs/arrays/#creating-task-arrays","title":"Creating task arrays","text":"<p>To create a task array, you must provide some source that will determine how many tasks should be created and what inputs (environment variables) should be passed to each task so that you can differentiate them.</p> <p>Currently, you can create a task array from a range of integers, from each line of a text file or from each item of a JSON array. You cannot combine these sources, as they are mutually exclusive.</p> <p>Handling many output files</p> <p>By default, each task in a task array will create two output files (containing <code>stdout</code> and <code>stderr</code> output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming.</p>"},{"location":"jobs/arrays/#integer-range","title":"Integer range","text":"<p>The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the <code>HQ_TASK_ID</code> environment variable.</p> <p>You can enter the range as two unsigned numbers separated by a dash<sup>1</sup>, where the first number should be smaller than the second one. The range is inclusive.</p> <p>The range is entered using the <code>--array</code> option:</p> <pre><code># Task array with 3 tasks, with ids 1, 2, 3\n$ hq submit --array 1-3 ...\n\n# Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10\n$ hq submit --array 0-10:2 ...\n</code></pre>"},{"location":"jobs/arrays/#lines-of-a-file","title":"Lines of a file","text":"<p>Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable <code>HQ_ENTRY</code>.</p> <p>This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the <code>--each-line</code> option:</p> <pre><code>$ hq submit --each-line entries.txt ...\n</code></pre> <p>Tip</p> <p>To directly use an environment variable in the submitted command, you have to make sure that it will be expanded when the command is executed, not when the command is submitted. You should also execute the command in a bash script if you want to specify it directly and not via a script file.</p> <p>For example, the following command is incorrect, as it will expand <code>HQ_ENTRY</code> during submission (probably to an empty string) and submit a command <code>ls</code>: <pre><code>$ hq submit --each-line files.txt ls $HQ_ENTRY\n</code></pre> To actually submit the command <code>ls $HQ_ENTRY</code>, you can e.g. wrap the command in apostrophes and run it in a shell: <pre><code>$ hq submit --each-line files.txt bash -c 'ls $HQ_ENTRY'\n</code></pre></p>"},{"location":"jobs/arrays/#json-array","title":"JSON array","text":"<p>You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable <code>HQ_ENTRY</code>.</p> <p>Note</p> <p>The root JSON value stored inside the file must be an array.</p> <p>You can create a task array in this way using the <code>--from-json</code> option:</p> <pre><code>$ hq submit --from-json items.json ...\n</code></pre> <p>If <code>items.json</code> contained this content: <pre><code>[{\n  \"batch_size\": 4,\n  \"learning_rate\": 0.01\n}, {\n  \"batch_size\": 8,\n  \"learning_rate\": 0.001\n}]\n</code></pre> then HyperQueue would create two tasks, one with <code>HQ_ENTRY</code> set to <code>{\"batch_size\": 4, \"learning_rate\": 0.01}</code> and the other with <code>HQ_ENTRY</code> set to <code>{\"batch_size\": 8, \"learning_rate\": 0.001}</code>.</p>"},{"location":"jobs/arrays/#combining-with-each-line-from-json-with-array","title":"Combining with <code>--each-line</code>/<code>--from-json</code> with <code>--array</code>","text":"<p>Option <code>--each-line</code> or <code>--from-json</code> can be combined with option <code>--array</code>. In such case, only a subset of lines/json will be submitted. If <code>--array</code> defines an ID that exceeds the number of lines in the file (or the number of elements in JSON), then the ID is silently removed.</p> <p>For example:</p> <pre><code>$ hq submit --each-line input.txt --array \"2, 8-10\"\n</code></pre> <p>If <code>input.txt</code> has sufficiently many lines then it will create array job with four tasks. One for 3rd line of file and three tasks for 9th-11th line (note that first line has id 0). It analogously works for <code>--from-json</code>.</p> <ol> <li> <p>The full syntax can be seen in the second selector of the ID selector shortcut.\u00a0\u21a9</p> </li> </ol>"},{"location":"jobs/cresources/","title":"CPU resource management","text":"<p>Note</p> <p>In this text, we use the term CPU for a resource that is provided by the operating system (e.g. what you get from <code>/proc/cpuinfo</code>). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use the term socket to refer to physical CPUs.</p>"},{"location":"jobs/cresources/#brief-introduction","title":"Brief introduction","text":"<p>HyperQueue allows you to select how many CPU cores will be allocated for each task. By default, each task requires a single CPU of the worker's node. This can be changed by the flag <code>--cpus</code>.</p> <p>For example, to submit a job with a task that requires 8 CPUs:</p> <pre><code>$ hq submit --cpus=8 &lt;program_name&gt; &lt;args...&gt;\n</code></pre> <p>This ensures that HyperQueue will exclusively reserve 8 CPUs for this task when it is started. This task would thus never be scheduled on a worker that has less than 8 CPUs.</p> <p>Note that this reservation exists on a logical level only. To ensure more direct mapping to physical cores, see pinning below.</p>"},{"location":"jobs/cresources/#cpus-are-a-resource","title":"CPUs are a resource","text":"<p>From version 0.13.0, CPUs are managed as any other resource under name \"cpus\", with the following additions:</p> <ul> <li> <p>If a task does not explicitly specify the number of cpus, then it requests 1 CPU as default.</p> </li> <li> <p>CPUs request can be specified by <code>hq submit --cpus=X ...</code> where <code>--cpus=X</code> is a shortcut for <code>--resource cpus=X</code>,    and X can be all valid requests for a resource, including values like <code>all</code> or <code>8 compact!</code>.   (More in Resource Management).</p> </li> <li> <p>A task may be automatically pinned to a given CPUs (see pinning).</p> </li> <li> <p>There are some extra environmental variables for CPUs (see below).</p> </li> <li> <p>CPUs are automatically detected. See below for information about NUMA or Hyper Threading.</p> </li> <li> <p>CPUs provided by a worker can be explicitly specified via <code>--cpus</code>, see below.</p> </li> </ul>"},{"location":"jobs/cresources/#cpu-related-environment-variables","title":"CPU related environment variables","text":"<p>The following variables are created when a task is executed:</p> <ul> <li><code>HQ_CPUS</code> - List of cores assigned to a task. (this is an alias for <code>HQ_RESOURCE_VALUES_cpus</code>).</li> <li><code>HQ_PIN</code> - Is set to <code>taskset</code> or <code>omp</code> (depending on the used pin mode) if the task was pinned by HyperQueue (see below).</li> <li><code>NUM_OMP_THREADS</code> -- Set to number of cores assigned for task. (For compatibility with OpenMP).                          This option is not set when you ask for a non-integer number of CPUs.</li> </ul>"},{"location":"jobs/cresources/#pinning","title":"Pinning","text":"<p>By default, HQ internally allocates CPUs on a logical level. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs of the worker, but process assignment to cores is left to the system scheduler, which may move processes across CPUs as it wants.</p> <p>If this is not desired, especially in the case of NUMA, processes could be pinned, either manually or automatically.</p>"},{"location":"jobs/cresources/#automatic-pinning","title":"Automatic pinning","text":"<p>HyperQueue can pin threads using two ways: with <code>taskset</code> or by setting <code>OpenMP</code> environment variables. You can use the <code>--pin</code> flag to choose between these two modes.</p> tasksetOpenMP <pre><code>$ hq submit --pin taskset --cpus=8 &lt;your-program&gt; &lt;args&gt;\n</code></pre> <p>will cause HyperQueue to execute your program like this: <pre><code>taskset -c \"&lt;allocated-cores&gt;\" &lt;your-program&gt; &lt;args&gt;`\n</code></pre></p> <pre><code>$ hq submit --pin omp --cpus=8 &lt;your-program&gt; &lt;args&gt;\n</code></pre> <p>will cause HyperQueue to execute your program like this: <pre><code>OMP_PROC_BIND=close OMP_PLACES=\"{&lt;allocated-cores&gt;}\" &lt;your-program&gt; &lt;args&gt;\n</code></pre></p> <p>If any automatic pinning mode is enabled, the environment variable <code>HQ_PIN</code> will be set.</p>"},{"location":"jobs/cresources/#manual-pinning","title":"Manual pinning","text":"<p>If you want to gain full control over core pinning, you may pin the process by yourself.</p> <p>The assigned CPUs are stored in the environment variable <code>HQ_CPUS</code> as a comma-delimited list of CPU IDs. You can use utilities such as <code>taskset</code> or <code>numactl</code> and pass them <code>HQ_CPUS</code> to pin a process to these CPUs.</p> <p>Warning</p> <p>If you manually pin your processes, do not also use the <code>--pin</code> flag of the <code>submit</code> command. It may have some unwanted interferences.</p> <p>Below you can find an example of a script file that pins the executed process manually using <code>taskset</code> and <code>numactl</code>:</p> tasksetnumactl <pre><code>#!/bin/bash\n\ntaskset -c $HQ_CPUS &lt;your-program&gt; &lt;args...&gt;\n</code></pre> <pre><code>#!/bin/bash\n\nnumactl -C $HQ_CPUS &lt;your-program&gt; &lt;args...&gt;\n</code></pre> <p>If you submit this script with <code>hq submit --cpus=4 script.sh</code>, it will pin your program to 4 CPUs allocated by HQ.</p>"},{"location":"jobs/cresources/#numa-allocation-strategy","title":"NUMA allocation strategy","text":"<p>Workers automatically detect the number of CPUs and on Linux systems they also detect their partitioning into sockets. When a NUMA architecture is automatically detected, indexed resource with groups is used for resource \"cpus\".</p> <p>You can then use allocation strategies for groups to specify how sockets are allocated. They follow the same rules as normal allocation strategies; for clarity we are rephrasing the group allocation strategies in terms of cores and sockets: </p> <ul> <li> <p>Compact (<code>compact</code>) - Tries to allocate cores on as few sockets as possible in the current worker state.</p> <pre><code>$ hq submit --cpus=\"8 compact\" ...\n</code></pre> </li> <li> <p>Strict Compact (<code>compact!</code>) - Always allocates cores on as few sockets as possible for a target node.   The task will not be executed until the requirement could be fully fulfilled.   For example, if your worker has 4 cores per socket, and you ask for 4 CPUs, it will always be   executed on a single socket. If you ask for 8 CPUs, it will always be executed on two sockets.</p> <pre><code>$ hq submit --cpus=\"8 compact!\" ...\n</code></pre> <p>Tip</p> <p>You might encounter a problem in your shell when you try to specify the strict compact policy, because the definition contains an exclamation mark (<code>!</code>). In that case, try to wrap the policy in single quotes, like this:</p> <pre><code>$ hq submit --cpus='8 compact!' ...\n</code></pre> </li> <li> <p>Scatter (<code>scatter</code>) - Allocate cores across as many sockets possible, based on the currently available   cores of a worker. If your worker has 4 sockets with 8 cores per socket, and you ask for 8 CPUs,   then HQ will try to run the process with 2 CPUs on each socket, if possible given the currently available   worker cores.</p> <pre><code>$ hq submit --cpus=\"8 scatter\" ...\n</code></pre> </li> </ul> <p>The default policy is the <code>compact</code> policy, i.e. <code>--cpus=&lt;X&gt;</code> is equivalent to <code>--cpus=\"&lt;X&gt; compact\"</code>.</p> <p>Note</p> <p>Specifying a policy only has an effect if you have more than one socket (physical CPUs). In case of a single socket, policies are indistinguishable.</p>"},{"location":"jobs/cresources/#cpu-configuration","title":"CPU configuration","text":"<p>Each worker will automatically detect the number of CPUs available. On Linux systems, it will also detect the partitioning into sockets (NUMA configuration). In most cases, it should work out of the box. If you want to see how will a HQ worker see your CPU configuration without actually starting the worker, you can use the <code>hq worker hwdetect</code> command, which will print the detected CPU configuration.</p>"},{"location":"jobs/cresources/#manual-specification-of-cpu-configuration","title":"Manual specification of CPU configuration","text":"<p>If the automatic detection fails for some reason, or you want to manually configure the CPU  configuration, you can use the <code>--cpus</code> flag when starting a worker.  It is an alias for <code>--resource cpus=...</code> (More in Resource Management), except it also allow to define <code>--cpus=N</code> where <code>N</code> is an integer; it is then interpreted as <code>1xN</code> in the resource definition.</p> <p>Below there are some examples  of configuration that you can specify:</p> <ul> <li> <p>Worker with 8 CPUs and a single socket.   <pre><code>$ hq worker start --cpus=8\n</code></pre></p> </li> <li> <p>Worker with 2 sockets with 12 cores per socket.   <pre><code>$ hq worker start --cpus=2x12\n</code></pre></p> </li> <li> <p>Manually specify that the worker should use the following core ids and how they are organized   into sockets. In this example, two sockets are defined, one with 3 cores and one with 2 cores.   <pre><code>$ hq worker start --cpus=[[2, 3, 4], [10, 14]]\n</code></pre></p> </li> </ul>"},{"location":"jobs/cresources/#disable-hyper-threading","title":"Disable Hyper Threading","text":"<p>If you want to detect CPUs but ignore HyperThreading then <code>--no-hyper-threading</code> flag can be used. It will detect only the first virtual core of each physical core.</p> <p>Example:</p> <pre><code>$ hq worker start --no-hyper-threading\n</code></pre>"},{"location":"jobs/directives/","title":"Directives","text":"<p>You can specify job parameters using special comments (<code>directives</code>) specified in a submitted shell script. Directives are lines that begin with the <code>#HQ</code> prefix. Any text following this prefix will be interpreted as a command line argument for <code>hq submit</code>.</p>"},{"location":"jobs/directives/#example-directive-file","title":"Example directive file","text":"<p>Suppose that <code>script.sh</code> has the following content: <pre><code>#!/bin/bash\n\n#HQ --name=Example\n#HQ --cpus=\"2 compact\" --pin taskset\n\n./my-program\n</code></pre></p> <p>If you execute <pre><code>$ hq submit script.sh\n</code></pre> it will behave as if you have executed <pre><code>$ hq submit --name=Example --cpus=\"2 compact\" --pin taskset script.sh\n</code></pre></p>"},{"location":"jobs/directives/#directives-mode","title":"Directives mode","text":"<p>You can select three modes using the <code>--directives</code> flag of <code>hq submit</code>. The mode will determine when should HyperQueue attempt to parse directives from the provided command.</p> <ul> <li><code>auto</code> (default) - Directives will be parsed if the first command passed to <code>hq submit</code> has the   <code>.sh</code> extension.</li> <li><code>file</code> - Directives will be parsed from the first command passed to <code>hq submit</code>.</li> <li><code>stdin</code> - Directives will be parsed from stdin (see <code>--stdin</code>) </li> <li><code>off</code> - Directives will not be parsed.</li> </ul> <p>Tip</p> <p>When HQ parses directives from a file, it will also try to parse a shebang line from the script and use it to select an interpreter for running the script.</p>"},{"location":"jobs/directives/#notes","title":"Notes","text":"<ul> <li>Directives have to be defined at the beginning of the file. Only comments or empty lines are allowed   to precede the directives.   </li> <li>Directives have to be defined in the first 32KiB of the file, the rest of the file is ignored.</li> <li>Parameters set via CLI have precedence over parameters set via direectives:<ul> <li>Parameters that cannot occur multiple times (like <code>--name</code>) will be overriden by values set from CLI.</li> <li>Parameters that can occur multiple times (like <code>--resource</code>) will be combined from CLI and from directives.</li> </ul> </li> <li>A script may contain more lines with the <code>#HQ</code> prefix, such lines are combined and evaluated as a continuous list of parameters.</li> </ul>"},{"location":"jobs/explain/","title":"Explain command","text":"<p>Sometimes, you may have connected workers and submitted tasks, but tasks are not running, and it is not clear why. For this purpose, there is a <code>hq task explain</code> command in HQ.</p> <p>It can be used as follows:</p> <p><code>hq task explain &lt;JOB_ID&gt; &lt;TASK_ID&gt;</code></p> <p>This command provides information about whether the given task can run on workers. If it cannot run, it returns explanation why. The explanation considers the following areas:</p> <ul> <li>Resources</li> <li>Time request and remaining worker time</li> <li>Size of the worker group for multi-node tasks</li> <li>Task dependencies</li> <li>If a task has multiple resource variants, the explanation is provided for each variant.</li> </ul> <p>Note: <code>explain</code> considers runtime information; therefore, it works only for waiting/running tasks and live workers.</p>"},{"location":"jobs/failure/","title":"Handling Failure","text":"<p>In distributed systems, failure is inevitable. This sections describes how HyperQueue handles various types of failures and how can you affect its behavior.</p>"},{"location":"jobs/failure/#resubmitting-array-jobs","title":"Resubmitting array jobs","text":"<p>When a job fails or is canceled, you can submit it again.  However, in case of task arrays, different tasks may end in different states, and often we want to  recompute only tasks with a specific status (e.g. failed tasks).</p> <p>By following combination of commands you may recompute only failed tasks. Let us assume that we want to recompute all failed tasks in job 5:</p> <p><pre><code>$ hq submit --array=`hq job task-ids 5 --filter=failed` ./my-computation\n</code></pre> The <code>hq job task-ids 5 --filter=failed</code> command returns IDs of failed jobs of job <code>5</code>, and we set it to <code>--array</code> parameter that starts only tasks for given IDs.</p> <p>If we want to recompute all failed tasks and all canceled tasks we can do it as follows:</p> <pre><code>$ hq submit --array=`hq job task-ids 5 --filter=failed,canceled` ./my-computation\n</code></pre> <p>Note that it also works with <code>--each-line</code> or <code>--from-json</code>, i.e.:</p> <pre><code># Original computation\n$ hq submit --each-line=input.txt ./my-computation\n\n\n# Resubmitting failed jobs\n$ hq submit --each-line=input.txt --array=`hq job task-ids last --filter=failed` ./my-computation\n</code></pre>"},{"location":"jobs/failure/#task-restart","title":"Task restart","text":"<p>Sometimes a worker might crash while it is executing some task. In that case the server will automatically reschedule that task to a different worker and the task will begin executing from the beginning.</p> <p>In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance ID. It is a 32b non-negative number that identifies each (re-)execution of a task.</p> <p>It is guaranteed that a newer execution of a task will have a larger instance ID, however HyperQueue explicitly does not guarantee any specific values or differences between two IDs. Each instance ID is valid only for a particular task. Two different tasks may have the same instance ID.</p> <p>Instance IDs can be useful e.g. when a task is restarted, and you want to distinguish the output of the first execution and the restarted execution (by default, HQ will overwrite the standard output/error file of the first execution). You can instead create a separate stdout/stderr file for each task execution using the instance ID placeholder.</p>"},{"location":"jobs/failure/#task-array-failures","title":"Task array failures","text":"<p>By default, when a single task of a task array fails, the computation of the job will continue.</p> <p>You can change this behavior with the <code>--max-fails=&lt;X&gt;</code> option of the <code>submit</code> command, where <code>X</code> is non-negative integer. If specified, once more tasks than <code>X</code> tasks fail, the rest of the job's tasks that were not completed yet will be canceled.</p> <p>For example: <pre><code>$ hq submit --array 1-1000 --max-fails 5 ...\n</code></pre> This will create a task array with <code>1000</code> tasks. Once <code>5</code> or more tasks fail, the remaining uncompleted tasks of the job will be canceled.</p>"},{"location":"jobs/jobfile/","title":"Job Definition File","text":"<p>Job Definition File (JDF) a way how to submit a complex pipeline into a HyperQueue. It is a TOML file that describes tasks of a job. JDF provides all functionalities as command line interface of HyperQueue and also adds access to additional features:</p> <ul> <li>Heterogeneous tasks -- Job may be composed of different tasks</li> <li>Dependencies -- Tasks may have dependencies</li> <li>Resource request alternatives -- Task may have alternative resource requests, e.g.: 4 cpus OR 1 cpus and 1 gpu</li> </ul> <p>Note that these features are also available through Python interface.</p>"},{"location":"jobs/jobfile/#minimal-example","title":"Minimal example","text":"<p>First, we create file with the following content:</p> <pre><code>[[task]]\ncommand = [\"sleep\", \"1\"]\n</code></pre> <p>Let us assume that we have named this file as <code>myfile.toml</code>, then we can run the <code>hq job submit-file</code> command to submit a job:</p> <pre><code>$ hq job submit-file myfile.toml\n</code></pre> <p>The effect will be same as running:</p> <pre><code>$ hq submit sleep 1\n</code></pre>"},{"location":"jobs/jobfile/#task-configuration","title":"Task configuration","text":"<p>The following shows how job and task may be configured in more detail. All options except <code>command</code> are optional. If not said otherwise, an option in format <code>xxx = ...</code> is an equivalent of <code>--xxx = ...</code> in <code>hq submit</code> command. The default are the same as CLI interface.</p> <pre><code>name = \"test-job\"\nstream = \"path/to/stream/dir\"  # Stdout/Stderr streaming (see --stream)\nmax_fails = 11\n\n[[task]]\nstdout = \"testout-%{TASK_ID}\"\nstderr = { path = \"testerr-%{TASK_ID}\", mode = \"rm-if-finished\" }\ntask_dir = true\ntime_limit = \"1m 10s\"\npriority = -1\ncrash_limit = 12\ncommand = [\"/bin/bash\", \"-c\", \"echo $ABC\"]\n\n# Environment variables\nenv = { \"ABC\" = \"123\", \"XYZ\" = \"aaaa\" }\n\n# Content that will be written on stdin\nstdin = \"Hello world!\"\n\n[[task.request]]\nresources = { \"cpus\" = \"4 compact!\", \"gpus\" = 2 }\ntime_request = \"10s\"\n</code></pre>"},{"location":"jobs/jobfile/#more-tasks","title":"More tasks","text":"<p>More tasks with different configuration may be defined as follows:</p> <pre><code>[[task]]\ncommand = [\"sleep\", \"1\"]\n\n[[task]]\ncommand = [\"sleep\", \"2\"]\n\n[[task]]\ncommand = [\"sleep\", \"3\"]\n</code></pre> <p>In the case above, tasks are given automatic task ids from id 0. You can also specify IDs manually:</p> <pre><code>[[task]]\nid = 10\ncommand = [\"sleep\", \"1\"]\n\n[[task]]\nid = 11\ncommand = [\"sleep\", \"2\"]\n\n[[task]]\nid = 2\ncommand = [\"sleep\", \"3\"]\n</code></pre>"},{"location":"jobs/jobfile/#task-arrays","title":"Task arrays","text":"<p>If you want to create uniform tasks you can define task array (similar to <code>--array</code>):</p> <pre><code>[array]\nids = \"1,2,50-100\"\ncommand = [\"sleep\", \"1\"]\n</code></pre> <p>You can also specify array with content of <code>HQ_ENTRIES</code>:</p> <pre><code>[array]\nentries = [\"One\", \"Two\", \"Three\"]\ncommand = [\"sleep\", \"1\"]\n</code></pre> <p>Note</p> <p>Options <code>entries</code> and <code>ids</code> can be used together.</p>"},{"location":"jobs/jobfile/#task-dependencies","title":"Task dependencies","text":"<p>Job Definition File allows to define a dependencies between tasks. In other words, it means that the task may be executed only if the previous tasks are already finished.</p> <p>The task's option <code>deps</code> defines on which tasks the given task dependents. The task is addressed by their IDs.</p> <p>The following example creates three tasks where the third task depends on the first two tasks.</p> <pre><code>[[task]]\nid = 1\ncommand = [...]\n\n[[task]]\nid = 3\ncommand = [...]\n\n[[task]]\nid = 5\ncommand = [...]\ndeps = [1, 3] # &lt;---- Dependency on tasks 1 and 3\n</code></pre>"},{"location":"jobs/jobfile/#resource-variants","title":"Resource variants","text":"<p>More resource configurations may be defined for a task. In this case, HyperQueue will take into account all these configurations during scheduling. When a task is started exactly one configuration is chosen. If in a given moment more configuration are possible for a given task, the configuration first defined has a higher priority.</p> <p>The following configuration defines that a task may be executed on 1 cpus and 1 gpu OR on 4 cpus.</p> <pre><code>[[task]]\ncommand = [...]\n[[task.request]]\nresources = { \"cpus\" = 1, \"gpus\" = 1 }\n[[task.request]]\nresources = { \"cpus\" = 4 }\n</code></pre> <p>In the case that many tasks with such a configuration are submitted to a worker with 16 cpus and 4 gpus then HyperQueue will run simultaneously 4 tasks in the first configuration and 3 tasks in the second one.</p> <p>For a task with resource variants, HyperQueue sets variable <code>HQ_RESOURCE_VARIANT</code> to an index of chosen variant (counted from 0) when a task is started.</p>"},{"location":"jobs/jobfile/#non-integer-resource-amounts","title":"Non-integer resource amounts","text":"<p>You may specify a resource number as float, e.g. <code>resources = { \"foo\" = 1.5 }</code>. It is valid but internally the type if converted to float, that may for some numbers lead to a rounding up when number is converted to 4-digit precision of resource amounts. If you want to avoid this, put the number into parentheses, e.g. <code>resources = { \"foo\" = \"1.5\" }</code>.</p>"},{"location":"jobs/jobs/","title":"Jobs and Tasks","text":"<p>The main unit of computation within HyperQueue is called a Task. It represents a single computation (currently, a single execution of some program) that is scheduled and executed on a worker.</p> <p>To actually compute something, you have to create a Job, which is a collection of tasks (a task graph). Jobs are units of computation management - you can submit, query or cancel jobs using the CLI.</p> <p>Note</p> <p>This section focuses on simple jobs, where each job contains exactly one task. See Task arrays to find out how to create jobs with multiple tasks.</p>"},{"location":"jobs/jobs/#identification-numbers","title":"Identification numbers","text":"<p>Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as <code>Job id</code>.</p> <p>Each task within a job is identified by an unsigned 32b integer called <code>Task id</code>. Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id.</p> <p>In simple jobs, task id is always set to <code>0</code>.</p>"},{"location":"jobs/jobs/#submitting-jobs","title":"Submitting jobs","text":"<p>To submit a simple job that will execute some executable with the provided arguments, use the <code>hq submit</code> command:</p> <pre><code>$ hq submit &lt;program&gt; &lt;arg1&gt; &lt;arg2&gt; ...\n</code></pre> <p>When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job.</p> <p>After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command.</p> <p>Warning</p> <p>The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it.</p> <p>Note</p> <p>When your command contains its own command line flags, you must put the command and its flags after <code>--</code>:</p> <pre><code>$ hq submit -- /bin/bash -c 'echo $PPID'\n</code></pre> <p>There are many parameters that you can set for the executed program, they are listed below.</p>"},{"location":"jobs/jobs/#name","title":"Name","text":"<p>Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the <code>--name</code> option:</p> <pre><code>$ hq submit --name=&lt;NAME&gt; ...\n</code></pre>"},{"location":"jobs/jobs/#working-directory","title":"Working directory","text":"<p>By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the <code>--cwd</code> option:</p> <pre><code>$ hq submit --cwd=&lt;path&gt; ...\n</code></pre> <p>Warning</p> <p>Make sure that the provided path exists on all worker nodes.</p> <p>Hint</p> <p>You can use placeholders in the working directory path.</p>"},{"location":"jobs/jobs/#output","title":"Output","text":"<p>By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are</p> <ul> <li><code>%{CWD}/job-%{JOB_ID}/%{TASK_ID}.stdout</code> for <code>stdout</code></li> <li><code>%{CWD}/job-%{JOB_ID}/%{TASK_ID}.stderr</code> for <code>stderr</code></li> </ul> <p><code>%{JOB_ID}</code> and <code>%{TASK_ID}</code> are so-called placeholders, you can read about them below.</p> <p>You can change these paths with the <code>--stdout</code> and <code>--stderr</code> options. You can also avoid creating <code>stdout</code>/<code>stderr</code> files completely by setting the value to <code>none</code>:</p> Change output paths <pre><code>$ hq submit --stdout=out.txt --stderr=err.txt ...\n</code></pre> Disable <code>stdout</code> <pre><code>$ hq submit --stdout=none ...\n</code></pre> <p>Warning</p> <p>Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the <code>%{CWD}</code> placeholder.</p>"},{"location":"jobs/jobs/#environment-variables","title":"Environment variables","text":"<p>You can set environment variables which will be passed to the provided command when the job is executed using the <code>--env &lt;KEY&gt;=&lt;VAL&gt;</code> option. Multiple environment variables can be passed if you repeat the option.</p> <pre><code>$ hq submit --env KEY1=VAL1 --env KEY2=VAL2 ...\n</code></pre> <p>Each executed task will also automatically receive the following environment variables:</p> Variable name Explanation <code>HQ_JOB_ID</code> Job id <code>HQ_TASK_ID</code> Task id <code>HQ_INSTANCE_ID</code> Instance id <code>HQ_RESOURCE_...</code> A set of variables related to allocated resources"},{"location":"jobs/jobs/#time-management","title":"Time management","text":"<p>You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job.</p> <ul> <li> <p>Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will   transition into the <code>Failed</code> state. This setting has no impact on scheduling.</p> <p>This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the <code>--time-limit</code> option<sup>1</sup>:</p> <pre><code>$ hq submit --time-limit=&lt;duration&gt; ...\n</code></pre> <p>Note</p> <p>Time limit is counted separately for each task. If you set a time limit of <code>3 minutes</code> and create two tasks, where each will run for two minutes, the time limit will not be hit.</p> </li> <li> <p>Time Request is the minimal remaining lifetime that a worker must have in   order   to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running   this   task.</p> <p>Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect.</p> <p>You can set the time request using the <code>--time-request</code> option<sup>1</sup>:</p> <pre><code>$ hq submit --time-request=&lt;duration&gt; ...\n</code></pre> <p>Note</p> <p>Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request.</p> </li> </ul> <p>Here is an example situation where time limit and time request can be used:</p> <p>Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within <code>10</code> minutes, but some of them run for (at most) <code>30</code> minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to <code>35</code> minutes to protect us against an error (deadlock, endless loop, etc.).</p> <p>However, since we know that each task will usually take at least <code>10</code> minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than <code>10</code> minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to <code>10</code> minutes.</p>"},{"location":"jobs/jobs/#priority","title":"Priority","text":"<p>You can modify the order in which tasks are executed using Priority. Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task <code>A</code> with priority <code>5</code> and task <code>B</code> with priority <code>3</code> are scheduled to the same worker and only one of them may be executed, then <code>A</code> will be executed first.</p> <p>You can set the priority using the <code>--priority</code> option:</p> <pre><code>$hq submit --priority=&lt;PRIORITY&gt;\n</code></pre> <p>If no priority is specified, then each task will have priority <code>0</code>.</p>"},{"location":"jobs/jobs/#placeholders","title":"Placeholders","text":"<p>You can use special variables when setting certain job parameters (working directory, output paths, stream path). These variables, called Placeholders, will be replaced by job or task-specific information before the job is executed.</p> <p>Placeholders are enclosed in curly braces (<code>{}</code>) and prefixed with a percent (<code>%</code>) sign.</p> <p>You can use the following placeholders:</p> Placeholder Will be replaced by Available for <code>%{JOB_ID}</code> Job ID <code>stdout</code>, <code>stderr</code>, <code>cwd</code>, <code>stream-dir</code> <code>%{TASK_ID}</code> Task ID <code>stdout</code>, <code>stderr</code>, <code>cwd</code> <code>%{INSTANCE_ID}</code> Instance ID <code>stdout</code>, <code>stderr</code>, <code>cwd</code> <code>%{SUBMIT_DIR}</code> Directory from which the job was submitted. <code>stdout</code>, <code>stderr</code>, <code>cwd</code>, <code>stream-dir</code> <code>%{CWD}</code> Working directory of the task. <code>stdout</code>, <code>stderr</code> <code>%{SERVER_UID}</code> Unique server ID. <code>stdout</code>, <code>stderr</code>, <code>cwd</code>, <code>stream-dir</code> <p><code>SERVER_UID</code> is a random string that is unique for each new server execution (each <code>hq server start</code> gets a separate value).</p> <p>As an example, if you wanted to include the Instance ID in the <code>stdout</code> path (to distinguish the individual outputs of restarted tasks), you can use placeholders like this:</p> <pre><code>$ hq submit --stdout '%{CWD}/job-%{JOB_ID}/%{TASK_ID}-%{INSTANCE_ID}.stdout' ...\n</code></pre>"},{"location":"jobs/jobs/#state","title":"State","text":"<p>At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job using the <code>hq job info</code> command<sup>2</sup>:</p> <pre><code>$ hq job info &lt;job-id&gt;\n</code></pre>"},{"location":"jobs/jobs/#task-state","title":"Task state","text":"<p>Each task starts in the <code>Waiting</code> state and can end up in one of the terminal states: <code>Finished</code>, <code>Failed</code> or <code>Canceled</code>.</p> <pre><code>Waiting-----------------\\\n   | ^                  |\n   | |                  |\n   v |                  |\nRunning-----------------|\n   | |                  |\n   | \\--------\\         |\n   |          |         |\n   v          v         v\nFinished    Failed   Canceled\n</code></pre> <ul> <li>Waiting The task was submitted and is now waiting to be executed.</li> <li>Running The task is running on a worker. It may become <code>Waiting</code> again when the worker where the task is running   crashes.</li> <li>Finished The task has successfully finished.</li> <li>Failed The task has failed.</li> <li>Canceled The task has been canceled.</li> </ul> <p>If a task is in the <code>Finished</code>, <code>Failed</code> or <code>Canceled</code> state, it is <code>completed</code>.</p>"},{"location":"jobs/jobs/#job-state","title":"Job state","text":"<p>The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules:</p> <ol> <li>If at least one task is <code>Running</code>, then job state is <code>Running</code>.</li> <li>If at least one task has not been <code>completed</code> yet, then job state is <code>Waiting</code>.</li> <li>If at least one task is <code>Failed</code>, then job state is <code>Failed</code>.</li> <li>If at least one task is <code>Canceled</code>, then job state is <code>Canceled</code>.</li> <li>If all tasks are finished and job is open (see Open Jobs), then job state is <code>Opened</code>.</li> <li>Remaining case: all tasks are <code>Finished</code> and job is closed, then job state is <code>Finished</code>.</li> </ol>"},{"location":"jobs/jobs/#cancelling-jobs","title":"Cancelling jobs","text":"<p>You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the <code>hq job cancel</code> command<sup>2</sup>:</p> <pre><code>$ hq job cancel &lt;job-selector&gt;\n</code></pre> <p>Cancelling a job will cancel all of its tasks that are not yet completed.</p>"},{"location":"jobs/jobs/#forgetting-jobs","title":"Forgetting jobs","text":"<p>If you want to completely forget a job, and thus free up its associated memory, you can do that using the <code>hq job forget</code> command<sup>2</sup>:</p> <pre><code>$ hq job forget &lt;job-selector&gt;\n</code></pre> <p>By default, all completed jobs (finished/failed/canceled) will be forgotten. You can use the <code>--status</code> parameter to only forget jobs in certain statuses:</p> <pre><code>$ hq job forget all --status finished,canceled\n</code></pre> <p>However, only jobs that are completed, i.e. that have been finished successfully, failed or have been canceled, can be forgotten. If you want to forget a waiting or a running job, cancel it first.</p> <p>Note that if you are using a journal, forgetting only free the memory of the server but the tasks remains in journal, run <code>hq journal prune</code> to remove completed jobs and workers from journal file.</p>"},{"location":"jobs/jobs/#waiting-for-jobs","title":"Waiting for jobs","text":"<p>There are three ways of waiting until a job completes:</p> <ul> <li> <p>Submit and wait You can use the <code>--wait</code> flag when submitting a job. This will cause the submission command to   wait until the job becomes complete:</p> <pre><code>$ hq submit --wait ...\n</code></pre> <p>Tip</p> <p>This method can be used for benchmarking the job duration.</p> </li> <li> <p>Wait command There is a separate <code>hq job wait</code> command that can be used to wait until an existing job   completes<sup>2</sup>:</p> <pre><code>$ hq job wait &lt;job-selector&gt;\n</code></pre> </li> <li> <p>Interactive wait If you want to interactively observe the status of a job (which is useful especially if it   has multiple tasks), you can use the <code>hq job progress</code> command:</p> Submit and observeObserve an existing job<sup>2</sup> <pre><code>$ hq submit --progress ...\n</code></pre> <pre><code>$ hq job progress &lt;selector&gt;\n</code></pre> </li> </ul>"},{"location":"jobs/jobs/#attaching-standard-input","title":"Attaching standard input","text":"<p>When <code>--stdin</code> flag is used, HQ captures standard input and attaches it to each task of a job. When a task is started then the attached data is written into the standard input of the task.</p> <p>This can be used to submitting scripts without creating file. The following command will capture stdin and executes it in Bash</p> <pre><code>$ hq submit --stdin bash\n</code></pre> <p>If you want to parse #HQ directives from standard input, you can use <code>--directives=stdin</code>.</p>"},{"location":"jobs/jobs/#task-directory","title":"Task directory","text":"<p>When a job is submitted with <code>--task-dir</code> then a temporary directory is created for each task and passed via environment variable <code>HQ_TASK_DIR</code>. This directory is automatically deleted when the task is completed (for any reason).</p>"},{"location":"jobs/jobs/#providing-own-error-message","title":"Providing own error message","text":"<p>A task may pass its own error message into the HyperQueue. HyperQueue provides a filename via environment variable <code>HQ_ERROR_FILENAME</code>, if a task creates this file and terminates with a non-zero return code, then the content of this file is taken as an error message.</p> <p><code>HQ_ERROR_FILENAME</code> is provided only if task directory is set on. The filename is always placed inside the task directory.</p> <p>If the message is longer than 2KiB, then it is truncated to 2KiB.</p> <p>If task terminates with zero return code, then the error file is ignored.</p>"},{"location":"jobs/jobs/#automatic-file-cleanup","title":"Automatic file cleanup","text":"<p>If you create a lot of tasks and do not use output streaming, a lot of <code>stdout</code>/<code>stderr</code> files can be created on the disk. In certain cases, you might not be interested in the contents of these files, especially if the task has finished successfully, and you instead want to remove them as soon as they are not needed.</p> <p>For that, you can use a file cleanup mode when specifying <code>stdout</code> and/or <code>stderr</code> to choose what should happen with the file when its task finishes. The mode is specified as a name following a colon (<code>:</code>) after the file path. Currently, one cleanup mode is implemented:</p> <ul> <li>Remove the file if the task has finished successfully:</li> </ul> <pre><code>$ hq submit --stdout=\"out.txt:rm-if-finished\" /my-program\n</code></pre> <p>The file will not be deleted if the task fails or is cancelled.</p> <p>Note</p> <p>If you want to use the default <code>stdout</code>/<code>stderr</code> file path (and you don't want to look it up), you can also specify just the cleanup mode without the file path: <pre><code>$ hq submit --stdout=\":rm-if-finished\" /my-program\n</code></pre></p>"},{"location":"jobs/jobs/#crash-limit","title":"Crash limit","text":"<p>When a worker is lost, then all running tasks on the worker are suspicious that they may cause the crash of the worker. HyperQueue server remembers how many times were a task running while a worker is lost (crash counter). If the count reaches the limit, then the task is set to the failed state. By default, this limit is <code>5</code> but it can be changed as follows:</p> <pre><code>$ hq submit --crash-limit=&lt;NEWLIMIT&gt; ...\n</code></pre> <p>The crash counter of a task is not increased when worker is stopped for known reason (via command <code>hq server stop</code> or time limit is reached), because it was not the cause of the termination.</p> <p>In addition to a numerical value, the option <code>--crash-limit</code> may have two special values:</p> <ul> <li><code>never-restart</code> -- Task is never restarted. It is similar to <code>--crash-counter=1</code>, but   the task is never restarted even in the case when the task   was running on a worker that was stopped by a way that does not increase crash counter.</li> <li><code>unlimited</code> -- Task will always be restarted.</li> </ul>"},{"location":"jobs/jobs/#useful-job-commands","title":"Useful job commands","text":"<p>Below you can find a list of useful job commands. The complete <code>hq job</code> CLI reference can be found here.</p>"},{"location":"jobs/jobs/#display-job-table","title":"Display job table","text":"<p>You can display basic job information using <code>hq job list</code>.</p> List queued and running jobsList all jobsList jobs by status <pre><code>$ hq job list\n</code></pre> <pre><code>$ hq job list --all\n</code></pre> <p>You can display only jobs having the selected states by using the <code>--filter</code> flag:</p> <pre><code>$ hq job list --filter running,waiting\n</code></pre> <p>Valid filter values are:</p> <ul> <li><code>waiting</code></li> <li><code>running</code></li> <li><code>finished</code></li> <li><code>failed</code></li> <li><code>canceled</code></li> </ul>"},{"location":"jobs/jobs/#display-a-summary-table-of-all-jobs","title":"Display a summary table of all jobs","text":"<p>The <code>hq job summary</code> command shows aggregated counts of jobs with a given status:</p> <pre><code>$ hq job summary\n</code></pre>"},{"location":"jobs/jobs/#display-information-about-a-specific-job","title":"Display information about a specific job","text":"<p>You can use the <code>hq job info</code> command to show detailed information about specific job(s).</p> <pre><code>$ hq job info &lt;job-selector&gt;\n</code></pre>"},{"location":"jobs/jobs/#display-information-about-individual-tasks","title":"Display information about individual tasks","text":"<p>You can use the <code>hq task list</code> command to display information about individual tasks, even across multiple jobs.</p> <pre><code>$ hq task list &lt;job-selector&gt; [--task-status &lt;status&gt;] [--tasks &lt;task-selector&gt;]\n</code></pre>"},{"location":"jobs/jobs/#display-job-stdoutstderr","title":"Display job <code>stdout</code>/<code>stderr</code>","text":"<p>You can use the <code>hq job cat</code> command to print the <code>stdout</code> or <code>stderr</code> of specific job(s) and task(s).</p> <pre><code>$ hq job cat &lt;job-id&gt; [--tasks &lt;task-selector&gt;] &lt;stdout/stderr&gt;\n</code></pre> <ol> <li> <p>You can use various shortcuts for the duration value.\u00a0\u21a9\u21a9</p> </li> <li> <p>You can use various shortcuts to select multiple jobs at once.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"jobs/multinode/","title":"Multinode Tasks","text":"<p>Warning</p> <p>Multi-node support is now in the experimental stage. The core functionality is working, but some features may be limited and quality of scheduling may vary.</p> <p>Multi-node tasks are tasks that spreads across multiple nodes. Each node reserved for such task is exclusively reserved, i.e. no other  tasks may run on such nodes.</p> <p>A job with multi-node task can be specified by <code>--nodes=X</code> option.</p> <p>An example of a job with multi-node task asking for 4 nodes:</p> <pre><code>$ hq submit --nodes 4 test.sh\n</code></pre> <p>When the task is started, four nodes are assigned to this task. One of them is chosen as \"root\" node where <code>test.sh</code> is started.</p> <p>Node names of all assigned nodes can be found in file which path is in environmental variable <code>HQ_NODE_FILE</code>. Each line is a node name. The first line is always the root node. The node is a short hostname, i.e. hostname stripped by a suffix after first \".\" (e.g. if a hostname of worker is \"cn690.karolina.it4i.cz\" then node name is \"cn690\"). Many HPC applications use only short hostnames, hence we provide them as default.</p> <p>If you need a full hostnames, there is file which name is written in <code>HQ_HOST_FILE</code> and it has the same meaning as <code>NQ_NODE_FILE</code> but contains the full node hostnames without stripping.</p> <p>Note: Both files are placed in task directory; therefore, a multi-node tasks always enables task directory (<code>--task-dir</code>).</p> <p>If a multinode task is started, HQ also creates variable <code>HQ_NUM_NODES</code> that holds the number of nodes assigned to a task (i.e. the number of lines of the node file)</p>"},{"location":"jobs/multinode/#groups","title":"Groups","text":"<p>A multi-node task is started only on workers that belong to the same group. By default, workers are grouped by PBS/Slurm allocations and workers outside any allocation  are put in \"default\" group.</p> <p>A group of a worker can be specified at the start of the worker and it may be any string. Example:</p> <pre><code>$ hq worker start --group my_group\n</code></pre>"},{"location":"jobs/multinode/#running-mpi-tasks","title":"Running MPI tasks","text":"<p>A script that starts an MPI program in multi-node task may look like as follows:</p> <pre><code>#!/bin/sh\n\nmpirun --node-list=$HQ_NODE_FILE ./a-program\n</code></pre> <p>If you are running SLURM you should start the MPI program as follows:</p> <pre><code>#!/bin/sh\n\nsrun --nodefile=$HQ_NODE_FILE --nodes=$HQ_NUM_NODES mpirun ...\n</code></pre> <p>Note: It is important to set <code>--nodes</code> otherwise the node file will not be respected.</p>"},{"location":"jobs/multinode/#automatic-allocation","title":"Automatic allocation","text":"<p>If you combine multi-node tasks with automatic allocation, you should configure the maximum number of tasks per allocation.</p>"},{"location":"jobs/openjobs/","title":"Open jobs","text":"<p>By default, a job is a set of tasks that are created atomically during a submit, and no other task can be added to the job. We call this job closed. In contrast, HQ allows you to create an open job that allows new tasks to be submitted as long as it is open.</p>"},{"location":"jobs/openjobs/#opening-a-job","title":"Opening a job","text":"<p>A job can be opened with the <code>hq job open</code> command:</p> <pre><code>$ hq job open\n</code></pre> <p>If opening was successful, this will be printed:</p> <pre><code>Job &lt;ID&gt; is open.\n</code></pre> <p>If you want to get just ID without any additional text, you can open job as follows:</p> <pre><code>$ hq --output-mode=quiet job open\n</code></pre> <p>Note: In the list of jobs, an open job is marked with \"*\" before the id.</p>"},{"location":"jobs/openjobs/#submitting-tasks-into-open-jobs","title":"Submitting tasks into open jobs","text":"<p>A submit to an open job is the same as a normal submit, except that you must specify the job you are submitting to with the <code>--job</code> argument. You may submit multiple times into the same job. Tasks are scheduled to the workers immediately when they are received by the server.</p> <pre><code>$ hq submit --job &lt;JOB_ID&gt; ... other submit args ...\n$ hq submit --job &lt;JOB_ID&gt; ... other submit args ...\n$ hq submit --job &lt;JOB_ID&gt; ... other submit args ...\n</code></pre>"},{"location":"jobs/openjobs/#task-ids","title":"Task Ids","text":"<p>All tasks in one job share the task ID space. When you do not specify task ids, HQ automatically assigns a smallest ID that is bigger then any existing task id.</p> <pre><code>$ hq job open\n$ hq submit --job &lt;JOB_ID&gt; -- hostname # Task ID is 0 \n$ hq submit --job &lt;JOB_ID&gt; -- hostname # Task ID is 1\n\n# Task IDs are 2, 3, 4 ...\n$ hq submit --job &lt;JOB_ID&gt; --each-line='test.txt' -- do-something\n</code></pre> <p>If you are explicitly specifying task IDs, it is an error if task ID is reused:</p> <pre><code>$ hq submit --job &lt;JOB_ID&gt; -- hostname # Task ID is 0\n\n# This is Ok \n$ hq submit --job &lt;JOB_ID&gt; --array 10-20 -- hostname\n\n# This fails: Task ID 0 and 10, 11, 12 already exist\n$ hq submit --job &lt;JOB_ID&gt; --array 0-12 -- hostname\n</code></pre>"},{"location":"jobs/openjobs/#job-name-and-max-fails","title":"Job name and <code>--max-fails</code>","text":"<p>Job's name and configuration open <code>--max-fails</code> are the property of the job. They can be set when job is opened and they cannot be later changed. Submit options <code>--name</code> and <code>--max-fails</code> cannot be used if you are submitting into an open job.</p> <pre><code># Configuring jobs's name and max fails\n$ hq job open --name=MyOpenJob --max-fails=10\n\n# Submit fails becase --max-fails cannot be used together with --job\n$ hq submit --job &lt;JOB_ID&gt; --max-fails=5 ...\n</code></pre>"},{"location":"jobs/openjobs/#submit-file-into-open-job","title":"Submit file into open job","text":"<p>Submitting job definition file into an open job works in the similar way as a normal submit, you just need to add <code>--job</code> parameter.</p> <pre><code>$ hq job submit-file --job &lt;JOB_ID&gt; job-definition.toml\n</code></pre>"},{"location":"jobs/openjobs/#closing-job","title":"Closing job","text":"<p>You can close a job with the <code>hq job close</code> command:</p> <pre><code>$ hq job close &lt;JOB_SELECTOR&gt;\n</code></pre> <p>When a job is closed, you are not allowed to submit any more tasks to the job. It has no effect on tasks already submitted to the job; they continue to be processed as usual.</p> <p>Closing of already closed job throws an error.</p>"},{"location":"jobs/openjobs/#completion-semantics","title":"Completion semantics","text":"<p>Leaving open jobs has no overhead, but it does affect the semantics of job completion. A job is considered completed when all tasks have been completed and the job is closed. Therefore, <code>hq job wait</code> will wait until all tasks of the selected jobs are complete and the jobs are closed.</p> <p>If you want to wait only for completion of tasks and ignoring if job is open or closed then there is <code>hq job wait --without-close ...</code>.</p>"},{"location":"jobs/resources/","title":"Resources","text":""},{"location":"jobs/resources/#resource-management","title":"Resource management","text":"<p>Resource management serves for defining arbitrary resources provided by workers and also corresponding resource requests required by tasks. HyperQueue will take care of matching task resource requests so that only workers that can fulfill them will be able to execute such tasks.</p> <p>Some generic resources are automatically detected; however, users may also define their own resources.</p> <p>From version 0.13.0, CPUs are also managed as other resources, but they have still some extra functionality; therefore, there is a special section about CPU resources.</p> <p>Important</p> <p>Resources in HyperQueue exist on a purely logical level. They can correspond to physical things (like GPUs), but it is the responsibility of the user to make sure that this correspondence makes sense. With exception of CPUs, HyperQueue by itself does not attach any semantics to resources, they are just numbers used for scheduling.</p>"},{"location":"jobs/resources/#worker-resources","title":"Worker resources","text":"<p>Each worker has one or more resources attached. Each resource is a resource pool identified by a name. A resource pool represents some resources provided by a worker; each task can then ask for a part of the resources contained in that pool.</p> <p>There are two kinds of resource pools:</p> <ul> <li>Indexed pool: This pool represents an enumerated set of resources represented by strings.   Each resource has its own identity. Tasks do not ask for specific values from the set, they just specify   how many resources they require and HyperQueue will allocate the specified amount of resources   from the pool for each task.</li> </ul> <p>This pool is useful for resources that have their own identity, for example individual GPU or   FPGA accelerators.</p> <p>HyperQueue guarantees that no individual resource from the indexed pool is allocated to more than   a single task at any given time and that a task will not be executed on a worker if it does not   currently have enough individual resources to fulfill the resource request   of the task.</p> <p>Indexed pool can be defined with groups where indices live in separated groups. Task may   then ask for different allocation policies (e.g. use resources from the same or different groups).   The main purpose of this is to capture NUMA architectures, each group then represents a socket with cores.</p> <ul> <li>Sum pool: This pool represents a resource that has a certain size which is split into individual   tasks. A typical example is memory; if a worker has <code>2000</code> bytes of memory, it can serve e.g. four   tasks, if each task asks for <code>500</code> bytes of memory.</li> </ul> <p>HyperQueue guarantees that the sum of resource request sizes of running tasks on a worker does   not exceed the total size of the sum pool.</p>"},{"location":"jobs/resources/#specifying-worker-resources","title":"Specifying worker resources","text":"<p>You can specify the resource pools of a worker when you start it:</p> <pre><code>$ hq worker start --resource \"&lt;NAME-1&gt;=&lt;DEF-1&gt;\" --resource \"&lt;NAME-2&gt;=&lt;DEF-2&gt;\" ...\n</code></pre> <p>where <code>NAME-i</code> is a name (string ) of the <code>i</code>-th resource pool and <code>DEF-i</code> is a definition of the <code>i-th</code> resource pool. You can define resource pools using one of the following formats:</p> <ul> <li><code>[&lt;VALUE&gt;, &lt;VALUE&gt;, ..., &lt;VALUE&gt;]</code> where <code>VALUE</code> is a string. This defines a   an indexed pool with the given values. If you need to enter a string resource that contains special   characters (<code>[</code>, <code>]</code>, <code>,</code>, whitespace), you can wrap the value in quotes:   <code>[\"foo [,]\", bar, \"my resource\"]</code>.</li> <li><code>range(&lt;START&gt;-&lt;END&gt;)</code> where <code>START</code> and <code>END</code> are non-negative integers. This defines   an indexed pool with numbers in the inclusive range <code>[START, END]</code>.</li> <li><code>[[&lt;VALUE&gt;, ..., &lt;VALUE&gt;], [&lt;VALUE&gt;, ..., &lt;VALUE&gt;], ...]</code> where <code>VALUE</code> is a string. This   defines an indexed pool where indices are grouped.</li> <li><code>&lt;N&gt;x&lt;M&gt;</code> Creates indexed pool with N groups of size M, indices are indexed from 0,   (e.g. \"2x3\" is equivalent to <code>[[0, 1, 2], [3, 4, 5]</code>)</li> <li><code>sum(&lt;SIZE&gt;)</code> where <code>SIZE</code> is a positive integer. This defines a sum pool with the given   size.</li> </ul> <p>Tip</p> <p>You might encounter a problem in your shell when you try to specify worker resources, because the definition contains parentheses (<code>()</code>). In that case just wrap the resource definition in quotes, like this:</p> <pre><code>$ hq worker start --resource \"foo=sum(5)\"\n</code></pre>"},{"location":"jobs/resources/#resource-names","title":"Resource names","text":"<p>Resource names are restricted by the following rules:</p> <ul> <li>They can only contain ASCII letters and digits (<code>a-z</code>, <code>A-Z</code>, <code>0-9</code>) and the slash (<code>/</code>) symbol.</li> <li>They need to begin with an ASCII letter.</li> </ul> <p>These restrictions exist because the resource names are passed as environment variable names to tasks, which often execute shell scripts. However, shells typically do not support environment variables containing anything else than ASCII letters, digits and the underscore symbol. Therefore, HQ limits resource naming to align with the behaviour of the shell.</p> <p>Important</p> <p>HQ will normalize the resource name when passing environment variables to a task (see below).</p>"},{"location":"jobs/resources/#automatically-detected-resources","title":"Automatically detected resources","text":"<p>The following resources are detected automatically if a resource of a given name is not explicitly defined.</p> <ul> <li> <p>CPUs are automatically detected as resource named \"cpus\" (more in CPU resources).</p> </li> <li> <p>GPUs that are available when a worker is started are automatically detected under the following   resource names:</p> <ul> <li>NVIDIA GPUs are stored the under resource name <code>gpus/nvidia</code>. These GPUs are detected from the   environment variable <code>CUDA_VISIBLE_DEVICES</code> or from the <code>procfs</code> filesystem.</li> <li>AMD GPUs are stored under the resource name <code>gpus/amd</code>. These GPUs are detected from the environment   variable <code>ROCR_VISIBLE_DEVICES</code>.</li> </ul> </li> </ul> <p>You can set these environment variables when starting a worker to override the list of available GPUs:</p> <pre><code>```bash\n$ CUDA_VISIBLE_DEVICES=2,3 hq worker start\n# The worker will have resource gpus/nvidia=[2,3]\n```\n</code></pre> <ul> <li>RAM of the node is detected as resource \"mem\" in megabytes; i.e. <code>--resource mem=100</code> asks for 100 MiBs of the memory.</li> </ul> <p>If you want to see how your system is seen by a worker without actually starting it, you can start:</p> <pre><code>$ hq worker hwdetect\n</code></pre> <p>The automatic detection of resources can be disabled by argument <code>--no-detect-resources</code> in <code>hq worker start ...</code>. It disables detection of resources other than \"cpus\"; if resource \"cpus\" are not explicitly defined, it will always be detected.</p>"},{"location":"jobs/resources/#resource-request","title":"Resource request","text":"<p>When you submit a job, you can define a resource requests with the <code>--resource</code> flag:</p> <pre><code>$ hq submit --resource &lt;NAME1&gt;=&lt;AMOUNT1&gt; --resource &lt;NAME2&gt;=&lt;AMOUNT2&gt; ...\n</code></pre> <p>Where <code>NAME</code> is a name of the requested resource and the <code>AMOUNT</code> is a positive number defining the size of the request.</p> <p>Tasks with such resource requests will only be executed on workers that fulfill all the specified task requests.</p> <p>Important</p> <p>Notice that task resource requests always ask for an amount of resources required by a task, regardless whether that resource corresponds to an indexed or a sum pool on workers.</p> <p>For example, let's say that a worker has an indexed pool of GPUs: <pre><code>$ hq worker start --resource \"gpus/nvidia=range(1-3)\"\n</code></pre> And we create two jobs, each with a single task. The first job wants 1 GPU, the second one wants two GPUs.</p> <pre><code>$ hq submit --resource gpus/nvidia=1 ...\n$ hq submit --resource gpus/nvidia=2 ...\n</code></pre> <p>Then the first job can be allocated e.g. the GPU <code>2</code> and the second job can be allocated the GPUs <code>1</code> and <code>3</code>.</p>"},{"location":"jobs/resources/#requesting-all-resources","title":"Requesting all resources","text":"<p>A task may ask for all given resources of that type by specifying <code>--resource &lt;NAME&gt;=all</code>. Such a task will be scheduled only on a worker that has at least <code>1</code> of such a resource and when a task is executed all resources of that type will be given to this task.</p>"},{"location":"jobs/resources/#resource-request-strategies","title":"Resource request strategies","text":"<p>When resource request is defined, after the amount you can define allocation strategy: <code>--resource &lt;NAME&gt;=\"&lt;AMOUNT&gt; &lt;STRATEGY&gt;\"</code>.</p> <p>Example:</p> <pre><code>$ hq submit --resource cpus=\"8 compact!\" ...\n</code></pre> <p>Specifying strategy has effect only if the worker provides indexed resource in groups. If resource is other type, then strategy is ignored.</p> <p>When strategy is not defined then <code>compact</code> is used as default.</p> <p>Strategies:</p> <ul> <li> <p>Compact (<code>compact</code>) -- Tries to allocate indices in few groups as possible in the current worker state. After   the minimal number of groups is chosen, then the indices   are taken evenly from the selected groups.</p> </li> <li> <p>Strict Compact (<code>compact!</code>) -- Always allocate indices on as few groups as possible for a target target.   The task is not executed until this requirement could not be fully fulfilled.   E.g. If a worker has 4 indices per a group and you ask for 4 indices in the strict compact mode,   it will always be executed with indices from a single group.   If you ask for 8 cpus in the same way, it will always be executed with indices from two groups.</p> </li> <li> <p>Tight (<code>tight</code>) - Similarly to <code>compact</code>, the mode tries to allocate indices in few groups as possible in the   current worker state. When the groups are chosen, then it packs as much as possible to the first group, then to the   second, etc. See the Example below.</p> </li> <li> <p>Strict tight (<code>tight!</code>) -- Strict mode for <code>tight</code>. Works as <code>tight</code> but   a task is not executed until the minial number of groups for the given worker could not be   achieved.</p> </li> <li> <p>Scatter (<code>scatter</code>) - Allocate indices across as many groups as possible in the current worker state.   E.g. Let us assume that a worker has 4 groups with 8 indices per group, and you ask for 8 cpus in the scatter mode.   If possible in the current situation, HQ tries to run a process with 2 cpus on each socket.</p> </li> </ul>"},{"location":"jobs/resources/#example","title":"Example","text":"<p>Let us assume that we have worker with resource <code>foo</code> that have indices organized to 3 groups with 4 indices in each group. We assume that all resources are free. The allocations in the square brackets below means how many indices we got from different groups, e.g. <code>[4, 2]</code> means that we get <code>4</code> indices from a group, and <code>2</code> indices from another group.</p> <ul> <li><code>6 compact</code> allocates: <code>[3, 3]</code></li> <li><code>6 tight</code> allocates: <code>[4, 2]</code></li> <li><code>6 scatter</code> allocates: <code>[2, 2, 2]</code></li> </ul>"},{"location":"jobs/resources/#resource-coupling","title":"Resource coupling","text":"<p>Resource coupling extends the HQ ability to capture NUMA architectures. It allows saying that two or more resources of the worker should be allocated together (from the same groups). For example, we may want to allocate CPUs and GPUs from the same NUMA node.</p> <p>The current version does not provide automatic detection of coupling; you need to specify it manually, via option <code>--coupling</code> when the worker is started.</p> <p>For example:</p> <pre><code>$ hq worker starts ... --coupling=cpus,gpus\n</code></pre> <p>Coupled resources have to be indexed resources with groups, and they all need to have the same number of groups. The allocation then considers the groups of these resources aligned, i.e., when we have a compact request from coupled resources, they should be allocated from the same groups.</p> <p>When coupling is enabled, it modifies the behavior of <code>compact</code> and <code>tight</code> strategies when more coupled resources are requested. They do not minimize the number of used groups individually, but minizies it for all requested coupled resources.</p>"},{"location":"jobs/resources/#example_1","title":"Example","text":"<p>Let us assume that we have worker with resource <code>cpus</code> that have indices organized to 3 groups with 4 indices in each group. And resource <code>gpus</code> that have 3 groups with 2 indices within each group.</p> <ul> <li><code>cpus=6 compact</code> and <code>gpus=2 compact</code> allocates <code>cpus=[3, 3]</code>, <code>gpus=[1, 1]</code></li> <li><code>cpus=1 compact</code> and <code>gpus=2 compact</code> allocates <code>cpus=[1]</code>, <code>gpus=[2]</code></li> <li><code>cpus=6 tight</code> and <code>gpus=2 compact</code> allocates <code>cpus=[4, 2]</code>, <code>gpus=[1, 1]</code></li> <li><code>cpus=6 tight</code> and <code>gpus=2 tight</code> allocates <code>cpus=[4, 2]</code>, <code>gpus=[2]</code></li> </ul>"},{"location":"jobs/resources/#strict-strategies","title":"Strict strategies","text":"<p>The coupling also modifies the semantics of strict strategies. The condition is stricter in such the case. The minimal number of groups has to be achievable not only individually, but also across all requested coupled resources with <code>compact!</code> or <code>tight!</code> strategies.</p> <p>Let us assume a worker from the example above. And assume that the there are the following free resources:</p> <ul> <li><code>cpus=[4, 4, 0]</code></li> <li><code>gpus=[0, 2, 2]</code></li> </ul> <p>The request <code>cpus=8 compact!</code> and <code>gpus=4 compact!</code> is enabled on a worker where <code>cpus</code> and <code>gpus</code> are not coupled; because each both resources can be taken from the minimal number of groups.</p> <p>The same request is not enabled on a worker where <code>cpus</code> and <code>gpus</code> are coupled, because resources are taken from three groups if we take an union of the used groups. But we can achieve a two groups in the union. e.g. the following situation:</p> <ul> <li><code>cpus=[4, 4, 0]</code></li> <li><code>gpus=[2, 2, 0]</code></li> </ul>"},{"location":"jobs/resources/#non-integer-allocation-of-resources","title":"Non-integer allocation of resources","text":"<p>Amount of the resource may be a non-integer number. E.g. you may ask for 0.5 of a resource. It tells the scheduler that you want to utilize only half of the resource and if another process asks for at most 0.5 of the resource, it may get the same resource. This resource sharing is done on logical of HyperQueue and actual resource sharing is up to tasks.</p> <p>The precision for defining amount is four decimal places. Therefore, the minimal resource amount that you can ask for is <code>0.0001</code>.</p> <p>For sum resources, the amount is simply removed from the pool as in the case of integer resources.</p> <p>In the case of an indexed resource, the partial resource is always taken from a single index. It means that if there is an indexed resource with two indices that are both utilized on 0.75, then a task that asks for 0.5 of this resource will not be started, despite there is available 0.5 of the resource in total, because there is no single index that is free at least on 0.5.</p> <p>If non-integer is bigger than 1, then integer part is always satisfied as whole indices and the rest is a part of another index. E.g., when you ask for 2.5 of an indexed resource, you will get 2 complete indices and one index allocated on 50%.</p> <p>Note</p> <p>In the current version, policy \"compact!\" is not allowed with non-integer amounts.</p>"},{"location":"jobs/resources/#resource-environment-variables","title":"Resource environment variables","text":"<p>When a task that has resource requests is executed, the following variables are passed to it for each resource request named <code>&lt;NAME&gt;</code>:</p> <ul> <li><code>HQ_RESOURCE_REQUEST_&lt;NAME&gt;</code> contains the amount of requested resources.</li> <li><code>HQ_RESOURCE_VALUES_&lt;NAME&gt;</code> contains the specific resource values allocated for the task as a   comma-separated list. This variable is only filled for an indexed resource pool.   In the case of non-integer amount, the partially allocated index is always the last index.</li> </ul> <p>The slash symbol (<code>/</code>) in resource name is normalized to underscore (<code>_</code>) when being used in the environment variable name.</p> <p>HQ also sets additional environment variables for various resources with special names:</p> <ul> <li>For the resource <code>gpus/nvidia</code>, HQ will set:<ul> <li><code>CUDA_VISIBLE_DEVICES</code> to the same value as <code>HQ_RESOURCE_VALUES_gpus_nvidia</code></li> <li><code>CUDA_DEVICE_ORDER</code> to <code>PCI_BUS_ID</code></li> </ul> </li> <li>For the resource <code>gpus/amd</code>, HQ will set:<ul> <li><code>ROCR_VISIBLE_DEVICES</code> to the same value as <code>HQ_RESOURCE_VALUES_gpus_amd</code></li> </ul> </li> </ul>"},{"location":"jobs/resources/#resource-requests-and-job-arrays","title":"Resource requests and job arrays","text":"<p>Resource requests are applied to each task of job. For example, if you submit the following:</p> <pre><code>$ hq submit --cpus=2 --array=1-10\n</code></pre> <p>then each task will require two cores.</p>"},{"location":"jobs/resources/#resource-variants","title":"Resource variants","text":"<p>A task may have attached more resource requests. There is no command line interface for this feature, but it can be configured through a Job Definition File.</p>"},{"location":"jobs/streaming/","title":"Output Streaming","text":"<p>Jobs containing many tasks will generate a large amount of <code>stdout</code> and <code>stderr</code> files, which can cause performance issues, especially on network-based shared filesystems, such as Lustre. For example, when you submit the following task array:</p> <pre><code>$ hq submit --array=1-10000 my-computation.sh\n</code></pre> <p><code>20000</code> files (<code>10000</code> for stdout and <code>10000</code> for stderr) will be created on the disk.</p> <p>To avoid this issue, HyperQueue can optionally stream the <code>stdout</code> and <code>stderr</code> output of tasks into a smaller number of files stored in a compact binary format.</p> <p>Note</p> <p>In this section, we refer to <code>stdout</code> and <code>stderr</code> as channels.</p> <p> </p>"},{"location":"jobs/streaming/#redirecting-task-output","title":"Redirecting task output","text":"<p>You can enable output streaming by using the <code>--stream</code> option of the <code>submit</code> command. You should pass it a path to a directory on disk where the streamed <code>stdout</code> and <code>stderr</code> output will be stored.</p> <pre><code>$ hq submit --stream=&lt;stream-dir&gt; --array=1-10_000 ...\n</code></pre> <p>Warning</p> <p>It is the user's responsibility to ensure that the <code>&lt;stream-dir&gt;</code> path is accessible and writable by each worker that might execute tasks of the submitted job. See also Working with a non-shared filesystem.</p> <p>The command above will cause the <code>stdout</code> and <code>stderr</code> of all <code>10_000</code> tasks to be streamed in a compact way into a small number of files located in <code>&lt;stream-dir&gt;</code>. Note that the number of files created in the directory will be independent of the number of tasks of the job, thus alleviating the performance issue on networked filesystems. The created binary files will also contain additional metadata, which allows the resulting files to be filtered/sorted by tasks or channel.</p> <p>Tip</p> <p>You can use selected placeholders inside the stream directory path.</p>"},{"location":"jobs/streaming/#partial-redirection","title":"Partial redirection","text":"<p>By default, both <code>stdout</code> and <code>stderr</code> will be streamed if you specify <code>--stream</code> and do not specify an explicit path for <code>stdout</code> and <code>stderr</code>. To stream only one of the channels, you can use the <code>--stdout</code>/<code>--stderr</code> options to redirect one of them to a file or to disable it completely.</p> <p>For example:</p> <pre><code># Redirecting stdout into a file, streaming stderr into `my-log`\n$ hq submit --stream=my-log --stdout=\"stdout-%{TASK_ID}\" ...\n\n# Streaming stdout into `my-log`, disabling stderr\n$ hq submit --stream=my-log --stderr=none ...\n</code></pre>"},{"location":"jobs/streaming/#guarantees","title":"Guarantees","text":"<p>HyperQueue provides the following guarantees regarding output streaming:</p> <p>When a task is <code>Finished</code> or <code>Failed</code> it is guaranteed that all data produced by the task is flushed into the streaming file. With the following two exceptions:</p> <ul> <li> <p>If the streaming itself fails (e.g. because there was insufficient disk space for the   stream file), then the task will fail with an error prefixed with <code>\"Streamer:\"</code> and no streaming guarantees   will be upheld.</p> </li> <li> <p>When a task is <code>Canceled</code> or task fails because of time limit is reached, then the part of   its stream that was buffered in the worker is dropped to avoid spending additional resources for this task.</p> </li> </ul>"},{"location":"jobs/streaming/#inspecting-the-stream-data","title":"Inspecting the stream data","text":"<p>HyperQueue lets you inspect the data stored inside the stream directory using various subcommands of <code>hq output-log</code>. All these commands have the following structure:</p> <pre><code>$ hq output-log &lt;stream-dir&gt; &lt;subcommand&gt; &lt;subcommand-args&gt;\n</code></pre>"},{"location":"jobs/streaming/#stream-summary","title":"Stream summary","text":"<p>You can display a summary of a stream directory using the <code>summary</code> subcommand:</p> <pre><code>$ hq output-log &lt;stream-dir&gt; summary\n</code></pre>"},{"location":"jobs/streaming/#stream-jobs","title":"Stream jobs","text":"<p>To print all job IDs that streaming in the stream directory, you can run the following command:</p> <pre><code>$ hq output-log &lt;stream-dir&gt; jobs\n</code></pre>"},{"location":"jobs/streaming/#printing-stream-content","title":"Printing stream content","text":"<p>If you want to simply print the (textual) content of the stream directory contents, without any associating metadata, you can use the <code>cat</code> subcommand:</p> <pre><code>$ hq output-log &lt;stream-dir&gt; cat &lt;job-id&gt; &lt;stdout/stderr&gt;\n</code></pre> <p>It will print the raw content of either <code>stdout</code> or <code>stderr</code>, ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script.</p> <p>By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the streaming directory). If you want to use <code>cat</code> even when streaming has not finished yet, use the <code>--allow-unfinished</code> option.</p> <p>If you want to see the output of a specific task, you can use the <code>--task=&lt;task-id&gt;</code> option.</p>"},{"location":"jobs/streaming/#stream-metadata","title":"Stream metadata","text":"<p>If you want to inspect the contents of the stream directory along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the <code>show</code> subcommand:</p> <pre><code>$ hq output-log &lt;stream-directory&gt; show\n</code></pre> <p>The output will have the form <code>J.T:C&gt; DATA</code> where <code>J</code> is a job id, <code>T</code> is a task id and <code>C</code> is <code>0</code> for <code>stdout</code> channel and <code>1</code> for <code>stderr</code> channel.</p> <p>You can filter a specific channel with the <code>--channel=stdout/stderr</code> flag.</p>"},{"location":"jobs/streaming/#exporting-the-stream-data","title":"Exporting the stream data","text":"<p>The contents of the stream directory can be exported into JSON by the following command:</p> <pre><code>$ hq output-log &lt;stream-dir&gt; export\n</code></pre> <p>This prints the streamed data into a JSON format to standard output.</p>"},{"location":"jobs/streaming/#superseded-streams","title":"Superseded streams","text":"<p>When a worker crashes while executing a task, the task will be restarted. HyperQueue gives each run of task a difference INSTANCE_ID, and it is a part of stream metadata, hence HyperQueue streaming is able to avoid mixing outputs from different executions of the same task, when a task is restarted.</p> <p>HyperQueue automatically marks all output from previous instance of a task except the last instance as superseded. You can see statistics about superseded data via <code>hq output-log &lt;stream-dir&gt; summary</code> command. In the current version, superseded data is ignored by all other commands.</p>"},{"location":"jobs/streaming/#multiple-server-instances","title":"Multiple server instances","text":"<p>HyperQueue supports writing streams from the different server instances into the same directory. If you run <code>hq output-log</code> commands over such directory then it will detect the situation and print all server UIDs that write into the directory. You have to specify the server instance via <code>hq output-log --server-uid=&lt;SERVER_UID&gt; ...</code> when working with such a streaming directory.</p> <p>Note</p> <p>When a server is restored from a journal file, it will maintain the same server UID. When a server is started \"from a scratch\" a new server UID is generated.</p>"},{"location":"jobs/streaming/#working-with-a-non-shared-filesystem","title":"Working with a non-shared filesystem","text":"<p>You do not need to have a shared filesystem when working with streaming. You just have to collect all generated files from the streaming directories in the different filesystems into a single directory before using the <code>hq output-log</code> commands.</p> <p>For example, you could use <code>/tmp/hq-stream</code> as a stream directory, which can be a local disk path on each worker, and then merge the contents of all such directories and use <code>hq output-log</code> on the resulting merged directory.</p>"},{"location":"python/","title":"Python API","text":"<p>To provide greater flexibility and support use-cases that are difficult to express using the CLI such as dynamically submitting tasks when some part is finished. Python API covers all task definition including all options available through Job Definition File (dependencies between tasks, resource variants, etc)</p> <p>You can find the HyperQueue Python API reference here.</p>"},{"location":"python/#requirements","title":"Requirements","text":"<p>To use the Python API, you will need at least Python 3.9 and some dependencies that will be installed automatically using pip.</p>"},{"location":"python/#installation","title":"Installation","text":"<p>You can install the HyperQueue Python API from <code>PyPi</code> with the following command:</p> <pre><code>$ python3 -m pip install hyperqueue\n</code></pre> <p>The Python package contains a pre-compiled version of HyperQueue, so you do not have to download <code>hq</code> manually if you just want to use the Python API.</p> <p>Warning</p> <p>The Python API is currently distributed only for the <code>x86-x64</code> architecture. If you need a build for another architecture, please contact us on GitHub.</p> <p>You can also build the Python package manually from our GitHub repository, but you will need to install a Rust toolchain for that.</p>"},{"location":"python/#quick-start","title":"Quick start","text":"<p>Here is a minimal code example that spawns a local HyperQueue cluster and uses it to submit a simple job:</p> <pre><code>from hyperqueue import Job, LocalCluster\n\n# Spawn a HQ server\nwith LocalCluster() as cluster:\n    # Add a single HyperQueue worker to the server\n    cluster.start_worker()\n\n    # Create a client and a job\n    client = cluster.client()\n    job = Job()\n\n    # Add a task that executes `ls` to the job\n    job.program([\"ls\"])\n\n    # Submit the job\n    submitted = client.submit(job)\n\n    # Wait until the job completes\n    client.wait_for_jobs([submitted])\n</code></pre>"},{"location":"python/client/","title":"Client","text":"<p>To submit jobs using the Python API, you first need to create a <code>Client</code> that connects to a running HyperQueue cluster. You have two options of deploying the cluster. Once you have an instance of a <code>Client</code>, you can use it to submit a job.</p>"},{"location":"python/client/#using-external-deployment","title":"Using external deployment","text":"<p>If you want to run the HyperQueue infrastructure on a distributed cluster or you want to use automatic allocation, then deploy HyperQueue in any of the supported ways and then pass the server directory to the <code>Client</code>:</p> <pre><code>from hyperqueue import Client\n\nclient = Client(\"/home/user/.hq-server/hq-current\")\n</code></pre> <p>If you have used the default server directory and the server is deployed on a file-system shared by the node that executes the Python code, you can simply create an instance of a <code>Client</code> without passing any parameters.</p>"},{"location":"python/client/#using-a-local-cluster","title":"Using a local cluster","text":"<p>You can use the <code>LocalCluster</code> class to spawn a HyperQueue server and a set of workers directly on your local machine. This functionality is primarily intended for local prototyping and debugging, but it can also be used for actual computations for simple use-cases that do not require a distributed deployment of HyperQueue.</p> <p>When you create the cluster, it will initially only start the HyperQueue server. To connect workers to it, use the <code>start_worker</code> method.</p> <pre><code>from hyperqueue import LocalCluster\nfrom hyperqueue.cluster import WorkerConfig\n\nwith LocalCluster() as cluster:\n    # Add a worker with 4 cores to the cluster\n    cluster.start_worker(WorkerConfig(cores=4))\n\n    # Create a client connected to the cluster\n    client = cluster.client()\n</code></pre> <p>Tip</p> <p>You can use <code>LocalCluster</code> instances as context managers to make sure that the cluster is properly cleaned up at the end of the <code>with</code> block.</p>"},{"location":"python/dependencies/","title":"Task dependencies","text":"<p>One of the most useful features of the HyperQueue Python API is that it allows you to define dependencies between individual tasks of a job.</p> <p>If a task <code>B</code> depends on task <code>A</code>, then <code>B</code> will not be executed until <code>A</code> has (successfully) finished. Using dependencies, you can describe arbitrarily complex DAG (directed acyclic graph) workflows.</p> <p>Note</p> <p>HyperQueue jobs are independent of each other, so dependencies can only be specified between tasks within a single job.</p>"},{"location":"python/dependencies/#defining-dependencies","title":"Defining dependencies","text":"<p>To define a dependency between tasks, you will first need to store the <code>Task</code> instances that you get when you create a task. You can then use the <code>deps</code> parameter when creating a new task and pass an existing task instance to define a dependency:</p> <pre><code>from hyperqueue import Job\n\njob = Job()\n\n# Create a first task that generates data\ntask_a = job.program([\"generate-data\", \"--file\", \"out.txt\"])\n\n# Create a dependent task that consumes the data\njob.program([\"consume-data\", \"--file\", \"out.txt\"], deps=[task_a])\n</code></pre> <p>The second task will not be started until the first one successfully finishes.</p> <p>You can also depend on multiple tasks at once: <pre><code># Create several tasks that generate data\ntasks = [job.program([\n    \"generate-data\",\n    \"--file\",\n    f\"out-{i}.txt\"\n]) for i in range(5)]\n\n# Create a dependent task that consumes the data\njob.program([\"consume-data\", \"--file\", \"out-%d.txt\"], deps=[tasks])\n</code></pre></p> <p>Dependencies are transitive, so you can build an arbitrary graph: <pre><code>task_a = job.program([\"generate\", \"1\"])\ntask_b = job.program([\"generate\", \"2\"])\n\ntask_c = job.program([\"compute\"], deps=[task_a, task_b])\n\ntask_d = job.program([\"postprocess\"], deps=[task_c])\n</code></pre> In this case, task <code>D</code> will not start until all the three previous tasks are successfully finished.</p>"},{"location":"python/submit/","title":"Submitting jobs","text":"<p>You can use the Python API to submit jobs (directed acyclic graphs of tasks) through a <code>Client</code>. In addition to the functionality offered by the HyperQueue CLI, you can use the Python API to add dependencies between jobs, configure each task individually and create tasks out of Python functions.</p>"},{"location":"python/submit/#job","title":"Job","text":"<p>To build a job, you first have to create an instance of the <code>Job</code> class.</p> <pre><code>from hyperqueue import Job\n\njob = Job()\n</code></pre>"},{"location":"python/submit/#tasks","title":"Tasks","text":"<p>Once you have created a job, you can add tasks to it. Currently, each task can represent either the execution of an external program or the execution of a Python function.</p> <p>To create complex workflows, you can also specify dependencies between tasks.</p>"},{"location":"python/submit/#external-programs","title":"External programs","text":"<p>To create a task that will execute an external program, you can use the <code>program</code> method of a <code>Job</code>:</p> <pre><code>job.program([\"/bin/my-program\", \"foo\", \"bar\", \"--arg\", \"42\"])\n</code></pre> <p>You can pass the program arguments or various other parameters to the task. The <code>program</code> method will return a <code>Task</code> object that represents the created task. This object can be used further e.g. for defining dependencies.</p>"},{"location":"python/submit/#python-functions","title":"Python functions","text":"<p>If you want to execute a Python function as a task, you can use the <code>function</code> method of a <code>Job</code>:</p> <pre><code>def preprocess_data(fast, path):\n    with open(path) as f:\n        data = f.read()\n    if fast:\n        preprocess_fast(data)\n    else:\n        preprocess(data)\n\njob.function(preprocess_data, args=(True, \"/data/a.txt\"))\njob.function(preprocess_data, args=(False, \"/data/b.txt\"))\n</code></pre> <p>You can pass both positional and keyword arguments to the function. The arguments will be serialized using cloudpickle.</p> <p>Python tasks can be useful to perform e.g. various data preprocessing and organization tasks. You can co-locate the logic of Python tasks together with the code that defines the submitted workflow (job), without the need to write an additional external script.</p> <p>Same as with the <code>program</code> method, <code>function</code> will return a <code>Task</code> that can used to define dependencies.</p> <p>Note</p> <p>Currently, a new Python interpreter will be started for each Python task.</p>"},{"location":"python/submit/#python-environment","title":"Python environment","text":"<p>When you use a Python function as a task, the task will attempt to import the <code>hyperqueue</code> package when it executes (to perform some bookkeeping on the background). This function will be executed on a worker - this means that it needs to have access to the correct Python version (and virtual environment) that contains the <code>hyperqueue</code> package!</p> <p>To make sure that the function will be executed in the correct Python environment, you can use <code>PythonEnv</code> and its <code>prologue</code> argument. It lets you specify a (shell) command that will be executed before the Python interpreter that executes your function is spawned.</p> <pre><code>from hyperqueue.task.function import PythonEnv\nfrom hyperqueue import Client\n\nenv = PythonEnv(\n    prologue=\"ml Python/XYZ &amp;&amp; source /&lt;my-path-to-venv&gt;/bin/activate\"\n)\nclient = Client(python_env=env)\n</code></pre> <p>If you use Python functions as tasks, it is pretty much required to use <code>PythonEnv</code>, unless your workers are already spawned in an environment that has the correct Python loaded (e.g. using <code>.bashrc</code> or a similar mechanism).</p>"},{"location":"python/submit/#parametrizing-tasks","title":"Parametrizing tasks","text":"<p>You can parametrize both external or Python tasks by setting their working directory, standard output paths, environment variables or HyperQueue specific parameters like resources or time limits. In contrast to the CLI, where you can only use a single set of parameters for all tasks of a job, with the Python API you can specify these parameters individually for each task.</p> <p>You can find more details in the documentation of the <code>program</code> or <code>function</code> methods.</p>"},{"location":"python/submit/#submitting-a-job","title":"Submitting a job","text":"<p>Once you have added some tasks to the job, you can submit it using the <code>Client</code>'s <code>submit</code> method:</p> <pre><code>client = Client()\nsubmitted = client.submit(job)\n</code></pre> <p>To wait until the job has finished executing, use the <code>wait_for_jobs</code> method:</p> <pre><code>client.wait_for_jobs([submitted])\n</code></pre>"}]}