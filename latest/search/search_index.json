{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HyperQueue is a tool designed to simplify execution of large workflows on HPC clusters. It allows you to execute a large number of tasks in a simple way, without having to manually submit jobs into batch schedulers like PBS or Slurm. You just specify what you want to compute \u2013 HyperQueue will automatically ask for computational resources and dynamically load-balance tasks across all allocated nodes and cores. Useful links # Installation Quick start Repository Features # Automatic management of batch jobs HQ automatically asks for computing resources Computation is distributed amongst all a Performance The inner scheduler can scale to hundreds of nodes The overhead for one task is below 0.1ms. HQ allows to stream outputs from tasks to avoid creating many small files on a distributed filesystem Easy deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed to use HQ Architecture # HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. You can find more information about the architecture of HyperQueue here .","title":"Overview"},{"location":"#useful-links","text":"Installation Quick start Repository","title":"Useful links"},{"location":"#features","text":"Automatic management of batch jobs HQ automatically asks for computing resources Computation is distributed amongst all a Performance The inner scheduler can scale to hundreds of nodes The overhead for one task is below 0.1ms. HQ allows to stream outputs from tasks to avoid creating many small files on a distributed filesystem Easy deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed to use HQ","title":"Features"},{"location":"#architecture","text":"HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. You can find more information about the architecture of HyperQueue here .","title":"Architecture"},{"location":"alloc/","text":"Automatic Allocation # The goal of automatic allocation is to autonomously allocate resources for workers in HPC scheduler (PBS/SLURM). In other words, it submits SLURM/PBS jobs as needed. Note: This is feature is in a preview mode. It works but semantics, default configuration, or API could be changed in future versions. To solve the terminology clash between HQ jobs and PBS/SLURM jobs, we will call the latter as \"mjobs\" (= manager jobs, where we use manager as an umbrella term for PBS or SLURM). Behavior # The current version works as follows: If there are waiting tasks in HQ, then HQ server tries to submit and maintain a given amount of WAITING mjobs in manager. This number can be configured and default value is 4. Note that this does not limit the total number of running workers, only waiting mjobs in queue. If a cluster is empty and HPC scheduler gives starts our mjobs then it will eventually allocate the whole cluster. If there no waiting tasks then the submission of mjobs are stopped. Each worker submited through auto allocation is configured that it will terminates after 5 minutes without tasks. Example # Start auto allocation over PBS in queue \"qexp\": $ hq alloc add pbs --queue qexp Start auto allocation over SLURM in queue \"qexp\": $ hq alloc add slurm --partition qexp Passing additional arguments # You can also add arbitrary arguments to the hq alloc add command, which will be forwarded to qsub or sbatch : $ hq alloc add pbs --queue qprod -- -A PROJ-1 Information about allocations # List of allocations: $ hq alloc list Allocation details: $ hq alloc info <ID> Events for a queue: $ hq alloc events <ID> Removing allocation # $ hq alloc remove <ID>","title":"Automatic allocation"},{"location":"alloc/#automatic-allocation","text":"The goal of automatic allocation is to autonomously allocate resources for workers in HPC scheduler (PBS/SLURM). In other words, it submits SLURM/PBS jobs as needed. Note: This is feature is in a preview mode. It works but semantics, default configuration, or API could be changed in future versions. To solve the terminology clash between HQ jobs and PBS/SLURM jobs, we will call the latter as \"mjobs\" (= manager jobs, where we use manager as an umbrella term for PBS or SLURM).","title":"Automatic Allocation"},{"location":"alloc/#behavior","text":"The current version works as follows: If there are waiting tasks in HQ, then HQ server tries to submit and maintain a given amount of WAITING mjobs in manager. This number can be configured and default value is 4. Note that this does not limit the total number of running workers, only waiting mjobs in queue. If a cluster is empty and HPC scheduler gives starts our mjobs then it will eventually allocate the whole cluster. If there no waiting tasks then the submission of mjobs are stopped. Each worker submited through auto allocation is configured that it will terminates after 5 minutes without tasks.","title":"Behavior"},{"location":"alloc/#example","text":"Start auto allocation over PBS in queue \"qexp\": $ hq alloc add pbs --queue qexp Start auto allocation over SLURM in queue \"qexp\": $ hq alloc add slurm --partition qexp","title":"Example"},{"location":"alloc/#passing-additional-arguments","text":"You can also add arbitrary arguments to the hq alloc add command, which will be forwarded to qsub or sbatch : $ hq alloc add pbs --queue qprod -- -A PROJ-1","title":"Passing additional arguments"},{"location":"alloc/#information-about-allocations","text":"List of allocations: $ hq alloc list Allocation details: $ hq alloc info <ID> Events for a queue: $ hq alloc events <ID>","title":"Information about allocations"},{"location":"alloc/#removing-allocation","text":"$ hq alloc remove <ID>","title":"Removing allocation"},{"location":"arrays/","text":"Task arrays # Task arrays is a mechanism for submitting many tasks at once. It allows to create a job with many tasks in a single submit and monitor and manage them as a single group. Note: From the user perspective, it is functionally equivalent for \"job arrays\" in SLURM/PBS; however, HQ does not use SLURM/PBS job arrays for providing this feature. HyperQueue's task arrays are handled as any other tasks, e.g. it may happen that two tasks from one array may run simultaneously in the same worker if there are enough resources. Submitting task array # The following submits 100 tasks in a single job with task ids 1-100: $ hq submit --array 1-100 <program> <args1> ... Generally, task ids may be specified with the following syntax (X and Y are unsigned integers): X-Y - Include range from X to Y X - An array with a single element X Env variables # When a task is started then the following environment variables are created: HQ_JOB_ID - Job id HQ_TASK_ID - Task id Task states # Each task has its own individual state as defined in the previous chapter . The number of tasks with each state can be displayed by the following command: $ hq job <job_id> Detailed state of each task will also be included if you pass the --tasks flag: $ hq job <job_id> --tasks A global job state for summary outputs is derived from the state of its tasks by the first rule that matches from the following list of rules: If at least one task is in state \"Running\", then job state is \"Running\". If at least one task has not been computed yet, then job state is \"Waiting\". A task has been computed once it has reached the canceled , failed or finished state. If at least one task is in state \"Canceled\" then job state is \"Canceled\". If at least one task is in state \"Failed\" then job state is \"Failed\". All tasks have to be in state \"Finished\", therefore the job state will also be \"Finished\". Task fail in array job # By default, when a task fails the computation of job continues. You can change it by --max-fails=X where X is non-negative integer. If more tasks then X fails, then the rest of non-finished tasks are canceled. Time limit # Time limit ( --time-limit ) is counted for each task separatatelly. Job canceling # When a job with more tasks is canceled then all non-finished tasks is canceled. Task for each line of a file # The switch --each-line=<FILE> is an alternative way to create an array job. It creates a job for each line of the file. The content of a line is stored in variable HQ_ENTRY . Example: $ hq submit --each-line /path/to/file my-program.sh","title":"Task Arrays"},{"location":"arrays/#task-arrays","text":"Task arrays is a mechanism for submitting many tasks at once. It allows to create a job with many tasks in a single submit and monitor and manage them as a single group. Note: From the user perspective, it is functionally equivalent for \"job arrays\" in SLURM/PBS; however, HQ does not use SLURM/PBS job arrays for providing this feature. HyperQueue's task arrays are handled as any other tasks, e.g. it may happen that two tasks from one array may run simultaneously in the same worker if there are enough resources.","title":"Task arrays"},{"location":"arrays/#submitting-task-array","text":"The following submits 100 tasks in a single job with task ids 1-100: $ hq submit --array 1-100 <program> <args1> ... Generally, task ids may be specified with the following syntax (X and Y are unsigned integers): X-Y - Include range from X to Y X - An array with a single element X","title":"Submitting task array"},{"location":"arrays/#env-variables","text":"When a task is started then the following environment variables are created: HQ_JOB_ID - Job id HQ_TASK_ID - Task id","title":"Env variables"},{"location":"arrays/#task-states","text":"Each task has its own individual state as defined in the previous chapter . The number of tasks with each state can be displayed by the following command: $ hq job <job_id> Detailed state of each task will also be included if you pass the --tasks flag: $ hq job <job_id> --tasks A global job state for summary outputs is derived from the state of its tasks by the first rule that matches from the following list of rules: If at least one task is in state \"Running\", then job state is \"Running\". If at least one task has not been computed yet, then job state is \"Waiting\". A task has been computed once it has reached the canceled , failed or finished state. If at least one task is in state \"Canceled\" then job state is \"Canceled\". If at least one task is in state \"Failed\" then job state is \"Failed\". All tasks have to be in state \"Finished\", therefore the job state will also be \"Finished\".","title":"Task states"},{"location":"arrays/#task-fail-in-array-job","text":"By default, when a task fails the computation of job continues. You can change it by --max-fails=X where X is non-negative integer. If more tasks then X fails, then the rest of non-finished tasks are canceled.","title":"Task fail in array job"},{"location":"arrays/#time-limit","text":"Time limit ( --time-limit ) is counted for each task separatatelly.","title":"Time limit"},{"location":"arrays/#job-canceling","text":"When a job with more tasks is canceled then all non-finished tasks is canceled.","title":"Job canceling"},{"location":"arrays/#task-for-each-line-of-a-file","text":"The switch --each-line=<FILE> is an alternative way to create an array job. It creates a job for each line of the file. The content of a line is stored in variable HQ_ENTRY . Example: $ hq submit --each-line /path/to/file my-program.sh","title":"Task for each line of a file"},{"location":"cheatsheet/","text":"Cheatsheet #","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"","title":"Cheatsheet"},{"location":"cpus/","text":"CPU management # Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU. Requesting more CPUs # By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus. Requesting all CPUs # Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...> Pinning # By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically. Automatic pinning # If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process. Manual pinning # If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> NUMA allocation policy # HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\" CPU requests and job arrays # Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU management"},{"location":"cpus/#cpu-management","text":"Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU.","title":"CPU management"},{"location":"cpus/#requesting-more-cpus","text":"By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus.","title":"Requesting more CPUs"},{"location":"cpus/#requesting-all-cpus","text":"Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...>","title":"Requesting all CPUs"},{"location":"cpus/#pinning","text":"By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically.","title":"Pinning"},{"location":"cpus/#automatic-pinning","text":"If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process.","title":"Automatic pinning"},{"location":"cpus/#manual-pinning","text":"If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...>","title":"Manual pinning"},{"location":"cpus/#numa-allocation-policy","text":"HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\"","title":"NUMA allocation policy"},{"location":"cpus/#cpu-requests-and-job-arrays","text":"Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU requests and job arrays"},{"location":"deployment/","text":"Deployment # This section describes, how to HyperQueue server Starting server # Server may run on any computer as long as computing nodes are able to connect to these machine. It is not necessary to be able to connect from server to computing nodes. In the most simple scenario, we expect that the user starts its own instance of HyperQueue directly on login of a HPC system. The server can be simply started by the following command: hq server start Note: The server opens two TCP/IP ports: one for submitting jobs and one for connecting workers. By default, these ports are automatically assigned by the operation system. A user does not remmber them, they are stored in the \"server directory\". Other components automatically reads these settings. Server directory # When a HQ server is started, it creates a server directory where it stores informations needed for submiting jobs and connecting workers. Important: Encryption keys are stored in the server directory. Who has access to server directory may submit jobs, connect workers to HyperQueue instance, and decrypt communication between HyperQueue components. By default, server directory is stored in $HOME/.hq-server . It may be changed via option --server-dir=<PATH> . In such case, all commands need to use the --server-dir settings. You can run more instances of HyperQueue under the same user. All you need is to set a different server directories for each instance. Stopping server # A server can be stopped by command: hq server stop Starting worker # A worker can be started by command. It reads server directory and connectes to the server. hq worker start Starting worker in PBS # Start worker on the first node of a PBS job qsub <qsub-settings> -- hq worker start Start worker on all nodes of a PBS job $ qsub <your-params-of-qsub> -- `which pbsdsh` hq worker start Starting worker in SLURM # Start worker on the first node of a SLURM job sbatch <your-params-of-sbatch> --wrap \"hq worker start\" Start worker on all nodes of a SLURM job`` $ sbatch <your-params-of-sbatch> --wrap \"srun hq worker start\" Worker's Time limit # When a worker is started in PBS or SLURM, it automatically gets the time limit from the outer system and propagates it into HQ scheduler. If you want to set time limit for workers outside of PBS or SLURM (or you want to override the detected settings), then there is an option --time-limit=DURATION (e.g. hq worker start --time-limit=2h ). If time limit is reached, the worker is terminated. List of workers # hq worker list State of workers: Running - Worker is running and is able to process tasks Connection lost - Worker closes connection. Probably someone manually killed the worker or wall time in PBS/SLURM job was reached Heartbeat lost - Communication between server and worker was interrputed. It usually means a network problem or an hardware crash of the computational node Stopped - Worker was stopped by hq worker stop ... Idle timeout - Idle timeout is enabled on server and worker did not received any task for more then the limit. Stopping worker # Stop a specific worker: hq worker stop <id> Stop all workers: hq worker stop all CPUs configuration # Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout. Manual specification of CPU configration # If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12 Idle timeout # Idle timeout allows to automatically stop a worker when no tasks are given to the worker for a specified time. When a worker is started, an idle timeout may be configurated via --idle-timeout=<TIMEOUT> for hq worker start where TIMEOUT is a string like \"2min 30s\" . (All possible formats are documented at https://docs.rs/humantime/2.1.0/humantime/fn.parse_duration.html). Idle timeout can be also configured for all workers at once by hq server start --idle-timeout=<TIMEOUT> . This value is then used for each worker that does not explicitly specifies its own timeout. Server address # By default, the server stores its own hostname as an address for connection of clients and workers. This can be changed by hq server start --host=HOST , where HOST is a hostname/address under which is server visible.","title":"Deployment"},{"location":"deployment/#deployment","text":"This section describes, how to HyperQueue server","title":"Deployment"},{"location":"deployment/#starting-server","text":"Server may run on any computer as long as computing nodes are able to connect to these machine. It is not necessary to be able to connect from server to computing nodes. In the most simple scenario, we expect that the user starts its own instance of HyperQueue directly on login of a HPC system. The server can be simply started by the following command: hq server start Note: The server opens two TCP/IP ports: one for submitting jobs and one for connecting workers. By default, these ports are automatically assigned by the operation system. A user does not remmber them, they are stored in the \"server directory\". Other components automatically reads these settings.","title":"Starting server"},{"location":"deployment/#server-directory","text":"When a HQ server is started, it creates a server directory where it stores informations needed for submiting jobs and connecting workers. Important: Encryption keys are stored in the server directory. Who has access to server directory may submit jobs, connect workers to HyperQueue instance, and decrypt communication between HyperQueue components. By default, server directory is stored in $HOME/.hq-server . It may be changed via option --server-dir=<PATH> . In such case, all commands need to use the --server-dir settings. You can run more instances of HyperQueue under the same user. All you need is to set a different server directories for each instance.","title":"Server directory"},{"location":"deployment/#stopping-server","text":"A server can be stopped by command: hq server stop","title":"Stopping server"},{"location":"deployment/#starting-worker","text":"A worker can be started by command. It reads server directory and connectes to the server. hq worker start","title":"Starting worker"},{"location":"deployment/#starting-worker-in-pbs","text":"Start worker on the first node of a PBS job qsub <qsub-settings> -- hq worker start Start worker on all nodes of a PBS job $ qsub <your-params-of-qsub> -- `which pbsdsh` hq worker start","title":"Starting worker in PBS"},{"location":"deployment/#starting-worker-in-slurm","text":"Start worker on the first node of a SLURM job sbatch <your-params-of-sbatch> --wrap \"hq worker start\" Start worker on all nodes of a SLURM job`` $ sbatch <your-params-of-sbatch> --wrap \"srun hq worker start\"","title":"Starting worker in SLURM"},{"location":"deployment/#workers-time-limit","text":"When a worker is started in PBS or SLURM, it automatically gets the time limit from the outer system and propagates it into HQ scheduler. If you want to set time limit for workers outside of PBS or SLURM (or you want to override the detected settings), then there is an option --time-limit=DURATION (e.g. hq worker start --time-limit=2h ). If time limit is reached, the worker is terminated.","title":"Worker's Time limit"},{"location":"deployment/#list-of-workers","text":"hq worker list State of workers: Running - Worker is running and is able to process tasks Connection lost - Worker closes connection. Probably someone manually killed the worker or wall time in PBS/SLURM job was reached Heartbeat lost - Communication between server and worker was interrputed. It usually means a network problem or an hardware crash of the computational node Stopped - Worker was stopped by hq worker stop ... Idle timeout - Idle timeout is enabled on server and worker did not received any task for more then the limit.","title":"List of workers"},{"location":"deployment/#stopping-worker","text":"Stop a specific worker: hq worker stop <id> Stop all workers: hq worker stop all","title":"Stopping worker"},{"location":"deployment/#cpus-configuration","text":"Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout.","title":"CPUs configuration"},{"location":"deployment/#manual-specification-of-cpu-configration","text":"If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12","title":"Manual specification of CPU configration"},{"location":"deployment/#idle-timeout","text":"Idle timeout allows to automatically stop a worker when no tasks are given to the worker for a specified time. When a worker is started, an idle timeout may be configurated via --idle-timeout=<TIMEOUT> for hq worker start where TIMEOUT is a string like \"2min 30s\" . (All possible formats are documented at https://docs.rs/humantime/2.1.0/humantime/fn.parse_duration.html). Idle timeout can be also configured for all workers at once by hq server start --idle-timeout=<TIMEOUT> . This value is then used for each worker that does not explicitly specifies its own timeout.","title":"Idle timeout"},{"location":"deployment/#server-address","text":"By default, the server stores its own hostname as an address for connection of clients and workers. This can be changed by hq server start --host=HOST , where HOST is a hostname/address under which is server visible.","title":"Server address"},{"location":"faq/","text":"How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs to the server. You may have hundreds of thousands of jobs; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign jobs to them. Workers are fully and dynamically controlled by server; you do not need to specify what jobs are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a job in HQ Right now, we support running arbitrary external programs or bash scripts. We plan to support Python defined workflows (with a Dask-like API). How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. (See example below). Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to smoothly work on systems using SLURM/PBS, they are not required for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely no. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on a HPC system each user (or a group of users that trust each other) may run her own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to it file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running. What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of poor load balancing. In the case of HQ, you do not have to aggregate jobs. You can submit millions of small jobs to HQ and it will take care of assigning them dynamically to individual SLURM/PBS jobs and workers. How many jobs may I submit into HQ? Our preliminary benchmarks show that overhead of HQ is around 0.1 ms per task. We also support streaming of task outputs into a single file (this file contains metadata, hence outputs for each task can be filtered or ordered). This avoids creating many small files for each task on a distributed file system that may have a large impact on scaling. Does HQ support multi-CPU jobs? Yes. You can define number of CPUs for each job. HQ is NUMA aware and you can choose the allocation strategy. Does HQ support job arrays? Yes. Does HQ support jobs with dependencies? Not yet. It is actually implemented in the scheduling core, but it has no user interface yet. But we consider it as a crucial must-have feature. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python. Who stands behind HyperQueue? We are a group of researchers interested in distributed computing and machine learning. We operate at IT4Innovations , the Czech National Supercomputing Center. We welcome any contribution from outside.","title":"FAQ"},{"location":"install/","text":"Installation # Binary distribution # Download latest binary distribution from https://github.com/It4innovations/hyperqueue/releases/latest Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-x64.tar.gz Compilation from source codes # Requirements: Git, Rust Clone HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build project: $ cargo build --release Final executable file will in ./target/release/hq","title":"Installation"},{"location":"install/#installation","text":"","title":"Installation"},{"location":"install/#binary-distribution","text":"Download latest binary distribution from https://github.com/It4innovations/hyperqueue/releases/latest Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-x64.tar.gz","title":"Binary distribution"},{"location":"install/#compilation-from-source-codes","text":"Requirements: Git, Rust Clone HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build project: $ cargo build --release Final executable file will in ./target/release/hq","title":"Compilation from source codes"},{"location":"jobs/","text":"Jobs # A job is a portion of work that can be submitted into a HyperQueue. Each job may have one or tasks . In the current version, a is a single execution of a program. In this section, we are introducing simple jobs, where each job has exactly one tasks. See section about Job arrays for submitting more tasks in one job. Identification numbers # Each job is identified by a positive integer that is assigned by HyperQueue server when the job is submitted, we refer to it as job id . Each task is identified by an unsigned 32b integer called task id . Task id can be assigned by a user and same task id may be used in two different jobs. In simple jobs, task id is set to 0. Submitting jobs # hq submit <program> <args1> ... HyperQueue assigns a unique job id when a job is submitted. When your command contains its own switches, you need to use -- after submit: hq submit -- /bin/bash -c 'echo $PPID' Name of a job # Each job has an assigned name. It has only an informative character for the user. By default, the name is extracted from the job's program name. You can set a job name explicitly by: hq submit --name=<NAME> ... Working directory of a job # You can change the working directory of a job using the --cwd parameter. By default it is set to the directory from which was the job submitted. !!! Hint You can use [placeholders](#placeholders) in the working directory path. Output of the job # !!! Warning If you want to avoid creating many files, see the section about streaming By default, each job will produce two files containing the standard output and standard error output, respectively. The paths where these files will be created can be changed via the parameters --stdout=<path> and --stderr=<path> . You can also disable creating stdout/stderr completely by setting value none . $ hq submit --stdout=none ... The default values for these paths are job-%{JOB_ID}/%{TASK_ID}.stdout and job-%{JOB_ID}/%{TASK_ID}.stderr . You can read about the %{JOB_ID} and %{TASK_ID} placeholders below . !!! Hint You can use [placeholders](#placeholders) in the `stdout` and `stderr` paths. Placeholders # You can use special variables in working directory, stdout and stderr paths, which will be interpolated with job/task-specific information before the job is executed. Placeholders are enclosed in curly braces and prepended with a percent sign. Currently, you can use the following placeholders: Placeholder Value %{JOB_ID} Job ID. %{TASK_ID} Task ID. %{INSTANCE_ID} Instance ID (see below) %{SUBMIT_DIR} Directory from which the job was submitted. %{CWD} Working directory of the job. This placeholder is only available for stdout and stderr paths. %{DATE} Current date when the job was executed in the RFC3339 format. Setting env variables # In a submit of a task, you can set an environment variable named KEY with the value VAL by: --env KEY=VAL You can pass the following flag multiple times to pass multiple variables. Information about jobs # List of all jobs: hq jobs List of jobs in a defined state: hq jobs <filters...> Where filters can be: waiting , running , finished , failed , or canceled . Detailed information about a job: hq job <job-id> !!! Hint You can also use `hq job last` to get information about the most recently submitted job. Task states # Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted - Only an informative state that a submission was successful; it is only shown immediately after a submit. Waiting - The task is waiting to be executed. Running - The task is running in a worker. It may become \"waiting\" again when a worker (where the task is running) is lost. Finished - The task has successfully finished. Failed - The task has failed. The error can be shown by hq job <job-id> . Canceled - The task has been canceled by a user. Time management # Time management is split into two settings: time limit - It is the maximal running time of a task. If it is reached then task is terminated and set into FAILED state. This setting does not have any impact on scheduling. time request - The minimal remaining lifetime of a worker needed to start the task. Workers that do not have enough remaining lifetime will not be scheduled to run this task. Once the task is scheduled and starts executing, it does not matter w.r.t this setting for how long it will actually run. Workers without known remaining lifetime is always assumed that it may execute any time request. We have separated these settings to solve the following scenario. Lets us assume that we have a collection of task where the vast majority of tasks usually finish within 10 minutes but some of them run 30 minutes. We do not know in advance which tasks are slow. In this situation, we want to set the time limit to 35 minutes to protect us against an error. However, we want to schedule the task to worker as long as it has at least 10 minutes of lifetime despite we are risking of rare losing some computation time when worker dies before the computation is finished. (Note that in this situation, a task is rescheduled to another worker; we have only lost some computational power and not the whole task.) Time limit # Time limit is set as follows: hq submit --time-limit=TIME ... Where TIME is a number followed by units (e.g. 10 min ) Time request # Time request is set as follows: hq submit --time-request=TIME Time Units # You can use the following units: msec, ms -- milliseconds seconds, second, sec, s minutes, minute, min, m hours, hour, hr, h days, day, d weeks, week, w months, month, M -- defined as 30.44 days years, year, y -- defined as 365.25 days Time can be also a combination of more units: hq submit --time-limit=\"1h 30min\" ... Task instance # It may happen that a task is started more than once when a worker crashes during execution of a task and the task is rescheduled to another worker. Instance IDs exist to distinguish each run when necessary. Instance ID is 32b non-negative number and it is guarantted that the newer execution has a bigger value. HyperQueue explicitly does not guarantee any specific value or differences between two ids. Instance ID is valid only for a particular task. Two different tasks may have the same instance ID. Job states # In simple jobs, job state corresponds directly to the state of its single task. In the case of task arrays, see the chapter about task arrays . Canceling jobs # A job cannot be canceled if it is already finished, failed, or canceled. Cancel specific job: hq cancel <job-id> Cancel all jobs: hq cancel all Cancel last submitted job: hq cancel last Waiting for jobs # You can submit a job with flag --wait and HQ will wait until the submitted job is not terminated (until all tasks are either finished, failed or canceled). You can also use hq wait <job-id> to wait for a specific job or hq wait last to wait for the last submitted job or hq wait all to wait for all jobs. If you want to interactively observe the status of the jobs while waiting for them to finish, use hq submit --progress or hq progress <job-id> instead. Priorities # Priorities affect the order in which the \"waiting\" tasks are executed. Priority can be any 32b signed integer. A lowest number marks the lowest priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker, and only one of them may be executed, then A will be executed first. In a submit of a task, you can set priority as follows hq submit --priority=<PRIORITY> If no priority is specified, then task has priority 0. Resubmit # If you want to recompute a previous job, jou can use: hq resubmit <job-id> that creates a new job with the exact the same configuration as the source job. By default, it submits all tasks of the original job; however, you can specify only a subset of tasks, e.g. hq resubmit <job-id> --status=failed,canceled Resubmits only tasks that failed or were canceled.","title":"Submitting jobs"},{"location":"jobs/#jobs","text":"A job is a portion of work that can be submitted into a HyperQueue. Each job may have one or tasks . In the current version, a is a single execution of a program. In this section, we are introducing simple jobs, where each job has exactly one tasks. See section about Job arrays for submitting more tasks in one job.","title":"Jobs"},{"location":"jobs/#identification-numbers","text":"Each job is identified by a positive integer that is assigned by HyperQueue server when the job is submitted, we refer to it as job id . Each task is identified by an unsigned 32b integer called task id . Task id can be assigned by a user and same task id may be used in two different jobs. In simple jobs, task id is set to 0.","title":"Identification numbers"},{"location":"jobs/#submitting-jobs","text":"hq submit <program> <args1> ... HyperQueue assigns a unique job id when a job is submitted. When your command contains its own switches, you need to use -- after submit: hq submit -- /bin/bash -c 'echo $PPID'","title":"Submitting jobs"},{"location":"jobs/#name-of-a-job","text":"Each job has an assigned name. It has only an informative character for the user. By default, the name is extracted from the job's program name. You can set a job name explicitly by: hq submit --name=<NAME> ...","title":"Name of a job"},{"location":"jobs/#working-directory-of-a-job","text":"You can change the working directory of a job using the --cwd parameter. By default it is set to the directory from which was the job submitted. !!! Hint You can use [placeholders](#placeholders) in the working directory path.","title":"Working directory of a job"},{"location":"jobs/#output-of-the-job","text":"!!! Warning If you want to avoid creating many files, see the section about streaming By default, each job will produce two files containing the standard output and standard error output, respectively. The paths where these files will be created can be changed via the parameters --stdout=<path> and --stderr=<path> . You can also disable creating stdout/stderr completely by setting value none . $ hq submit --stdout=none ... The default values for these paths are job-%{JOB_ID}/%{TASK_ID}.stdout and job-%{JOB_ID}/%{TASK_ID}.stderr . You can read about the %{JOB_ID} and %{TASK_ID} placeholders below . !!! Hint You can use [placeholders](#placeholders) in the `stdout` and `stderr` paths.","title":"Output of the job"},{"location":"jobs/#placeholders","text":"You can use special variables in working directory, stdout and stderr paths, which will be interpolated with job/task-specific information before the job is executed. Placeholders are enclosed in curly braces and prepended with a percent sign. Currently, you can use the following placeholders: Placeholder Value %{JOB_ID} Job ID. %{TASK_ID} Task ID. %{INSTANCE_ID} Instance ID (see below) %{SUBMIT_DIR} Directory from which the job was submitted. %{CWD} Working directory of the job. This placeholder is only available for stdout and stderr paths. %{DATE} Current date when the job was executed in the RFC3339 format.","title":"Placeholders"},{"location":"jobs/#setting-env-variables","text":"In a submit of a task, you can set an environment variable named KEY with the value VAL by: --env KEY=VAL You can pass the following flag multiple times to pass multiple variables.","title":"Setting env variables"},{"location":"jobs/#information-about-jobs","text":"List of all jobs: hq jobs List of jobs in a defined state: hq jobs <filters...> Where filters can be: waiting , running , finished , failed , or canceled . Detailed information about a job: hq job <job-id> !!! Hint You can also use `hq job last` to get information about the most recently submitted job.","title":"Information about jobs"},{"location":"jobs/#task-states","text":"Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted - Only an informative state that a submission was successful; it is only shown immediately after a submit. Waiting - The task is waiting to be executed. Running - The task is running in a worker. It may become \"waiting\" again when a worker (where the task is running) is lost. Finished - The task has successfully finished. Failed - The task has failed. The error can be shown by hq job <job-id> . Canceled - The task has been canceled by a user.","title":"Task states"},{"location":"jobs/#time-management","text":"Time management is split into two settings: time limit - It is the maximal running time of a task. If it is reached then task is terminated and set into FAILED state. This setting does not have any impact on scheduling. time request - The minimal remaining lifetime of a worker needed to start the task. Workers that do not have enough remaining lifetime will not be scheduled to run this task. Once the task is scheduled and starts executing, it does not matter w.r.t this setting for how long it will actually run. Workers without known remaining lifetime is always assumed that it may execute any time request. We have separated these settings to solve the following scenario. Lets us assume that we have a collection of task where the vast majority of tasks usually finish within 10 minutes but some of them run 30 minutes. We do not know in advance which tasks are slow. In this situation, we want to set the time limit to 35 minutes to protect us against an error. However, we want to schedule the task to worker as long as it has at least 10 minutes of lifetime despite we are risking of rare losing some computation time when worker dies before the computation is finished. (Note that in this situation, a task is rescheduled to another worker; we have only lost some computational power and not the whole task.)","title":"Time management"},{"location":"jobs/#time-limit","text":"Time limit is set as follows: hq submit --time-limit=TIME ... Where TIME is a number followed by units (e.g. 10 min )","title":"Time limit"},{"location":"jobs/#time-request","text":"Time request is set as follows: hq submit --time-request=TIME","title":"Time request"},{"location":"jobs/#time-units","text":"You can use the following units: msec, ms -- milliseconds seconds, second, sec, s minutes, minute, min, m hours, hour, hr, h days, day, d weeks, week, w months, month, M -- defined as 30.44 days years, year, y -- defined as 365.25 days Time can be also a combination of more units: hq submit --time-limit=\"1h 30min\" ...","title":"Time Units"},{"location":"jobs/#task-instance","text":"It may happen that a task is started more than once when a worker crashes during execution of a task and the task is rescheduled to another worker. Instance IDs exist to distinguish each run when necessary. Instance ID is 32b non-negative number and it is guarantted that the newer execution has a bigger value. HyperQueue explicitly does not guarantee any specific value or differences between two ids. Instance ID is valid only for a particular task. Two different tasks may have the same instance ID.","title":"Task instance"},{"location":"jobs/#job-states","text":"In simple jobs, job state corresponds directly to the state of its single task. In the case of task arrays, see the chapter about task arrays .","title":"Job states"},{"location":"jobs/#canceling-jobs","text":"A job cannot be canceled if it is already finished, failed, or canceled. Cancel specific job: hq cancel <job-id> Cancel all jobs: hq cancel all Cancel last submitted job: hq cancel last","title":"Canceling jobs"},{"location":"jobs/#waiting-for-jobs","text":"You can submit a job with flag --wait and HQ will wait until the submitted job is not terminated (until all tasks are either finished, failed or canceled). You can also use hq wait <job-id> to wait for a specific job or hq wait last to wait for the last submitted job or hq wait all to wait for all jobs. If you want to interactively observe the status of the jobs while waiting for them to finish, use hq submit --progress or hq progress <job-id> instead.","title":"Waiting for jobs"},{"location":"jobs/#priorities","text":"Priorities affect the order in which the \"waiting\" tasks are executed. Priority can be any 32b signed integer. A lowest number marks the lowest priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker, and only one of them may be executed, then A will be executed first. In a submit of a task, you can set priority as follows hq submit --priority=<PRIORITY> If no priority is specified, then task has priority 0.","title":"Priorities"},{"location":"jobs/#resubmit","text":"If you want to recompute a previous job, jou can use: hq resubmit <job-id> that creates a new job with the exact the same configuration as the source job. By default, it submits all tasks of the original job; however, you can specify only a subset of tasks, e.g. hq resubmit <job-id> --status=failed,canceled Resubmits only tasks that failed or were canceled.","title":"Resubmit"},{"location":"other-tools/","text":"TODO","title":"Comparison with other tools"},{"location":"quickstart/","text":"","title":"Quickstart"},{"location":"streaming/","text":"Streaming # The main goal of streaming is to avoid creating many files when a large array job is submitted. For example when you submit: hq submit --array=1-10000 my-computation.sh will create 20000 files (10000 for stdout and 10000 for stderr). This can be solved by using --log option: hq submit --log=<LOG_FILENAME> --array=1-10000 my-computation.sh All stdout and stderr streams will be streamed into server and into a single file specified via . The streamed data is stored with additional metadata, so the resulting file can be filtered/sorted by tasks or type of stream. Superseded streams # It may happen that a task started a stream but the worker where the task is running crashed. In such situation, the scheduler starts a task on another worker and it produces a new stream. To avoid mixing outputs from different runs, HyperQueue automatically marks all previous runs as superseded and ignores them by default. Log summary # hq log <LOG_FILENAME> summary Displays summary of the log. Log show # hq log <LOG_FILENAME> show show the content of the log with metadata printed in the order how it was received by the server. Prompt has form X:Y> where X is task id and Y is 0 for stdout and 1 for stderr. You can filter only stdout/stderr stream via --channel=X where X is stdout or stderr . By default, HQ does not show closing information from streams that are empty, you can change that with the flag --show-empty . Note: Superseded streams are completely ignored by show command. Log cat # hq log <LOG_FILENAME> cat <stdout/stderr> prints raw content of stdout or stderr ordered by task id. All outputs are concatenated one after another. By default, this command will fail if there is an unfinished stream (i.e. a task is still running). If you want to use cat even with running tasks, use option --allow-unfinished . If you want to see only output of a specific task, use option --task=<task_id> Note: Superseded streams are completely ignored by cat command. Partial redirection # If you want to stream only one channel and redirect the other one into a file, you can still use --stdout / --stderr options. Redirecting stdout into a file, stderr will be streamed into my-log . hq submit --log=my-log --stdout=stdout-%{TASK_ID} ... Disabling stderr and streaming only stdout into log file. hq submit --log=my-log --stderr=none ... Guarantees # When a task is finished or failed (except fail of streaming, see below) then it is guaranteed that its stream is fully flushed into the log file. When a task is canceled then the stream is not necessarily fully written into the log file in the moment when the state occurs and some parts may be written later, but the stream will be eventually closed. When a task is canceled or the time limit is reached then part of the stream buffered in the worker is dropped to void spending additional resources for this task. In practice, this should be only part that is produced immediately before the event, because data are sent to the server as soon as possible. If streaming failed (e.g. insufficient disk space for the log file) then task fails with an error prefixed \"Streamer:\" and no guarantees for streaming are provided. Current limitations # The current version does not support streaming of more jobs into the same file, In other words, if you submit more jobs with the same log filename, like this: hq submit --log=my-log ... hq submit --log=my-log ... Then log will contain only data from one job and other will be overwritten.","title":"Reducing amount of output files"},{"location":"streaming/#streaming","text":"The main goal of streaming is to avoid creating many files when a large array job is submitted. For example when you submit: hq submit --array=1-10000 my-computation.sh will create 20000 files (10000 for stdout and 10000 for stderr). This can be solved by using --log option: hq submit --log=<LOG_FILENAME> --array=1-10000 my-computation.sh All stdout and stderr streams will be streamed into server and into a single file specified via . The streamed data is stored with additional metadata, so the resulting file can be filtered/sorted by tasks or type of stream.","title":"Streaming"},{"location":"streaming/#superseded-streams","text":"It may happen that a task started a stream but the worker where the task is running crashed. In such situation, the scheduler starts a task on another worker and it produces a new stream. To avoid mixing outputs from different runs, HyperQueue automatically marks all previous runs as superseded and ignores them by default.","title":"Superseded streams"},{"location":"streaming/#log-summary","text":"hq log <LOG_FILENAME> summary Displays summary of the log.","title":"Log summary"},{"location":"streaming/#log-show","text":"hq log <LOG_FILENAME> show show the content of the log with metadata printed in the order how it was received by the server. Prompt has form X:Y> where X is task id and Y is 0 for stdout and 1 for stderr. You can filter only stdout/stderr stream via --channel=X where X is stdout or stderr . By default, HQ does not show closing information from streams that are empty, you can change that with the flag --show-empty . Note: Superseded streams are completely ignored by show command.","title":"Log show"},{"location":"streaming/#log-cat","text":"hq log <LOG_FILENAME> cat <stdout/stderr> prints raw content of stdout or stderr ordered by task id. All outputs are concatenated one after another. By default, this command will fail if there is an unfinished stream (i.e. a task is still running). If you want to use cat even with running tasks, use option --allow-unfinished . If you want to see only output of a specific task, use option --task=<task_id> Note: Superseded streams are completely ignored by cat command.","title":"Log cat"},{"location":"streaming/#partial-redirection","text":"If you want to stream only one channel and redirect the other one into a file, you can still use --stdout / --stderr options. Redirecting stdout into a file, stderr will be streamed into my-log . hq submit --log=my-log --stdout=stdout-%{TASK_ID} ... Disabling stderr and streaming only stdout into log file. hq submit --log=my-log --stderr=none ...","title":"Partial redirection"},{"location":"streaming/#guarantees","text":"When a task is finished or failed (except fail of streaming, see below) then it is guaranteed that its stream is fully flushed into the log file. When a task is canceled then the stream is not necessarily fully written into the log file in the moment when the state occurs and some parts may be written later, but the stream will be eventually closed. When a task is canceled or the time limit is reached then part of the stream buffered in the worker is dropped to void spending additional resources for this task. In practice, this should be only part that is produced immediately before the event, because data are sent to the server as soon as possible. If streaming failed (e.g. insufficient disk space for the log file) then task fails with an error prefixed \"Streamer:\" and no guarantees for streaming are provided.","title":"Guarantees"},{"location":"streaming/#current-limitations","text":"The current version does not support streaming of more jobs into the same file, In other words, if you submit more jobs with the same log filename, like this: hq submit --log=my-log ... hq submit --log=my-log ... Then log will contain only data from one job and other will be overwritten.","title":"Current limitations"}]}