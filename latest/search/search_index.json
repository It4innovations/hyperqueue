{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HyperQueue is a tool designed to simplify execution of large workflows on HPC clusters. It allows you to execute a large number of tasks in a simple way, without having to manually submit jobs into batch schedulers like PBS or Slurm. You just specify what you want to compute \u2013 HyperQueue will automatically ask for computational resources and dynamically load-balance tasks across all allocated nodes and cores. Useful links # Installation Quick start Repository Features # Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Overview"},{"location":"#useful-links","text":"Installation Quick start Repository","title":"Useful links"},{"location":"#features","text":"Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Features"},{"location":"cheatsheet/","text":"Cheatsheet # Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"faq/","text":"How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs to the server. You may have hundreds of thousands of jobs; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign jobs to them. Workers are fully and dynamically controlled by server; you do not need to specify what jobs are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a job in HQ? Right now, we support running arbitrary external programs or bash scripts. We plan to support Python defined workflows (with a Dask-like API). How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. (See example below). Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to smoothly work on systems using SLURM/PBS, they are not required for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely no. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on a HPC system each user (or a group of users that trust each other) may run her own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to it file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running. What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of poor load balancing. In the case of HQ, you do not have to aggregate jobs. You can submit millions of small jobs to HQ and it will take care of assigning them dynamically to individual SLURM/PBS jobs and workers. How many jobs may I submit into HQ? Our preliminary benchmarks show that overhead of HQ is around 0.1 ms per task. We also support streaming of task outputs into a single file (this file contains metadata, hence outputs for each task can be filtered or ordered). This avoids creating many small files for each task on a distributed file system that may have a large impact on scaling. Does HQ support multi-CPU jobs? Yes. You can define number of CPUs for each job. HQ is NUMA aware and you can choose the allocation strategy. Does HQ support job arrays? Yes. Does HQ support jobs with dependencies? Not yet. It is actually implemented in the scheduling core, but it has no user interface yet. But we consider it as a crucial must-have feature. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python. Who stands behind HyperQueue? We are a group of researchers interested in distributed computing and machine learning. We operate at IT4Innovations , the Czech National Supercomputing Center. We welcome any contribution from outside.","title":"FAQ"},{"location":"installation/","text":"Binary distribution (recommended) # The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation. Compilation from source code # You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Installation"},{"location":"installation/#binary-distribution-recommended","text":"The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation.","title":"Binary distribution (recommended)"},{"location":"installation/#compilation-from-source-code","text":"You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Compilation from source code"},{"location":"other-tools/","text":"TODO Dask Parsl Merlin SnakeMake","title":"Comparison With Other Tools"},{"location":"quickstart/","text":"Here we provide an example of deploying HyperQueue on a local computer and running a simple \"Hello world\" script. Run each of the following three commands in separate terminals. Start the HyperQueue server $ hq server start The server will manage computing resources (workers) and distribute submitted tasks amongst them. Start a HyperQueue worker $ hq worker start The worker will connect to the server and execute submitted tasks. Submit a simple computation $ hq submit echo \"Hello world\" This command will submit a job with a single task that will execute echo \"Hello world\" on a worker. You can find the output of the task in job-1/0.stdout . That's it! For a more in-depth explanation of how HyperQueue works and what it can do, check the Deployment and Jobs sections.","title":"Quickstart"},{"location":"deployment/","text":"Architecture # HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/#architecture","text":"HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/allocation/","text":"Automatic allocation is one of the core features of HyperQueue. When you run HyperQueue on an HPC cluster, it allows you to autonomously ask the job manager (PBS/Slurm) for computing resources and spawn HyperQueue workers on the provided nodes. Using this mechanism, you can submit computations into HyperQueue without caring about the underlying PBS/Slurm jobs. Job terminology It is common to use the term \"job\" for jobs created by an HPC job manager, such as PBS or Slurm, which are used to perform computations on HPC clusters. However, HyperQueue also uses the term \"job\" for ensembles of tasks . To differentiate between these two, we will refer to jobs created by PBS or Slurm as allocations . We will also refer to PBS/Slurm as a job manager . Allocation queue # To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs -- -qqprod -AAccount1 $ hq alloc add slurm -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues. Parameters # In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue: --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. --workers-per-alloc <count> How many nodes should be requested in each allocation. --time-limit <duration> Sets the walltime of created allocations 1 . If unset, the default walltime of the selected PBS queue or Slurm partition will be used. --name <name> Name of the allocation queue. Will be used to name allocations. Serves for debug purposes only. Behavior # The automatic allocator is a periodic process that runs every few seconds and does the following: If there are no waiting tasks in HQ, it immediately ends. This avoids queuing allocations if there is nothing to compute. Otherwise, the allocator queues new allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if backlog was set to 4 and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. Note The backlog value does not limit the number of running allocations, only the number of queued allocations. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes. Stopping automatic allocation # If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> This will also stop and remove any running or queued allocations created by the specified allocation queue from PBS/Slurm. Debugging automatic allocation # Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides various sources of information that can help you find out what is going on. Basic queue information This command will show you details about allocations created by the automatic allocator. Allocator events Each time the allocator performs some action or notices that a status of some allocation was changed, it will create a corresponding event. You can use this command to list most recent events to see what was the allocator doing. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq_submit.sh Useful autoalloc commands # Here is a list of useful commands to manage automatic allocation: Display a list of all allocation queues # $ hq alloc list Display information about an allocation queue # $ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option. Display events of an allocation queue # $ hq alloc events <queue-id> You can use various shortcuts for the duration value. \u21a9","title":"Automatic Allocation"},{"location":"deployment/allocation/#allocation-queue","text":"To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs -- -qqprod -AAccount1 $ hq alloc add slurm -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues.","title":"Allocation queue"},{"location":"deployment/allocation/#parameters","text":"In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue: --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. --workers-per-alloc <count> How many nodes should be requested in each allocation. --time-limit <duration> Sets the walltime of created allocations 1 . If unset, the default walltime of the selected PBS queue or Slurm partition will be used. --name <name> Name of the allocation queue. Will be used to name allocations. Serves for debug purposes only.","title":"Parameters"},{"location":"deployment/allocation/#behavior","text":"The automatic allocator is a periodic process that runs every few seconds and does the following: If there are no waiting tasks in HQ, it immediately ends. This avoids queuing allocations if there is nothing to compute. Otherwise, the allocator queues new allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if backlog was set to 4 and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. Note The backlog value does not limit the number of running allocations, only the number of queued allocations. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes.","title":"Behavior"},{"location":"deployment/allocation/#stopping-automatic-allocation","text":"If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> This will also stop and remove any running or queued allocations created by the specified allocation queue from PBS/Slurm.","title":"Stopping automatic allocation"},{"location":"deployment/allocation/#debugging-automatic-allocation","text":"Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides various sources of information that can help you find out what is going on. Basic queue information This command will show you details about allocations created by the automatic allocator. Allocator events Each time the allocator performs some action or notices that a status of some allocation was changed, it will create a corresponding event. You can use this command to list most recent events to see what was the allocator doing. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq_submit.sh","title":"Debugging automatic allocation"},{"location":"deployment/allocation/#useful-autoalloc-commands","text":"Here is a list of useful commands to manage automatic allocation:","title":"Useful autoalloc commands"},{"location":"deployment/allocation/#display-a-list-of-all-allocation-queues","text":"$ hq alloc list","title":"Display a list of all allocation queues"},{"location":"deployment/allocation/#display-information-about-an-allocation-queue","text":"$ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option.","title":"Display information about an allocation queue"},{"location":"deployment/allocation/#display-events-of-an-allocation-queue","text":"$ hq alloc events <queue-id> You can use various shortcuts for the duration value. \u21a9","title":"Display events of an allocation queue"},{"location":"deployment/server/","text":"The server is a crucial component of HyperQueue which manages workers and jobs . Before running any computations or deploying workers, you must first start the server. Starting the server # The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST Server directory # When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server. Keeping the server alive # The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux . Stopping server # You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Server"},{"location":"deployment/server/#starting-the-server","text":"The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST","title":"Starting the server"},{"location":"deployment/server/#server-directory","text":"When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server.","title":"Server directory"},{"location":"deployment/server/#keeping-the-server-alive","text":"The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux .","title":"Keeping the server alive"},{"location":"deployment/server/#stopping-server","text":"You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Stopping server"},{"location":"deployment/worker/","text":"Workers connect to a running instance of a HyperQueue server and wait for task assignments. Once some task is assigned to them, they will compute it and notify the server of its completion. Starting workers # Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually. Automatic worker deployment (recommended) # If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers. Manual worker deployment # If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though. Deploying a worker using PBS/Slurm # If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment. Stopping workers # If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : # Stop a specific worker $ hq worker stop <worker-id> # Stop all workers $ hq worker stop all Time limit # HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit manually for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated. Idle timeout # When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout. Worker state # Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout . Useful worker commands # Here is a list of useful worker commands: Display worker list # This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command. Display information about a specific worker # $ hq worker info <worker-id> You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Workers"},{"location":"deployment/worker/#starting-workers","text":"Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually.","title":"Starting workers"},{"location":"deployment/worker/#automatic-worker-deployment-recommended","text":"If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers.","title":"Automatic worker deployment (recommended)"},{"location":"deployment/worker/#manual-worker-deployment","text":"If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though.","title":"Manual worker deployment"},{"location":"deployment/worker/#deploying-a-worker-using-pbsslurm","text":"If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment.","title":"Deploying a worker using PBS/Slurm"},{"location":"deployment/worker/#stopping-workers","text":"If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : # Stop a specific worker $ hq worker stop <worker-id> # Stop all workers $ hq worker stop all","title":"Stopping workers"},{"location":"deployment/worker/#time-limit","text":"HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit manually for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated.","title":"Time limit"},{"location":"deployment/worker/#idle-timeout","text":"When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout.","title":"Idle timeout"},{"location":"deployment/worker/#worker-state","text":"Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout .","title":"Worker state"},{"location":"deployment/worker/#useful-worker-commands","text":"Here is a list of useful worker commands:","title":"Useful worker commands"},{"location":"deployment/worker/#display-worker-list","text":"This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command.","title":"Display worker list"},{"location":"deployment/worker/#display-information-about-a-specific-worker","text":"$ hq worker info <worker-id> You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Display information about a specific worker"},{"location":"jobs/arrays/","text":"It is a common use case to execute the same command for multiple input parameters, for example: Performing a simulation for each input file in a directory Training many machine learning models using hyperparameter search HyperQueue allows you to do this using a job containing many tasks. We call such jobs Task arrays . You can create a task array with a single submit command and then manage all created tasks as a single group using its containing job. Note Task arrays are somewhat similar to \"job arrays\" used by PBS and Slurm. However, HQ does not use PBS/Slurm job arrays for implementing this feature. Therefore, the limits that are commonly enforced on job arrays on HPC clusters do not apply to HyperQueue task arrays. Creating task arrays # To create a task array, you must provide some source that will determine how many tasks should be created and what environment variables should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming . Integer range # The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ... Lines of a file # Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ... JSON array # You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"Task Arrays"},{"location":"jobs/arrays/#creating-task-arrays","text":"To create a task array, you must provide some source that will determine how many tasks should be created and what environment variables should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming .","title":"Creating task arrays"},{"location":"jobs/arrays/#integer-range","text":"The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ...","title":"Integer range"},{"location":"jobs/arrays/#lines-of-a-file","text":"Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ...","title":"Lines of a file"},{"location":"jobs/arrays/#json-array","text":"You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"JSON array"},{"location":"jobs/failure/","text":"In distributed systems, failure is inevitable. This sections describes how HyperQueue handles various types of failures and how can you affect its behaviour. Resubmitting jobs # When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array. Task restart # Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id. Task array failures # By default, when a single task of a task array fails, the computation of the job will continue. You can change this behaviour with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Handling Failure"},{"location":"jobs/failure/#resubmitting-jobs","text":"When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array.","title":"Resubmitting jobs"},{"location":"jobs/failure/#task-restart","text":"Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id.","title":"Task restart"},{"location":"jobs/failure/#task-array-failures","text":"By default, when a single task of a task array fails, the computation of the job will continue. You can change this behaviour with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Task array failures"},{"location":"jobs/jobs/","text":"To compute something using HyperQueue, you have to create a Job . It is a unit of computation management \u2013 you can submit, query or cancel jobs. Each job consists of one or more Tasks . Task represents a single computation (currently, a single execution of some program) and is an indivisible unit of scheduling and computation. Note This section focuses on simple jobs , where each job contains exactly one task. See Task arrays to find out how to create jobs with multiple tasks. Identification numbers # Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 . Submitting jobs # To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below. Name # Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ... Working directory # By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path. Output # By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are job-%{JOB_ID}/%{TASK_ID}.stdout for stdout job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder . Environment variables # You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id Time management # You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes. Priority # You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 . Placeholders # You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by %{JOB_ID} Job ID %{TASK_ID} Task ID (*) %{INSTANCE_ID} Instance ID %{SUBMIT_DIR} Directory from which the job was submitted. (*) %{CWD} Working directory of the job. This placeholder is only available for stdout and stderr paths. %{DATE} Date when the job was executed in the RFC3339 format. (*) * These placeholders are available for the streaming log path. State # At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following commands 2 : Simple job Job with multiple tasks $ hq job <job-id> $ hq job --tasks <job-id> Task state # Each task starts in the Submitted state and can end up in one of the terminal states: Finished , Failed or Canceled . Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted Only an informative state that a submission was successful; it is only shown immediately after a submit. Waiting The task is waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed . Job state # The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Canceled , then job state is Canceled . If at least one task is Failed , then job state is Failed . All tasks have to be Finished , therefore the job state will also be Finished . Cancelling jobs # You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq cancel command 2 : $ hq cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed. Waiting for jobs # There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq wait command that can be used to wait until an existing job completes 2 : $ hq wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq progress <selector> Useful job commands # Here is a list of useful job commands: Display all jobs # $ hq jobs You can display only jobs having the selected states by appending them to the command: $ hq jobs running waiting Valid filter values are: waiting running finished failed canceled Display information about a specific job # $ hq job <job-selector> You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9","title":"Jobs and Tasks"},{"location":"jobs/jobs/#identification-numbers","text":"Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 .","title":"Identification numbers"},{"location":"jobs/jobs/#submitting-jobs","text":"To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below.","title":"Submitting jobs"},{"location":"jobs/jobs/#name","text":"Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ...","title":"Name"},{"location":"jobs/jobs/#working-directory","text":"By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path.","title":"Working directory"},{"location":"jobs/jobs/#output","text":"By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are job-%{JOB_ID}/%{TASK_ID}.stdout for stdout job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder .","title":"Output"},{"location":"jobs/jobs/#environment-variables","text":"You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id","title":"Environment variables"},{"location":"jobs/jobs/#time-management","text":"You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes.","title":"Time management"},{"location":"jobs/jobs/#priority","text":"You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 .","title":"Priority"},{"location":"jobs/jobs/#placeholders","text":"You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by %{JOB_ID} Job ID %{TASK_ID} Task ID (*) %{INSTANCE_ID} Instance ID %{SUBMIT_DIR} Directory from which the job was submitted. (*) %{CWD} Working directory of the job. This placeholder is only available for stdout and stderr paths. %{DATE} Date when the job was executed in the RFC3339 format. (*) * These placeholders are available for the streaming log path.","title":"Placeholders"},{"location":"jobs/jobs/#state","text":"At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following commands 2 : Simple job Job with multiple tasks $ hq job <job-id> $ hq job --tasks <job-id>","title":"State"},{"location":"jobs/jobs/#task-state","text":"Each task starts in the Submitted state and can end up in one of the terminal states: Finished , Failed or Canceled . Submitted | | v Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Submitted Only an informative state that a submission was successful; it is only shown immediately after a submit. Waiting The task is waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed .","title":"Task state"},{"location":"jobs/jobs/#job-state","text":"The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Canceled , then job state is Canceled . If at least one task is Failed , then job state is Failed . All tasks have to be Finished , therefore the job state will also be Finished .","title":"Job state"},{"location":"jobs/jobs/#cancelling-jobs","text":"You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq cancel command 2 : $ hq cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed.","title":"Cancelling jobs"},{"location":"jobs/jobs/#waiting-for-jobs","text":"There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq wait command that can be used to wait until an existing job completes 2 : $ hq wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq progress <selector>","title":"Waiting for jobs"},{"location":"jobs/jobs/#useful-job-commands","text":"Here is a list of useful job commands:","title":"Useful job commands"},{"location":"jobs/jobs/#display-all-jobs","text":"$ hq jobs You can display only jobs having the selected states by appending them to the command: $ hq jobs running waiting Valid filter values are: waiting running finished failed canceled","title":"Display all jobs"},{"location":"jobs/jobs/#display-information-about-a-specific-job","text":"$ hq job <job-selector> You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9","title":"Display information about a specific job"},{"location":"jobs/resources/","text":"Note: In this text we use term CPU as a resource that is provided by operating system (e.g. what you get from /proc/cpuinfo). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use term socket to refer to a physical CPU. TODO: generic resources Requesting more CPUs # By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus. Requesting all CPUs # Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...> Pinning # By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically. Automatic pinning # If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process. Manual pinning # If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> NUMA allocation policy # HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\" CPU requests and job arrays # Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs. CPUs configuration # Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout. Manual specification of CPU configration # If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12","title":"Resources"},{"location":"jobs/resources/#requesting-more-cpus","text":"By default, each task allocates a single CPU on worker's node. This can be changed by argument --cpus=... . Example: Request for a job with a task that needs 8 cpus: $ hq submit --cpus=8 <program_name> <args...> This ensures that 8 cpus will be exclusively reserved when this task is started. This task will never be scheduled on a worker that has less then 8 cpus.","title":"Requesting more CPUs"},{"location":"jobs/resources/#requesting-all-cpus","text":"Setting --cpus=all ensures that will request all CPUs of the worker and ensures an exclusive run of the task. $ hq submit --cpus=all <program_name> <args...>","title":"Requesting all CPUs"},{"location":"jobs/resources/#pinning","text":"By default, HQ internally allocates CPUs on logical level without pinning. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs in the worker, but the the process placement is left on the system scheduler that may move processes across CPUs as it wants. If this is not desired, especially in case of NUMA, processes could be pinned, either manually or automatically.","title":"Pinning"},{"location":"jobs/resources/#automatic-pinning","text":"If you just want to pin your processes to allocated CPUs, use --pin flag. $ hq submit --pin --cpus=8 <your-program> <args> When an automatic pinning is enabled then the environment variable HQ_PIN is set to 1 in the task process.","title":"Automatic pinning"},{"location":"jobs/resources/#manual-pinning","text":"If you want to gain a full controll over pinning processes, you may pin the process by yourself. The assigned CPUs is stored in environment HQ_CPUS as a comma-delimited list of CPU ids. You can use utilities as taskset or numactl and pass there HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not use --pin flag in submit command. It may have some unwanted interferences. For example you can create a following script.sh (with executable permission) #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> If it is submitted as $ hq submit --cpus=4 script.sh It will pin your program to 4 CPUs allocated by HQ. In case of numactl , the equivalent script is: #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...>","title":"Manual pinning"},{"location":"jobs/resources/#numa-allocation-policy","text":"HQ currently ofsers the following allocation strategies how CPUs are allocated. It can be specified by --cpus argument in form \"<#cpus> <policy>\" . Note: Specifying policy has effect only if you have more then one sockets(physical CPUs). In case of a single socket, policies are indistinguishable. Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. Example: hq --cpus=\"8 compact\" ... Strict Compact ( compact! ) - Always allocate cores on as few sockets as possible for a target node. The task is not executed until the requirement could not be fully fullfiled. E.g. If your worker has 4 cores per socket and you ask for 4 cpus, it will be always executed on a single socket. If you ask for 8 cpus, it will be always executed on two sockets. Example: hq --cpus=\"8 compact!\" ... Scatter ( scatter ) - Allocate cores across as many sockets possible in the current worker state. If your worker has 4 sockets with 8 cores per socket and you ask for 8 cpus than if possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: hq --cpus=\"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=XX is equivalent to --cpus=\"XX compact\"","title":"NUMA allocation policy"},{"location":"jobs/resources/#cpu-requests-and-job-arrays","text":"Resource requests are applied to each task of job. For example, if you submit the following: hq --cpus=2 --array=1-10 it will create 10 tasks where each task needs two CPUs.","title":"CPU requests and job arrays"},{"location":"jobs/resources/#cpus-configuration","text":"Worker automatically detect number of CPUs and on Linux system it also detects partitioning into sockets. In most cases, it should work without need of any touch. If you want to see how is your seen by a worker without actually starting it, you can start $ hq worker hwdetect that only prints CPUs layout.","title":"CPUs configuration"},{"location":"jobs/resources/#manual-specification-of-cpu-configration","text":"If automatic detection fails, or you want to manually configure set CPU configuration, you can use --cpus parameter; for example as follows: 8 CPUs for worker $ hq worker start --cpus=8 2 sockets with 12 cores per socket $ hq worker --cpus=2x12","title":"Manual specification of CPU configration"},{"location":"jobs/streaming/","text":"Jobs containing many tasks will generate a large amount of stdout and stderr files, which can be problematic, especially on network-based shared filesystems, such as Lustre. For example, when you submit the following task array: $ hq submit --array = 1 -10000 my-computation.sh 20000 files ( 10000 for stdout and 10000 for stderr) will be created on the disk. To avoid this situation, HyperQueue can optionally stream the stdout and stderr output of all tasks of a job over a network to the server, which will continuously append it to a single file called the Log . Note In this section, we refer to stdout and stderr as channels . Redirecting output to the log # You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path. Partial redirection # By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ... Guarantees # HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld. Superseded streams # When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default. Current limitations # The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten. Inspecting the log file # HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args> Log summary # You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary Printing log content # If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand. Log metadata # If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand.","title":"Output Streaming"},{"location":"jobs/streaming/#redirecting-output-to-the-log","text":"You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path.","title":"Redirecting output to the log"},{"location":"jobs/streaming/#partial-redirection","text":"By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ...","title":"Partial redirection"},{"location":"jobs/streaming/#guarantees","text":"HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld.","title":"Guarantees"},{"location":"jobs/streaming/#superseded-streams","text":"When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default.","title":"Superseded streams"},{"location":"jobs/streaming/#current-limitations","text":"The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten.","title":"Current limitations"},{"location":"jobs/streaming/#inspecting-the-log-file","text":"HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args>","title":"Inspecting the log file"},{"location":"jobs/streaming/#log-summary","text":"You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary","title":"Log summary"},{"location":"jobs/streaming/#printing-log-content","text":"If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand.","title":"Printing log content"},{"location":"jobs/streaming/#log-metadata","text":"If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand.","title":"Log metadata"},{"location":"tips/cli-shortcuts/","text":"Various HyperQueue CLI command options let you enter some value in a specific syntactical format for convenience. Here you can find a list of such shortcuts. ID selector # When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq cancel all - cancel all jobs last The most recently created ID hq cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 . Supported commands and options # hq submit --array=<selector> hq worker stop <selector> does not support last hq job <selector> does not support all (use hq jobs instead) hq cancel <selector> hq wait <selector> hq progress <selector> Duration # You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here . Supported commands and options # hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ...","title":"CLI Shortcuts"},{"location":"tips/cli-shortcuts/#id-selector","text":"When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq cancel all - cancel all jobs last The most recently created ID hq cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 .","title":"ID selector"},{"location":"tips/cli-shortcuts/#supported-commands-and-options","text":"hq submit --array=<selector> hq worker stop <selector> does not support last hq job <selector> does not support all (use hq jobs instead) hq cancel <selector> hq wait <selector> hq progress <selector>","title":"Supported commands and options"},{"location":"tips/cli-shortcuts/#duration","text":"You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here .","title":"Duration"},{"location":"tips/cli-shortcuts/#supported-commands-and-options_1","text":"hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ...","title":"Supported commands and options"}]}