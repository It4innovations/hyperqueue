{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HyperQueue is a tool designed to simplify execution of large workflows on HPC clusters. It allows you to execute a large number of tasks in a simple way, without having to manually submit jobs into batch schedulers like PBS or Slurm. You just specify what you want to compute \u2013 HyperQueue will automatically ask for computational resources and dynamically load-balance tasks across all allocated nodes and cores. Useful links # Installation Quick start Python API Repository Discussion forum Zulip (chat platform) Features # Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Overview"},{"location":"#useful-links","text":"Installation Quick start Python API Repository Discussion forum Zulip (chat platform)","title":"Useful links"},{"location":"#features","text":"Resource management Batch jobs are submitted and managed automatically Computation is distributed amongst all allocated nodes and cores Tasks can specify resource requirements (# of cores, GPUs, memory, ...) Performance Scales to millions of tasks and hundreds of nodes Overhead per task is around 0.1 ms Task output can be streamed to a single file to avoid overloading distributed filesystems Simple deployment HQ is provided as a single, statically linked binary without any dependencies No admin access to a cluster is needed","title":"Features"},{"location":"cheatsheet/","text":"Cheatsheet # Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"cheatsheet/#cheatsheet","text":"Here you can find a cheatsheet with the most basic HQ commands.","title":"Cheatsheet"},{"location":"events/","text":"Events # HyperQueue internally records various events that describe what has happened during the lifetime of the HQ cluster (worker has connected, task was finished, an allocation was submitted to PBS, etc.). These events might be useful for some power-users, for example to analyze task execution statistics or allocation durations. To access these events, you have to start the HyperQueue server with the --event-log-path option: $ hq server start --event-log-path = events.bin If you use this flag, HQ will continuously stream its events into a log file at the provided path. The events are serialized using a compressed binary encoding. To access the event data from the log file, you first have to export them. JSON export # To export data from the log file to JSON, you can use the following command: $ hq event-log export <event-log-path> The events will be read from the provided log file and printed to stdout encoded in JSON, one event per line (this corresponds to line-delimited JSON, i.e. NDJSON ). Warning The JSON format of the events and their definition is currently unstable and can change with a new HyperQueue version.","title":"Events"},{"location":"events/#events","text":"HyperQueue internally records various events that describe what has happened during the lifetime of the HQ cluster (worker has connected, task was finished, an allocation was submitted to PBS, etc.). These events might be useful for some power-users, for example to analyze task execution statistics or allocation durations. To access these events, you have to start the HyperQueue server with the --event-log-path option: $ hq server start --event-log-path = events.bin If you use this flag, HQ will continuously stream its events into a log file at the provided path. The events are serialized using a compressed binary encoding. To access the event data from the log file, you first have to export them.","title":"Events"},{"location":"events/#json-export","text":"To export data from the log file to JSON, you can use the following command: $ hq event-log export <event-log-path> The events will be read from the provided log file and printed to stdout encoded in JSON, one event per line (this corresponds to line-delimited JSON, i.e. NDJSON ). Warning The JSON format of the events and their definition is currently unstable and can change with a new HyperQueue version.","title":"JSON export"},{"location":"faq/","text":"FAQ # HQ fundamentals # How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs containing tasks to the server. You may have hundreds of thousands of tasks; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign tasks to them. Workers are fully and dynamically controlled by server; you do not need to specify what tasks are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a task in HQ? Task is a unit of computation. Currently, it is either the execution of an arbitrary external program (specified via CLI) or the execution of a single Python function (specified via our Python API). What is a job in HQ? Job is a collection of tasks (a task graph). You can display and manage jobs using the CLI. How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. How many jobs/tasks may I submit into HQ? Our preliminary benchmarks show that the overhead of HQ is around 0.1 ms per task. It should be thus possible to submit a job with tens or hundreds of thousands tasks into HQ. Note that HQ is designed for a large number of tasks, not jobs. If you want to perform a lot of computations, use task arrays , i.e. create a job with many tasks, not many jobs each with a single task. HQ also supports streaming of task outputs into a single file. This avoids creating many small files for each task on a distributed file system, which improves scaling. Does HQ support multi-CPU tasks? Yes. You can define an arbitrary amount of CPUs for each task. HQ is also NUMA aware and you can select the NUMA allocation strategy. Does HQ support job/task arrays? Yes, see task arrays . Does HQ support tasks with dependencies? Yes, although it is currently only implemented in the Python API, which is experimental. It is currently not possible to specify dependencies using the CLI. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python. Relation to HPC technologies # Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to work smoothly on systems using SLURM/PBS, they are not required in order for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely not. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on an HPC system each user (or a group of users that trust each other) may run their own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally on your personal computer. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to that file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running. Relation to other task runtimes # What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one HPC job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of missing load balancing. In the case of HQ, you do not have to aggregate tasks. You can submit millions of small tasks to HQ and it will take care of assigning them dynamically to individual workers (and SLURM/PBS jobs, if automatic allocation is used).","title":"FAQ"},{"location":"faq/#faq","text":"","title":"FAQ"},{"location":"faq/#hq-fundamentals","text":"How does HQ work? You start a HQ server somewhere (e.g. a login node or a cloud partition of a cluster). Then you can submit your jobs containing tasks to the server. You may have hundreds of thousands of tasks; they may have various CPUs and other resource requirements. Then you can connect any number of HQ workers to the server (either manually or via SLURM/PBS). The server will then immediately start to assign tasks to them. Workers are fully and dynamically controlled by server; you do not need to specify what tasks are executed on a particular worker or preconfigure it in any way. HQ provides a command line tool for submitting and controlling jobs. What is a task in HQ? Task is a unit of computation. Currently, it is either the execution of an arbitrary external program (specified via CLI) or the execution of a single Python function (specified via our Python API). What is a job in HQ? Job is a collection of tasks (a task graph). You can display and manage jobs using the CLI. How to deploy HQ? HQ is distributed as a single, self-contained and statically linked binary. It allows you to start the server, the workers, and it also serves as CLI for submitting and controlling jobs. No other services are needed. How many jobs/tasks may I submit into HQ? Our preliminary benchmarks show that the overhead of HQ is around 0.1 ms per task. It should be thus possible to submit a job with tens or hundreds of thousands tasks into HQ. Note that HQ is designed for a large number of tasks, not jobs. If you want to perform a lot of computations, use task arrays , i.e. create a job with many tasks, not many jobs each with a single task. HQ also supports streaming of task outputs into a single file. This avoids creating many small files for each task on a distributed file system, which improves scaling. Does HQ support multi-CPU tasks? Yes. You can define an arbitrary amount of CPUs for each task. HQ is also NUMA aware and you can select the NUMA allocation strategy. Does HQ support job/task arrays? Yes, see task arrays . Does HQ support tasks with dependencies? Yes, although it is currently only implemented in the Python API, which is experimental. It is currently not possible to specify dependencies using the CLI. How is HQ implemented? HQ is implemented in Rust and uses Tokio ecosystem. The scheduler is work-stealing scheduler implemented in our project Tako , that is derived from our previous work RSDS . Integration tests are written in Python, but HQ itself does not depend on Python.","title":"HQ fundamentals"},{"location":"faq/#relation-to-hpc-technologies","text":"Do I need to SLURM or PBS to run HQ? No. Even though HQ is designed to work smoothly on systems using SLURM/PBS, they are not required in order for HQ to work. Is HQ a replacement for SLURM or PBS? Definitely not. Multi-tenancy is out of the scope of HQ, i.e. HQ does not provide user isolation. HQ is light-weight and easy to deploy; on an HPC system each user (or a group of users that trust each other) may run their own instance of HQ. Do I need an HPC cluster to run HQ? No. None of functionality is bound to any HPC technology. Communication between all components is performed using TCP/IP. You can also run HQ locally on your personal computer. Is it safe to run HQ on a login node shared by other users? Yes. All communication is secured and encrypted. The server generates a secret file and only those users that have access to that file may submit jobs and connect workers. Users without access to the secret file will only see that the service is running.","title":"Relation to HPC technologies"},{"location":"faq/#relation-to-other-task-runtimes","text":"What is the difference between HQ and Snakemake? In cluster mode, Snakemake submits each Snakemake job as one HPC job into SLURM/PBS. If your jobs are too small, you will have to manually aggregate them to avoid exhausting SLURM/PBS resources. Manual job aggregation is often quite arduous and since the aggregation is static, it might also waste resources because of missing load balancing. In the case of HQ, you do not have to aggregate tasks. You can submit millions of small tasks to HQ and it will take care of assigning them dynamically to individual workers (and SLURM/PBS jobs, if automatic allocation is used).","title":"Relation to other task runtimes"},{"location":"installation/","text":"Binary distribution (recommended) # The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation. Compilation from source code # You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Installation"},{"location":"installation/#binary-distribution-recommended","text":"The easiest way to install HyperQueue is to download and unpack the prebuilt hq executable: Download the latest release archive from this link . Target architecture Make sure to choose the correct binary for your architecture. Currently, we provide prebuilt binaries for x86-64 and PowerPC architectures. Unpack the downloaded archive: $ tar -xvzf hq-<version>-linux-<architecture>.tar.gz The archive contains a single binary hq , which is used both for deploying the HQ cluster and submitting tasks into HQ . You can add hq to your system $PATH to make its usage easier. See Quickstart for an example \"Hello world\" HyperQueue computation.","title":"Binary distribution (recommended)"},{"location":"installation/#compilation-from-source-code","text":"You can also compile HyperQueue from source. This allows you to build HyperQueue for architectures for which we do not provide prebuilt binaries. It can also generate binaries with support for vectorization, which could in theory improve the performance of HyperQueue in extreme cases. Setup a Rust toolchain Clone the HyperQueue repository: $ git clone https://github.com/It4innovations/hyperqueue/ Build HyperQueue: $ RUSTFLAGS = \"-C target-cpu=native\" cargo build --release Jemalloc dependency HyperQueue by default depends on the Jemalloc memory allocator, which is a C library. If you're having problems with installing HyperQueue because of this dependency, you can opt-out of it and use the default system allocator by building HQ with --no-default-features : $ cargo build --release --no-default-features Use the executable located in ./target/release/hq","title":"Compilation from source code"},{"location":"other-tools/","text":"Comparison with other task runtimes # There is a large number of task runtimes, so we cannot list all of them here. Below you can find a selection of other task runtimes that we have experience with and/or that are somehow relevant for HyperQueue. Dask # Dask is a task runtime that is very popular within the Python community, which allows executing arbitrary task graphs composed of Python functions on a distributed cluster. It also supports distributing code using numpy or pandas compatible API. While Dask by itself does not interact with PBS or Slurm, you can use Dask-JobQueue to make it operate in a similar fashion as HyperQueue - with the centralized server running on a login node and the workers running on compute nodes. Dask does not support arbitrary resource requirements and since it is written in Python, it can have problems with scaling to very large task graphs. If your use-case is primarily Python-based, you should definitely give Dask a try, it's a great tool. SnakeMake # SnakeMake is a workflow execution system that focuses on scientific reproducibility. It lets users specify computational workflows using a DSL that combined configuration files and Python. It can operate both as a meta-scheduler (outside of PBS/Slurm) and also as a classical task runtime within a PBS/Slurm job. With SnakeMake, you can submit a workflow either using a task-per-job model (which has high overhead) or you can partition the workflow into several jobs, but in that case SnakeMake will not provide load balancing across these partitions. HyperQueue allows you to submit large workflows without partitioning them manually in any way, as the server will dynamically load balance the tasks onto workers from different PBS/Slurm allocations. Since SnakeMake workflows are defined in configuration files, it's a bit more involved to run computations in SnakeMake than in HyperQueue. On the other hand, SnakeMake lets you define more complex workflows with improved traceability and reproducibility. Merlin # Merlin is a workflow execution system focused on HPC-scale machine learning workflows. It has a relatively similar architecture to HyperQueue, although it uses configuration files rather than CLI for specifying jobs.","title":"Comparison With Other Tools"},{"location":"other-tools/#comparison-with-other-task-runtimes","text":"There is a large number of task runtimes, so we cannot list all of them here. Below you can find a selection of other task runtimes that we have experience with and/or that are somehow relevant for HyperQueue.","title":"Comparison with other task runtimes"},{"location":"other-tools/#dask","text":"Dask is a task runtime that is very popular within the Python community, which allows executing arbitrary task graphs composed of Python functions on a distributed cluster. It also supports distributing code using numpy or pandas compatible API. While Dask by itself does not interact with PBS or Slurm, you can use Dask-JobQueue to make it operate in a similar fashion as HyperQueue - with the centralized server running on a login node and the workers running on compute nodes. Dask does not support arbitrary resource requirements and since it is written in Python, it can have problems with scaling to very large task graphs. If your use-case is primarily Python-based, you should definitely give Dask a try, it's a great tool.","title":"Dask"},{"location":"other-tools/#snakemake","text":"SnakeMake is a workflow execution system that focuses on scientific reproducibility. It lets users specify computational workflows using a DSL that combined configuration files and Python. It can operate both as a meta-scheduler (outside of PBS/Slurm) and also as a classical task runtime within a PBS/Slurm job. With SnakeMake, you can submit a workflow either using a task-per-job model (which has high overhead) or you can partition the workflow into several jobs, but in that case SnakeMake will not provide load balancing across these partitions. HyperQueue allows you to submit large workflows without partitioning them manually in any way, as the server will dynamically load balance the tasks onto workers from different PBS/Slurm allocations. Since SnakeMake workflows are defined in configuration files, it's a bit more involved to run computations in SnakeMake than in HyperQueue. On the other hand, SnakeMake lets you define more complex workflows with improved traceability and reproducibility.","title":"SnakeMake"},{"location":"other-tools/#merlin","text":"Merlin is a workflow execution system focused on HPC-scale machine learning workflows. It has a relatively similar architecture to HyperQueue, although it uses configuration files rather than CLI for specifying jobs.","title":"Merlin"},{"location":"quickstart/","text":"Here we provide an example of deploying HyperQueue on a local computer and running a simple \"Hello world\" script. Run each of the following three commands in separate terminals. Start the HyperQueue server $ hq server start The server will manage computing resources (workers) and distribute submitted tasks amongst them. Start a HyperQueue worker $ hq worker start The worker will connect to the server and execute submitted tasks. Submit a simple computation $ hq submit echo \"Hello world\" This command will submit a job with a single task that will execute echo \"Hello world\" on a worker. You can find the output of the task in job-1/0.stdout . That's it! For a more in-depth explanation of how HyperQueue works and what it can do, check the Deployment and Jobs sections.","title":"Quickstart"},{"location":"cli/output-mode/","text":"By default, HyperQueue CLI commands output information in a human-readable way, usually in the form of a table. If you want to use the CLI commands programmatically, HyperQueue offers two additional output modes that are designed to be machine-readable. You can change the output type of any HyperQueue CLI command either by using the --output-mode flag or by setting the HQ_OUTPUT_MODE environment variable. Flag Environment variable $ hq --output-mode = json job list $ HQ_OUTPUT_MODE = json hq job list Currently, there are three output modes available. The default, human-readable cli mode, and then two machine-readable modes, JSON and Quiet . Important Each machine-readable mode supports a set of commands. You can also use commands that are not listed here, but their output might be unstable, or they might not output anything for a given output mode. JSON # The json output mode is intended to provide very detailed information in the form of a JSON value. With this mode, HyperQueue will always output exactly one JSON value, either an array or an object. Error handling # When an error occurs during the execution of a command, the program will exit with exit code 1 and the program will output a JSON object with a single error key containing a human-readable description of the error. Date formatting # Time-based items are formatted in the following way: Duration - formatted as a floating point number of seconds. Datetime (timestamp) - formatted as a ISO8601 date in UTC Supported commands # Server info: hq server info Example { \"host\" : \"my-machine\" , \"hq_port\" : 42189 , \"pid\" : 32586 , \"server_dir\" : \"/foo/bar/.hq-server\" , \"start_date\" : \"2021-12-20T08:45:41.775753188Z\" , \"version\" : \"0.7.0\" , \"worker_port\" : 38627 } Worker list: hq worker list Example [{ \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 }] Worker info: hq worker info <worker-id> Example { \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 } Submit a job: hq submit <command> Example { \"id\" : 1 } Job list: hq job list Example [{ \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }] Job info: hq job info <job-id> --tasks Example { \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"info\" : { \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }, \"max_fails\" : null , \"pin\" : null , \"priority\" : 0 , \"program\" : { \"args\" : [ \"ls\" ], \"cwd\" : \"%{SUBMIT_DIR}\" , \"env\" : { \"FOO\" : \"BAR\" }, \"stderr\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stderr\" }, \"stdout\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stdout\" } }, \"started_at\" : \"2021-12-20T08:45:53.458919345Z\" , \"tasks\" : [{ \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"id\" : 0 , \"started_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"state\" : \"finished\" , \"worker\" : 1 , \"cwd\" : \"/tmp/foo\" , \"stderr\" : { \"File\" : \"job-1/0.stderr\" }, \"stdout\" : { \"File\" : \"job-1/0.stdout\" } }], \"time_limit\" : null , \"submit_dir\" : \"/foo/bar/submit\" } Automatic allocation queue list: hq alloc list Example [{ \"additional_args\" : [], \"backlog\" : 4 , \"id\" : 1 , \"manager\" : \"PBS\" , \"max_worker_count\" : null , \"name\" : null , \"timelimit\" : 1800.0 , \"worker_cpu_args\" : null , \"worker_resource_args\" : [], \"workers_per_alloc\" : 1 }] Automatic allocation queue info: hq alloc info <allocation-queue-id> Example [{ \"id\" : \"pbs-1\" , \"worker_count\" : 4 , \"queue_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"started_at\" : \"2021-12-20T08:58:25.538001256Z\" , \"ended_at\" : null , \"status\" : \"running\" , \"workdir\" : \"/foo/bar\" }] Automatic allocation queue events: hq alloc events <allocation-queue-id> Example [{ \"date\" : \"2021-12-20T08:56:16.437123396Z\" , \"event\" : \"allocation-finished\" , \"params\" : { \"id\" : \"pbs-1\" } }, { \"date\" : \"2021-12-20T08:58:16.437123396Z\" , \"event\" : \"status-fail\" , \"params\" : { \"error\" : \"qstat failed\" } }] Quiet # The quiet output mode will cause HyperQueue to output only the most important information that should be parseable without any complex parsing logic, e.g. using only Bash scripts. Error handling # When an error occurs during the execution of a command, the program will exit with exit code 1 and the error will be printed to the standard error output. Supported commands # Submit a job: hq submit <command> Schema Outputs a single line containing the ID of the created job. Example $ hq --output-mode = quiet submit ls 1","title":"Output mode"},{"location":"cli/output-mode/#json","text":"The json output mode is intended to provide very detailed information in the form of a JSON value. With this mode, HyperQueue will always output exactly one JSON value, either an array or an object.","title":"JSON"},{"location":"cli/output-mode/#error-handling","text":"When an error occurs during the execution of a command, the program will exit with exit code 1 and the program will output a JSON object with a single error key containing a human-readable description of the error.","title":"Error handling"},{"location":"cli/output-mode/#date-formatting","text":"Time-based items are formatted in the following way: Duration - formatted as a floating point number of seconds. Datetime (timestamp) - formatted as a ISO8601 date in UTC","title":"Date formatting"},{"location":"cli/output-mode/#supported-commands","text":"Server info: hq server info Example { \"host\" : \"my-machine\" , \"hq_port\" : 42189 , \"pid\" : 32586 , \"server_dir\" : \"/foo/bar/.hq-server\" , \"start_date\" : \"2021-12-20T08:45:41.775753188Z\" , \"version\" : \"0.7.0\" , \"worker_port\" : 38627 } Worker list: hq worker list Example [{ \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 }] Worker info: hq worker info <worker-id> Example { \"configuration\" : { \"heartbeat_interval\" : 8.0 , \"hostname\" : \"my-machine\" , \"idle_timeout\" : null , \"listen_address\" : \"my-machine:45611\" , \"log_dir\" : \"...\" , \"resources\" : { \"cpus\" : [[ 0 , 1 , 2 , 3 ]], \"generic\" : [{ \"kind\" : \"sum\" , \"name\" : \"resource1\" , \"params\" : { \"size\" : 1000 } }] }, \"time_limit\" : null , \"work_dir\" : \"...\" }, \"ended\" : null , \"id\" : 1 } Submit a job: hq submit <command> Example { \"id\" : 1 } Job list: hq job list Example [{ \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }] Job info: hq job info <job-id> --tasks Example { \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"info\" : { \"id\" : 1 , \"name\" : \"ls\" , \"resources\" : { \"cpus\" : { \"cpus\" : 1 , \"type\" : \"compact\" }, \"generic\" : [], \"min_time\" : 0.0 }, \"task_count\" : 1 , \"task_stats\" : { \"canceled\" : 0 , \"failed\" : 0 , \"finished\" : 1 , \"running\" : 0 , \"waiting\" : 0 } }, \"max_fails\" : null , \"pin\" : null , \"priority\" : 0 , \"program\" : { \"args\" : [ \"ls\" ], \"cwd\" : \"%{SUBMIT_DIR}\" , \"env\" : { \"FOO\" : \"BAR\" }, \"stderr\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stderr\" }, \"stdout\" : { \"File\" : \"job-%{JOB_ID}/%{TASK_ID}.stdout\" } }, \"started_at\" : \"2021-12-20T08:45:53.458919345Z\" , \"tasks\" : [{ \"finished_at\" : \"2021-12-20T08:56:16.438062340Z\" , \"id\" : 0 , \"started_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"state\" : \"finished\" , \"worker\" : 1 , \"cwd\" : \"/tmp/foo\" , \"stderr\" : { \"File\" : \"job-1/0.stderr\" }, \"stdout\" : { \"File\" : \"job-1/0.stdout\" } }], \"time_limit\" : null , \"submit_dir\" : \"/foo/bar/submit\" } Automatic allocation queue list: hq alloc list Example [{ \"additional_args\" : [], \"backlog\" : 4 , \"id\" : 1 , \"manager\" : \"PBS\" , \"max_worker_count\" : null , \"name\" : null , \"timelimit\" : 1800.0 , \"worker_cpu_args\" : null , \"worker_resource_args\" : [], \"workers_per_alloc\" : 1 }] Automatic allocation queue info: hq alloc info <allocation-queue-id> Example [{ \"id\" : \"pbs-1\" , \"worker_count\" : 4 , \"queue_at\" : \"2021-12-20T08:56:16.437123396Z\" , \"started_at\" : \"2021-12-20T08:58:25.538001256Z\" , \"ended_at\" : null , \"status\" : \"running\" , \"workdir\" : \"/foo/bar\" }] Automatic allocation queue events: hq alloc events <allocation-queue-id> Example [{ \"date\" : \"2021-12-20T08:56:16.437123396Z\" , \"event\" : \"allocation-finished\" , \"params\" : { \"id\" : \"pbs-1\" } }, { \"date\" : \"2021-12-20T08:58:16.437123396Z\" , \"event\" : \"status-fail\" , \"params\" : { \"error\" : \"qstat failed\" } }]","title":"Supported commands"},{"location":"cli/output-mode/#quiet","text":"The quiet output mode will cause HyperQueue to output only the most important information that should be parseable without any complex parsing logic, e.g. using only Bash scripts.","title":"Quiet"},{"location":"cli/output-mode/#error-handling_1","text":"When an error occurs during the execution of a command, the program will exit with exit code 1 and the error will be printed to the standard error output.","title":"Error handling"},{"location":"cli/output-mode/#supported-commands_1","text":"Submit a job: hq submit <command> Schema Outputs a single line containing the ID of the created job. Example $ hq --output-mode = quiet submit ls 1","title":"Supported commands"},{"location":"cli/shortcuts/","text":"Various HyperQueue CLI command options let you enter some value in a specific syntactical format for convenience. Here you can find a list of such shortcuts. ID selector # When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq job cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq job cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq job cancel all - cancel all jobs last The most recently created ID hq worker stop last - stop most recently connected worker hq job cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 . Tip You can add underscore ( _ ) separators to any of the entered numeric values to improve readability: $ hq submit --array = 1 -1000_000 ... Supported commands and options # hq submit --array=<selector> hq worker stop <selector> hq job info <selector> does not support all (use hq job list instead) hq job cancel <selector> hq job wait <selector> hq job progress <selector> Duration # You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here . Supported commands and options # hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ... Tip For increased compatibility with PBS and Slurm , you can also specify the --time-limit option of hq alloc add using the HH:MM:SS format.","title":"Shortcuts"},{"location":"cli/shortcuts/#id-selector","text":"When you enter (job/task/worker) IDs to various HyperQueue CLI commands, you can use the following selectors to select multiple IDs at once or to reference the most recently created ID: <id> Single ID hq worker stop 1 - stop a worker with ID 1 hq job cancel 5 - cancel a job with ID 5 <start>-<end>:<step> Inclusive range of IDs, starting at start and ending at end with step step hq submit --array=1-10 - create a task array with 10 tasks hq worker stop 1-3 - stop workers with IDs 1 , 2 and 3 hq job cancel 2-10:2 - cancel jobs with IDs 2 , 4 , 6 , 8 and 10 all All valid IDs hq worker stop all - stop all workers hq job cancel all - cancel all jobs last The most recently created ID hq worker stop last - stop most recently connected worker hq job cancel last - cancel most recently submitted job You can also combine the first two types of selectors with a comma. For example, the command $ hq worker stop 1,3,5-8 would stop workers with IDs 1 , 3 , 5 , 6 , 7 and 8 . Tip You can add underscore ( _ ) separators to any of the entered numeric values to improve readability: $ hq submit --array = 1 -1000_000 ...","title":"ID selector"},{"location":"cli/shortcuts/#supported-commands-and-options","text":"hq submit --array=<selector> hq worker stop <selector> hq job info <selector> does not support all (use hq job list instead) hq job cancel <selector> hq job wait <selector> hq job progress <selector>","title":"Supported commands and options"},{"location":"cli/shortcuts/#duration","text":"You can enter durations using various time suffixes, for example: 1h - one hour 3m - three minutes 14s - fourteen seconds 15days 2min 2s - fifteen days, two minutes and two seconds You can also combine these suffixed values together by separating them with a space. The full specification of allowed suffixed can be found here .","title":"Duration"},{"location":"cli/shortcuts/#supported-commands-and-options_1","text":"hq worker start --time-limit=<duration> hq worker start --idle-timeout=<duration> hq alloc add pbs --time-limit=<duration> hq submit --time-limit=<duration> ... hq submit --time-request=<duration> ... Tip For increased compatibility with PBS and Slurm , you can also specify the --time-limit option of hq alloc add using the HH:MM:SS format.","title":"Supported commands and options"},{"location":"deployment/","text":"Architecture # HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/#architecture","text":"HyperQueue has two runtime components: Server : a long-lived component which can run e.g. on a login node of a computing cluster. It handles task submitted by the user, manages and asks for HPC resources (PBS/Slurm jobs) and distributes tasks to available workers. Worker : runs on a computing node and actually executes submitted tasks. Server and the workers communicate over encrypted TCP/IP channels. The server may run on any machine, as long as the workers are able to connect to it over TCP/IP. Connecting in the other direction (from the server machine to the worker nodes) is not required. A common use-case is to start the server on a login of an HPC system. Learn more about deploying server and the workers .","title":"Architecture"},{"location":"deployment/allocation/","text":"Automatic allocation is one of the core features of HyperQueue. When you run HyperQueue on an HPC cluster, it allows you to autonomously ask the job manager (PBS/Slurm) for computing resources and spawn HyperQueue workers on the provided nodes. Using this mechanism, you can submit computations into HyperQueue without caring about the underlying PBS/Slurm jobs. Job terminology It is common to use the term \"job\" for jobs created by an HPC job manager, such as PBS or Slurm, which are used to perform computations on HPC clusters. However, HyperQueue also uses the term \"job\" for ensembles of tasks . To differentiate between these two, we will refer to jobs created by PBS or Slurm as allocations . We will also refer to PBS/Slurm as a job manager . Allocation queue # To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs --time-limit 1h -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues. Parameters # In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue: Time limit # Format 1 : --time-limit <duration> Sets the walltime of created allocations. This parameter is required , as HyperQueue must know the duration of the individual allocations. Make sure that you pass a time limit that does not exceed the limit of the PBS/Slurm queue that you intend to use, otherwise the allocation submissions will fail. You can use the dry-run command to debug this. Workers in this allocation queue will be by default created with a time limit equal to the time limit of the queue (unless overridden with Worker time limit ). Important If you specify a time request for a task, you should be aware that the time limit for the allocation queue should be larger than the time request if you want to run this task on workers created by this allocations queue, because it will always take some time before a worker is fully initialized. For example, if you set --time-request 1h when submitting a task, and --time-limit 1h when creating an allocation queue, this task will never get scheduled on workers from this queue. Backlog # Format: --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. Has to be a positive integer. Workers per allocation # Format: --workers-per-alloc <count> How many workers should be requested in each allocation. This corresponds to the number of requested nodes, as the allocator will always create a single worker per node. Max worker count # Format: --max-worker-count <count> Maximum number of workers that can be queued or running in the allocation queue. The total amount of workers will be usually limited by the manager (PBS/Slurm), but you can use this parameter to make the limit smaller, for example if you also want to create manager allocations outside HyperQueue. Worker resources # You can specify CPU and generic resources of workers spawned by the allocation queue. The name and syntax of these parameters is the same as when you create a worker manually : PBS Slurm $ hq alloc add pbs --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- --partition = p1 If you do not pass any resources, they will be detected automatically (same as it works with hq worker start ). Idle timeout # Format 1 : --idle-timeout <duration> Sets the idle timeout for workers started by the allocation queue. We suggest that you do not use a long duration for this parameter, as it can result in wasting precious allocation time. Worker start command # Format: --worker-start-cmd <cmd> Specifies a shell command that will be executed on each allocated node just before a worker is started on that node. You can use it e.g. to initialize some shared environment for the node, or to load software modules. Worker stop command # Format: - --worker-stop-cmd <cmd> Specifies a shell command that will be executed on each allocated node just after the worker stops on that node. You can use it e.g. to clean up a previously initialized environment for the node. Warning The execution of this command is best-effort! It is not guaranteed that the command will always be executed. For example, PBS/Slurm can kill the allocation without giving HQ a chance to run the command. Worker time limit # Format 1 : --worker-time-limit <duration> Sets the time limit of workers spawned by the allocation queue. After the time limit expires, the worker will be stopped. By default, the worker time limit is set to the time limit of the allocation queue. But if you want, you can shorten it with this flag to make the worker exit sooner, for example to give more time for a worker stop command to execute. Note This command is not designed to stop workers early if they have nothing to do. This functionality is provided by idle timeout . Name # Format: --name <name> Name of the allocation queue. It will be used to name allocations submitted to the job manager. Serves for debug purposes only. Behavior # The automatic allocator will submit allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if backlog was set to 4 and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. Note The backlog value does not limit the number of running allocations, only the number of queued allocations. Warning Do not set the backlog to a large number to avoid overloading the job manager. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes. Stopping automatic allocation # If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> When an allocation queue is removed, all its corresponding queued and running allocations will be canceled immediately. By default, HQ will not allow you to remove an allocation queue that contains a running allocation. If you want to force its removal, use the --force flag. When the HQ server stops, it will automatically remove all allocation queues and cleanup all allocations. Debugging automatic allocation # Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides a \"dry-run\" command that you can use to test allocation parameters. HyperQueue also provides various sources of information that can help you find out what is going on. To mitigate the case of incorrectly entered allocation parameters, HQ will also try to submit a test allocation (do a \"dry run\") into the target HPC job manager when you add a new allocation queue. If the test allocation fails, the queue will not be created. You can avoid this behaviour by passing the --no-dry-run flag to hq alloc add . There are also additional safety limits. If 10 allocations in a succession fail to be submitted, or if 3 allocations that were submitted fail during runtime in a succession, the corresponding allocation queue will be automatically removed. Dry-run command # To test whether PBS/Slurm will accept the submit parameters that you provide to the auto allocator without creating an allocation queue, you can use the dry-run command. It accepts the same parameters as hq alloc add , which it will use to immediately submit an allocation and print any encountered errors. $ hq alloc dry-run pbs --timelimit 2h -- q qexp -A Project1 If the allocation was submitted successfully, it will be canceled immediately to avoid wasting resources. Finding information about allocations # Basic queue information This command will show you details about allocations created by the automatic allocator. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq-submit.sh Useful autoalloc commands # Here is a list of useful commands to manage automatic allocation: Display a list of all allocation queues # $ hq alloc list Display information about an allocation queue # $ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option. You can use various shortcuts for the duration value. \u21a9 \u21a9 \u21a9","title":"Automatic Allocation"},{"location":"deployment/allocation/#allocation-queue","text":"To enable automatic allocation, you have to create an Allocation queue . It describes a specific configuration that will be used by HyperQueue to request computing resources from the job manager on your behalf. Each allocation queue has a set of parameters . You can use them to modify the behavior of automatic allocation, but for start you can simply use the defaults. However, you will almost certainly need to specify some credentials to be able to ask for computing resources using PBS/Slurm. To create a new allocation queue, use the following command and pass any required credentials (queue/partition name, account ID, etc.) after -- . These trailing arguments will then be passed directly to qsub / sbatch : PBS Slurm $ hq alloc add pbs --time-limit 1h -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h -- --partition = p1 Tip Make sure that a HyperQueue server is running when you execute this command. Allocation queues are not persistent, so you have to set them up each time you (re)start the server. Warning Do not pass the number of nodes that should be allocated or the allocation walltime using these trailing arguments. These parameters are configured using other means, see below . Once the queue is created, HyperQueue will start asking for allocations in order to provide computing resources (HyperQueue workers). The exact behavior of the automatic allocation process is described below . You can create multiple allocation queues, and you can even combine PBS queues with Slurm queues.","title":"Allocation queue"},{"location":"deployment/allocation/#parameters","text":"In addition to arguments that are passed to qsub / sbatch , you can also use several other command line options when creating a new allocation queue:","title":"Parameters"},{"location":"deployment/allocation/#time-limit","text":"Format 1 : --time-limit <duration> Sets the walltime of created allocations. This parameter is required , as HyperQueue must know the duration of the individual allocations. Make sure that you pass a time limit that does not exceed the limit of the PBS/Slurm queue that you intend to use, otherwise the allocation submissions will fail. You can use the dry-run command to debug this. Workers in this allocation queue will be by default created with a time limit equal to the time limit of the queue (unless overridden with Worker time limit ). Important If you specify a time request for a task, you should be aware that the time limit for the allocation queue should be larger than the time request if you want to run this task on workers created by this allocations queue, because it will always take some time before a worker is fully initialized. For example, if you set --time-request 1h when submitting a task, and --time-limit 1h when creating an allocation queue, this task will never get scheduled on workers from this queue.","title":"Time limit"},{"location":"deployment/allocation/#backlog","text":"Format: --backlog <count> How many allocations should be queued (waiting to be started) in PBS/Slurm at any given time. Has to be a positive integer.","title":"Backlog"},{"location":"deployment/allocation/#workers-per-allocation","text":"Format: --workers-per-alloc <count> How many workers should be requested in each allocation. This corresponds to the number of requested nodes, as the allocator will always create a single worker per node.","title":"Workers per allocation"},{"location":"deployment/allocation/#max-worker-count","text":"Format: --max-worker-count <count> Maximum number of workers that can be queued or running in the allocation queue. The total amount of workers will be usually limited by the manager (PBS/Slurm), but you can use this parameter to make the limit smaller, for example if you also want to create manager allocations outside HyperQueue.","title":"Max worker count"},{"location":"deployment/allocation/#worker-resources","text":"You can specify CPU and generic resources of workers spawned by the allocation queue. The name and syntax of these parameters is the same as when you create a worker manually : PBS Slurm $ hq alloc add pbs --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- -qqprod -AAccount1 $ hq alloc add slurm --time-limit 1h --cpus 4x4 --resource \"gpus/nvidia=range(1-2)\" -- --partition = p1 If you do not pass any resources, they will be detected automatically (same as it works with hq worker start ).","title":"Worker resources"},{"location":"deployment/allocation/#idle-timeout","text":"Format 1 : --idle-timeout <duration> Sets the idle timeout for workers started by the allocation queue. We suggest that you do not use a long duration for this parameter, as it can result in wasting precious allocation time.","title":"Idle timeout"},{"location":"deployment/allocation/#worker-start-command","text":"Format: --worker-start-cmd <cmd> Specifies a shell command that will be executed on each allocated node just before a worker is started on that node. You can use it e.g. to initialize some shared environment for the node, or to load software modules.","title":"Worker start command"},{"location":"deployment/allocation/#worker-stop-command","text":"Format: - --worker-stop-cmd <cmd> Specifies a shell command that will be executed on each allocated node just after the worker stops on that node. You can use it e.g. to clean up a previously initialized environment for the node. Warning The execution of this command is best-effort! It is not guaranteed that the command will always be executed. For example, PBS/Slurm can kill the allocation without giving HQ a chance to run the command.","title":"Worker stop command"},{"location":"deployment/allocation/#worker-time-limit","text":"Format 1 : --worker-time-limit <duration> Sets the time limit of workers spawned by the allocation queue. After the time limit expires, the worker will be stopped. By default, the worker time limit is set to the time limit of the allocation queue. But if you want, you can shorten it with this flag to make the worker exit sooner, for example to give more time for a worker stop command to execute. Note This command is not designed to stop workers early if they have nothing to do. This functionality is provided by idle timeout .","title":"Worker time limit"},{"location":"deployment/allocation/#name","text":"Format: --name <name> Name of the allocation queue. It will be used to name allocations submitted to the job manager. Serves for debug purposes only.","title":"Name"},{"location":"deployment/allocation/#behavior","text":"The automatic allocator will submit allocations to make sure that there are is a specific number of allocations waiting to be started by the job manager. This number is called backlog and you can set it when creating the queue. For example, if backlog was set to 4 and there is currently only one allocation queued into the job manager, the allocator would queue three more allocations. The backlog serves to pre-queue allocations, because it can take some time before the job manager starts them, and also as a load balancing factor, since it will allocate as many resources as the job manager allows. Note The backlog value does not limit the number of running allocations, only the number of queued allocations. Warning Do not set the backlog to a large number to avoid overloading the job manager. When an allocation starts, a HyperQueue worker will start and connect to the HyperQueue server that queued the allocation. The worker has the idle timeout set to five minutes, therefore it will terminate if it doesn't receive any new tasks for five minutes.","title":"Behavior"},{"location":"deployment/allocation/#stopping-automatic-allocation","text":"If you want to remove an allocation queue, use the following command: $ hq alloc remove <queue-id> When an allocation queue is removed, all its corresponding queued and running allocations will be canceled immediately. By default, HQ will not allow you to remove an allocation queue that contains a running allocation. If you want to force its removal, use the --force flag. When the HQ server stops, it will automatically remove all allocation queues and cleanup all allocations.","title":"Stopping automatic allocation"},{"location":"deployment/allocation/#debugging-automatic-allocation","text":"Since the automatic allocator is a \"background\" process that interacts with an external job manager, it can be challenging to debug its behavior. To aid with this process, HyperQueue provides a \"dry-run\" command that you can use to test allocation parameters. HyperQueue also provides various sources of information that can help you find out what is going on. To mitigate the case of incorrectly entered allocation parameters, HQ will also try to submit a test allocation (do a \"dry run\") into the target HPC job manager when you add a new allocation queue. If the test allocation fails, the queue will not be created. You can avoid this behaviour by passing the --no-dry-run flag to hq alloc add . There are also additional safety limits. If 10 allocations in a succession fail to be submitted, or if 3 allocations that were submitted fail during runtime in a succession, the corresponding allocation queue will be automatically removed.","title":"Debugging automatic allocation"},{"location":"deployment/allocation/#dry-run-command","text":"To test whether PBS/Slurm will accept the submit parameters that you provide to the auto allocator without creating an allocation queue, you can use the dry-run command. It accepts the same parameters as hq alloc add , which it will use to immediately submit an allocation and print any encountered errors. $ hq alloc dry-run pbs --timelimit 2h -- q qexp -A Project1 If the allocation was submitted successfully, it will be canceled immediately to avoid wasting resources.","title":"Dry-run command"},{"location":"deployment/allocation/#finding-information-about-allocations","text":"Basic queue information This command will show you details about allocations created by the automatic allocator. Extended logging To get more information about what is happening inside the allocator, start the HyperQueue server with the following environment variable: $ RUST_LOG = hyperqueue::server::autoalloc = debug hq server start The log output of the server will then contain a detailed trace of allocator actions. Allocation files Each time the allocator queues an allocation into the job manager, it will write the submitted bash script, allocation ID and stdout and stderr of the allocation to disk. You can find these files inside the server directory: $ ls <hq-server-dir>/hq-current/autoalloc/<queue-id>/<allocation-num>/ stderr stdout job-id hq-submit.sh","title":"Finding information about allocations"},{"location":"deployment/allocation/#useful-autoalloc-commands","text":"Here is a list of useful commands to manage automatic allocation:","title":"Useful autoalloc commands"},{"location":"deployment/allocation/#display-a-list-of-all-allocation-queues","text":"$ hq alloc list","title":"Display a list of all allocation queues"},{"location":"deployment/allocation/#display-information-about-an-allocation-queue","text":"$ hq alloc info <queue-id> You can filter allocations by their state ( queued , running , finished , failed ) using the --filter option. You can use various shortcuts for the duration value. \u21a9 \u21a9 \u21a9","title":"Display information about an allocation queue"},{"location":"deployment/cloud/","text":"Starting HQ without shared file system # On system without shared file system, all what is needed is to distribute access file ( access.json ) to clients and workers. This file contains address and port where server is running and secret keys. By default, client and worker search for access.json in $HOME/.hq-server . Generate access file in advance # In many cases you, we want to generate an access file in advance before any server is started; moreover, we do not want to regenerate secret keys in every start of server, because we do not want to redistribute access when server is restarted. To solve this, an access file can be generated in advance by command \"generate-access\", e.g.: $ hq server generate-access myaccess.json --client-port=6789 --worker-port=1234 This generates myaccess.json that contains generates keys and host information. The server can be later started with this configuration as follows: $ hq server start --access-file=myaccess.json Note: That server still generates and manages \"own\" access.json in the server directory path. For connecting clients and workers you can use both, myaccess.json or newly generated access.json , they are same. Example of starting a worker from myaccess.json $ mv myaccess.json /mydirectory/access.json $ hq --server-dir=/mydirectory worker start Splitting access for client and workers # Access file contains two secret keys and two points to connect, for clients and for workers. This information can be divided into two separate files, containing only information needed only by clients or only by workers. $ hq server generate-access full.json --client-file=client.json --worker-file=worker.json --client-port=6789 --worker-port=1234 This command creates three files: full.json , client.json , worker.json . For starting a client you can use client.json as access.json while it does not contain information for workers. For starting a worker you can use worker.json as access.json while it does not contain information for clients. For starting server ( hq server start --access-file=... ) you have to use full.json as it contains all necessary information. Setting different server hostname for workers and clients # You can use the following command to configure different hostnames under which the server is visible to workers and clients. hq server generate-access full.json --worker-host=<WORKER_HOST> --client-host=<CLIENT_HOST> ...","title":"Without Shared Filesystem"},{"location":"deployment/cloud/#starting-hq-without-shared-file-system","text":"On system without shared file system, all what is needed is to distribute access file ( access.json ) to clients and workers. This file contains address and port where server is running and secret keys. By default, client and worker search for access.json in $HOME/.hq-server .","title":"Starting HQ without shared file system"},{"location":"deployment/cloud/#generate-access-file-in-advance","text":"In many cases you, we want to generate an access file in advance before any server is started; moreover, we do not want to regenerate secret keys in every start of server, because we do not want to redistribute access when server is restarted. To solve this, an access file can be generated in advance by command \"generate-access\", e.g.: $ hq server generate-access myaccess.json --client-port=6789 --worker-port=1234 This generates myaccess.json that contains generates keys and host information. The server can be later started with this configuration as follows: $ hq server start --access-file=myaccess.json Note: That server still generates and manages \"own\" access.json in the server directory path. For connecting clients and workers you can use both, myaccess.json or newly generated access.json , they are same. Example of starting a worker from myaccess.json $ mv myaccess.json /mydirectory/access.json $ hq --server-dir=/mydirectory worker start","title":"Generate access file in advance"},{"location":"deployment/cloud/#splitting-access-for-client-and-workers","text":"Access file contains two secret keys and two points to connect, for clients and for workers. This information can be divided into two separate files, containing only information needed only by clients or only by workers. $ hq server generate-access full.json --client-file=client.json --worker-file=worker.json --client-port=6789 --worker-port=1234 This command creates three files: full.json , client.json , worker.json . For starting a client you can use client.json as access.json while it does not contain information for workers. For starting a worker you can use worker.json as access.json while it does not contain information for clients. For starting server ( hq server start --access-file=... ) you have to use full.json as it contains all necessary information.","title":"Splitting access for client and workers"},{"location":"deployment/cloud/#setting-different-server-hostname-for-workers-and-clients","text":"You can use the following command to configure different hostnames under which the server is visible to workers and clients. hq server generate-access full.json --worker-host=<WORKER_HOST> --client-host=<CLIENT_HOST> ...","title":"Setting different server hostname for workers and clients"},{"location":"deployment/server/","text":"The server is a crucial component of HyperQueue which manages workers and jobs . Before running any computations or deploying workers, you must first start the server. Starting the server # The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST Server directory # When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server. Keeping the server alive # The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux . Stopping server # You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Server"},{"location":"deployment/server/#starting-the-server","text":"The server can be started by running the following command: $ hq server start You can change the hostname under which the server is visible to workers with the --host option: $ hq server start --host = HOST","title":"Starting the server"},{"location":"deployment/server/#server-directory","text":"When the server is started, it creates a server directory where it stores information needed for submitting jobs and connecting workers . This directory is then used to select a running HyperQueue instance. By default, the server directory will be stored in $HOME/.hq-server . This location may be changed with the option --server-dir=<PATH> , which is available for all HyperQueue CLI commands. You can run more instances of HyperQueue under the same Unix user, by making them use different server directories. If you use a non-default server directory, make sure to pass the same --server-dir to all HyperQueue commands that should use the selected HyperQueue server: $ hq --server-dir = foo server start $ hq --server-dir = foo worker start Important When you start the server, it will create a new subdirectory in the server directory, which will store the data of the current running instance. It will also create a symlink hq-current which will point to the currently active subdirectory. Using this approach, you can start a server using the same server directory multiple times without overwriting data of the previous runs. Server directory access Encryption keys are stored in the server directory. Whoever has access to the server directory may submit jobs, connect workers to the server and decrypt communication between HyperQueue components. By default, the directory is only accessible by the user who started the server.","title":"Server directory"},{"location":"deployment/server/#keeping-the-server-alive","text":"The server is supposed to be a long-lived component. If you shut it down, all workers will disconnect and all computations will be stopped. Therefore, it is important to make sure that the server will stay running e.g. even after you disconnect from a cluster where the server is deployed. For example, if you SSH into a login node of an HPC cluster and then run the server like this: $ hq server start The server will quit when your SSH session ends, because it will receive a SIGHUP signal. You can use established Unix approaches to avoid this behavior, for example prepending the command with nohup or using a terminal multiplexer like tmux .","title":"Keeping the server alive"},{"location":"deployment/server/#stopping-server","text":"You can stop a running server with the following command: $ hq server stop When a server is stopped, all running jobs and connected workers will be immediately stopped.","title":"Stopping server"},{"location":"deployment/worker/","text":"Workers connect to a running instance of a HyperQueue server and wait for task assignments. Once some task is assigned to them, they will compute it and notify the server of its completion. Starting workers # Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually. Automatic worker deployment (recommended) # If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers. Manual worker deployment # If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though. Deploying a worker using PBS/Slurm # If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment. Stopping workers # If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : $ hq worker stop <selector> Time limit # HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started manually inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated. The time limit of a worker affects what tasks can be scheduled to it. For example, a task submitted with --time-request 10m will not be scheduled onto a worker that only has a remaining time limit of 5 minutes. Idle timeout # When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout. Worker state # Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout . Lost connection to the server # The behavior of what should happen with a worker that lost its connection to the server is configured via hq worker start --on-server-lost=<policy> . You can select from two policies: stop - The worker immediately terminates and kills all currently running tasks. finish-running - The worker does not start to execute any new tasks, but it tries to finish tasks that are already running. When all such tasks finish, the worker will terminate. stop is the default policy when a worker is manually started by hq worker start . When a worker is started by the automatic allocator , then finish-running is used as the default value. Useful worker commands # Here is a list of useful worker commands: Display worker list # This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command. Display information about a specific worker # $ hq worker info <worker-id> Worker groups # Each worker is a member exactly of one group. Groups are used when multi-node tasks are used. See more here You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Workers"},{"location":"deployment/worker/#starting-workers","text":"Workers should be started on machines that will actually execute the submitted computations, e.g. computing nodes on an HPC cluster. You can either use the automatic allocation system of HyperQueue to start workers as needed, or deploy workers manually.","title":"Starting workers"},{"location":"deployment/worker/#automatic-worker-deployment-recommended","text":"If you are using a job manager (PBS or Slurm) on an HPC cluster, the easiest way of deploying workers is to use Automatic allocation . It is a component of HyperQueue that takes care of submitting PBS/Slurm jobs and spawning HyperQueue workers.","title":"Automatic worker deployment (recommended)"},{"location":"deployment/worker/#manual-worker-deployment","text":"If you want to start a worker manually, you can use the following command: $ hq worker start Each worker will be assigned a unique ID that you can use in later commands to query information about the worker or to stop it. By default, the worker will try to connect to a server using the default server directory . If you want to connect to a different server, use the --server-dir option. Sharing the server directory When you start a worker, it will need to read the server directory to find out how to connect to the server. The directory thus has to be accesible both by the server and the worker machines. On HPC clusters, it is common that login nodes and compute nodes use a shared filesystem, so this shouldn't be a problem. However, if a shared filesystem is not available on your cluster, you can just copy the server directory from the server machine to the worker machine and access it from there. The worker machine still has to be able to initiate a TCP/IP connection to the server machine though.","title":"Manual worker deployment"},{"location":"deployment/worker/#deploying-a-worker-using-pbsslurm","text":"If you want to manually start a worker using PBS or Slurm, simply use the corresponding submit command ( qsub or sbatch ) and run the hq worker start command inside the allocated job. If you want to start a worker on each allocated node, you can run this command on each node using e.g. mpirun . Example submission script: PBS Slurm #!/bin/bash #PBS -q <queue> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager pbs # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager pbs #!/bin/bash #SBATCH --partition <partition> # Run a worker on the main node /<path-to-hyperqueue>/hq worker start --manager slurm # Run a worker on all allocated nodes ml OpenMPI mpirun /<path-to-hyperqueue>/hq worker start --manager slurm The worker will try to automatically detect that it is started under a PBS/Slurm job, but you can also explicitly pass the option --manager <pbs/slurm> to tell the worker that it should expect a specific environment.","title":"Deploying a worker using PBS/Slurm"},{"location":"deployment/worker/#stopping-workers","text":"If you have started a worker manually, and you want to stop it, you can use the hq worker stop command 1 : $ hq worker stop <selector>","title":"Stopping workers"},{"location":"deployment/worker/#time-limit","text":"HyperQueue workers are designed to be volatile, i.e. it is expected that they will be stopped from time to time, because they are often started inside PBS/Slurm jobs that have a limited duration. It is very useful for the workers to know how much remaining time (\"lifetime\") do they have until they will be stopped. This duration is called the Worker time limit . When a worker is started manually inside a PBS or Slurm job, it will automatically calculate the time limit from the job's metadata. If you want to set time limit for workers started outside of PBS/Slurm jobs or if you want to override the detected settings, you can use the --time-limit=<DURATION> option 2 when starting the worker. When the time limit is reached, the worker is automatically terminated. The time limit of a worker affects what tasks can be scheduled to it. For example, a task submitted with --time-request 10m will not be scheduled onto a worker that only has a remaining time limit of 5 minutes.","title":"Time limit"},{"location":"deployment/worker/#idle-timeout","text":"When you deploy HQ workers inside a PBS or Slurm job, keeping the worker alive will drain resources from your accounting project (unless you use a free queue). If a worker has nothing to do, it might be better to terminate it sooner to avoid paying these costs for no reason. You can achieve this using Worker idle timeout . If you use it, the worker will automatically stop if it receives no task to compute for the specified duration. For example, if you set the idle duration to five minutes, the worker will stop once it hadn't received any task to compute for five minutes. You can set the idle timeout using the --idle-timeout option 2 when starting the worker. Tip Workers started automatically have the idle timeout set to five minutes. Idle timeout can also be configured globally for all workers using the --idle-timeout option when starting a server: $ hq server start --idle-timeout = <TIMEOUT> This value will be then used for each worker that does not explicitly specify its own idle timeout.","title":"Idle timeout"},{"location":"deployment/worker/#worker-state","text":"Each worker can be in one of the following states: Running Worker is running and is able to process tasks Connection lost Worker lost connection to the server. Probably someone manually killed the worker or job walltime in its PBS/Slurm job was reached . Heartbeat lost Communication between server and worker was interrupted. It usually signifies a network problem or a hardware crash of the computational node. Stopped Worker was stopped . Idle timeout Worker was terminated due to Idle timeout .","title":"Worker state"},{"location":"deployment/worker/#lost-connection-to-the-server","text":"The behavior of what should happen with a worker that lost its connection to the server is configured via hq worker start --on-server-lost=<policy> . You can select from two policies: stop - The worker immediately terminates and kills all currently running tasks. finish-running - The worker does not start to execute any new tasks, but it tries to finish tasks that are already running. When all such tasks finish, the worker will terminate. stop is the default policy when a worker is manually started by hq worker start . When a worker is started by the automatic allocator , then finish-running is used as the default value.","title":"Lost connection to the server"},{"location":"deployment/worker/#useful-worker-commands","text":"Here is a list of useful worker commands:","title":"Useful worker commands"},{"location":"deployment/worker/#display-worker-list","text":"This command will display a list of workers that are currently connected to the server: $ hq worker list If you also want to include workers that are offline (i.e. that have crashed or disconnected in the past), pass the --all flag to the list command.","title":"Display worker list"},{"location":"deployment/worker/#display-information-about-a-specific-worker","text":"$ hq worker info <worker-id>","title":"Display information about a specific worker"},{"location":"deployment/worker/#worker-groups","text":"Each worker is a member exactly of one group. Groups are used when multi-node tasks are used. See more here You can use various shortcuts to select multiple workers at once. \u21a9 You can use various shortcuts for the duration value. \u21a9 \u21a9","title":"Worker groups"},{"location":"jobs/arrays/","text":"It is a common use case to execute the same command for multiple input parameters, for example: Perform a simulation for each input file in a directory or for each line in a CSV file. Train many machine learning models using hyperparameter search for each model configuration. HyperQueue allows you to do this using a job that contains many tasks. We call such jobs Task arrays . You can create a task array with a single submit command and then manage all created tasks as a single group using its containing job. Note Task arrays are somewhat similar to \"job arrays\" used by PBS and Slurm. However, HQ does not use PBS/Slurm job arrays for implementing this feature. Therefore, the limits that are commonly enforced on job arrays on HPC clusters do not apply to HyperQueue task arrays. Creating task arrays # To create a task array, you must provide some source that will determine how many tasks should be created and what inputs (environment variables) should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming . Integer range # The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ... Lines of a file # Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ... JSON array # You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... If items.json contained this content: [{ \"batch_size\" : 4 , \"learning_rate\" : 0.01 }, { \"batch_size\" : 8 , \"learning_rate\" : 0.001 }] then HyperQueue would create two tasks, one with HQ_ENTRY set to {\"batch_size\": 4, \"learning_rate\": 0.01} and the other with HQ_ENTRY set to {\"batch_size\": 8, \"learning_rate\": 0.001} . The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"Task Arrays"},{"location":"jobs/arrays/#creating-task-arrays","text":"To create a task array, you must provide some source that will determine how many tasks should be created and what inputs (environment variables) should be passed to each task so that you can differentiate them. Currently, you can create a task array from a range of integers , from each line of a text file or from each item of a JSON array . You cannot combine these sources, as they are mutually exclusive. Handling many output files By default, each task in a task array will create two output files (containing stdout and stderr output). Creating large task arrays will thus generate a lot of files, which can be problematic especially on network-based shared filesystems, such as Lustre. To avoid this, you can either disable the output or use Output streaming .","title":"Creating task arrays"},{"location":"jobs/arrays/#integer-range","text":"The simplest way of creating a task array is to specify an integer range. A task will be started for each integer in the range. You can then differentiate between the individual tasks using task id that can be accessed through the HQ_TASK_ID environment variable . You can enter the range as two unsigned numbers separated by a dash 1 , where the first number should be smaller than the second one. The range is inclusive. The range is entered using the --array option: # Task array with 3 tasks, with ids 1, 2, 3 $ hq submit --array 1 -3 ... # Task array with 6 tasks, with ids 0, 2, 4, 6, 8, 10 $ hq submit --array 0 -10:2 ...","title":"Integer range"},{"location":"jobs/arrays/#lines-of-a-file","text":"Another way of creating a task array is to provide a text file with multiple lines. Each line from the file will be passed to a separate task, which can access the value of the line using the environment variable HQ_ENTRY . This is useful if you want to e.g. process each file inside some directory. You can generate a text file that will contain each filepath on a separate line and then pass it to the submit command using the --each-line option: $ hq submit --each-line entries.txt ...","title":"Lines of a file"},{"location":"jobs/arrays/#json-array","text":"You can also specify the source using a JSON array stored inside a file. HyperQueue will then create a task for each item in the array and pass the item as a JSON string to the corresponding task using the environment variable HQ_ENTRY . Note The root JSON value stored inside the file must be an array. You can create a task array in this way using the --from-json option: $ hq submit --from-json items.json ... If items.json contained this content: [{ \"batch_size\" : 4 , \"learning_rate\" : 0.01 }, { \"batch_size\" : 8 , \"learning_rate\" : 0.001 }] then HyperQueue would create two tasks, one with HQ_ENTRY set to {\"batch_size\": 4, \"learning_rate\": 0.01} and the other with HQ_ENTRY set to {\"batch_size\": 8, \"learning_rate\": 0.001} . The full syntax can be seen in the second selector of the ID selector shortcut . \u21a9","title":"JSON array"},{"location":"jobs/cresources/","text":"CPU resource management # Note In this text, we use the term CPU for a resource that is provided by the operating system (e.g. what you get from /proc/cpuinfo ). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use the term socket to refer to physical CPUs. Brief introduction # HyperQueue allows you to select how many CPU cores will be allocated for each task. By default, each task requires a single CPU of the worker's node. This can be changed by the flag --cpus . For example, to submit a job with a task that requires 8 CPUs: $ hq submit --cpus = 8 <program_name> <args...> This ensures that HyperQueue will exclusively reserve 8 CPUs for this task when it is started. This task would thus never be scheduled on a worker that has less than 8 CPUs. Note that this reservation exists on a logical level only. To ensure more direct mapping to physical cores, see pinning below. CPUs are a resource # From version 0.13.0, CPUs are managed as any other resource under name \"cpus\", with the following additions: If a task does not explicitly specify the number of cpus, then it requests 1 CPU as default. CPUs request can be specified by hq submit --cpus=X ... where --cpus=X is a shortcut for --resource cpus=X , and X can be all valid requests for a resource, including values like all or 8 compact! . (More in Resource Management ). A task may be automatically pinned to a given CPUs (see pinning ). There are some extra environmental variables for CPUs (see below). CPUs are automatically detected. See below for information about NUMA or Hyper Threading. CPUs provided by a worker can be explicitly specified via --cpus , see below. CPU related environment variables # The following variables are created when a task is executed: HQ_CPUS - List of cores assigned to a task. (this is an alias for HQ_RESOURCE_VALUES_cpus ). HQ_PIN - Is set to taskset or omp (depending on the used pin mode) if the task was pinned by HyperQueue (see below). NUM_OMP_THREADS -- Set to number of cores assigned for task. (For compatibility with OpenMP). Pinning # By default, HQ internally allocates CPUs on a logical level. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs of the worker, but process assignment to cores is left to the system scheduler, which may move processes across CPUs as it wants. If this is not desired, especially in the case of NUMA, processes could be pinned, either manually or automatically. Automatic pinning # HyperQueue can pin threads using two ways: with taskset or by setting OpenMP environment variables. You can use the --pin flag to choose between these two modes. taskset OpenMP $ hq submit --pin taskset --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: taskset -c \"<allocated-cores>\" <your-program> <args> ` $ hq submit --pin omp --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: OMP_PROC_BIND = close OMP_PLACES = \"{<allocated-cores>}\" <your-program> <args> If any automatic pinning mode is enabled, the environment variable HQ_PIN will be set. Manual pinning # If you want to gain full control over core pinning, you may pin the process by yourself. The assigned CPUs are stored in the environment variable HQ_CPUS as a comma-delimited list of CPU IDs. You can use utilities such as taskset or numactl and pass them HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not also use the --pin flag of the submit command. It may have some unwanted interferences. Below you can find an example of a script file that pins the executed process manually using taskset and numactl : taskset numactl #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> If you submit this script with hq submit --cpus=4 script.sh , it will pin your program to 4 CPUs allocated by HQ. NUMA allocation strategy # Workers automatically detect the number of CPUs and on Linux systems they also detect their partitioning into sockets. When a NUMA architecture is automatically detected, indexed resource with groups is used for resource \"cpus\". You can then use allocation strategies for groups to specify how sockets are allocated. They follow the same rules as normal allocation strategies; for clarity we are rephrasing the group allocation strategies in terms of cores and sockets: Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. $ hq submit --cpus = \"8 compact\" ... Strict Compact ( compact! ) - Always allocates cores on as few sockets as possible for a target node. The task will not be executed until the requirement could be fully fulfilled. For example, if your worker has 4 cores per socket, and you ask for 4 CPUs, it will always be executed on a single socket. If you ask for 8 CPUs, it will always be executed on two sockets. $ hq submit --cpus = \"8 compact!\" ... Tip You might encounter a problem in your shell when you try to specify the strict compact policy, because the definition contains an exclamation mark ( ! ). In that case, try to wrap the policy in single quotes, like this: $ hq submit --cpus = '8 compact!' ... Scatter ( scatter ) - Allocate cores across as many sockets possible, based on the currently available cores of a worker. If your worker has 4 sockets with 8 cores per socket, and you ask for 8 CPUs, then HQ will try to run the process with 2 CPUs on each socket, if possible given the currently available worker cores. $ hq submit --cpus = \"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=<X> is equivalent to --cpus=\"<X> compact\" . Note Specifying a policy only has an effect if you have more than one socket (physical CPUs). In case of a single socket, policies are indistinguishable. CPU configuration # Each worker will automatically detect the number of CPUs available. On Linux systems, it will also detect the partitioning into sockets (NUMA configuration). In most cases, it should work out of the box. If you want to see how will a HQ worker see your CPU configuration without actually starting the worker, you can use the hq worker hwdetect command, which will print the detected CPU configuration. Manual specification of CPU configuration # If the automatic detection fails for some reason, or you want to manually configure the CPU configuration, you can use the --cpus flag when starting a worker. It is an alias for --resource cpus=... (More in Resource Management ), except it also allow to define --cpus=N where N is an integer; it is then interpreted as 1xN in the resource definition. Below there are some examples of configuration that you can specify: Worker with 8 CPUs and a single socket. $ hq worker start --cpus = 8 Worker with 2 sockets with 12 cores per socket. $ hq worker start --cpus = 2x12 Manually specify that the worker should use the following core ids and how they are organized into sockets. In this example, two sockets are defined, one with 3 cores and one with 2 cores. $ hq worker start --cpus =[[ 2 , 3 , 4 ] , [ 10 , 14 ]] Disable Hyper Threading # If you want to detect CPUs but ignore HyperThreading then --no-hyper-threading flag can be used. It will detect only the first virtual core of each physical core. Example: $ hq worker start --no-hyper-threading","title":"CPU Resources"},{"location":"jobs/cresources/#cpu-resource-management","text":"Note In this text, we use the term CPU for a resource that is provided by the operating system (e.g. what you get from /proc/cpuinfo ). In this meaning, it is usually a core of a physical CPU. In the text related to NUMA we use the term socket to refer to physical CPUs.","title":"CPU resource management"},{"location":"jobs/cresources/#brief-introduction","text":"HyperQueue allows you to select how many CPU cores will be allocated for each task. By default, each task requires a single CPU of the worker's node. This can be changed by the flag --cpus . For example, to submit a job with a task that requires 8 CPUs: $ hq submit --cpus = 8 <program_name> <args...> This ensures that HyperQueue will exclusively reserve 8 CPUs for this task when it is started. This task would thus never be scheduled on a worker that has less than 8 CPUs. Note that this reservation exists on a logical level only. To ensure more direct mapping to physical cores, see pinning below.","title":"Brief introduction"},{"location":"jobs/cresources/#cpus-are-a-resource","text":"From version 0.13.0, CPUs are managed as any other resource under name \"cpus\", with the following additions: If a task does not explicitly specify the number of cpus, then it requests 1 CPU as default. CPUs request can be specified by hq submit --cpus=X ... where --cpus=X is a shortcut for --resource cpus=X , and X can be all valid requests for a resource, including values like all or 8 compact! . (More in Resource Management ). A task may be automatically pinned to a given CPUs (see pinning ). There are some extra environmental variables for CPUs (see below). CPUs are automatically detected. See below for information about NUMA or Hyper Threading. CPUs provided by a worker can be explicitly specified via --cpus , see below.","title":"CPUs are a resource"},{"location":"jobs/cresources/#cpu-related-environment-variables","text":"The following variables are created when a task is executed: HQ_CPUS - List of cores assigned to a task. (this is an alias for HQ_RESOURCE_VALUES_cpus ). HQ_PIN - Is set to taskset or omp (depending on the used pin mode) if the task was pinned by HyperQueue (see below). NUM_OMP_THREADS -- Set to number of cores assigned for task. (For compatibility with OpenMP).","title":"CPU related environment variables"},{"location":"jobs/cresources/#pinning","text":"By default, HQ internally allocates CPUs on a logical level. In other words, HQ ensures that the sum of requests of concurrently running tasks does not exceed the number of CPUs of the worker, but process assignment to cores is left to the system scheduler, which may move processes across CPUs as it wants. If this is not desired, especially in the case of NUMA, processes could be pinned, either manually or automatically.","title":"Pinning"},{"location":"jobs/cresources/#automatic-pinning","text":"HyperQueue can pin threads using two ways: with taskset or by setting OpenMP environment variables. You can use the --pin flag to choose between these two modes. taskset OpenMP $ hq submit --pin taskset --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: taskset -c \"<allocated-cores>\" <your-program> <args> ` $ hq submit --pin omp --cpus = 8 <your-program> <args> will cause HyperQueue to execute your program like this: OMP_PROC_BIND = close OMP_PLACES = \"{<allocated-cores>}\" <your-program> <args> If any automatic pinning mode is enabled, the environment variable HQ_PIN will be set.","title":"Automatic pinning"},{"location":"jobs/cresources/#manual-pinning","text":"If you want to gain full control over core pinning, you may pin the process by yourself. The assigned CPUs are stored in the environment variable HQ_CPUS as a comma-delimited list of CPU IDs. You can use utilities such as taskset or numactl and pass them HQ_CPUS to pin a process to these CPUs. Warning If you manually pin your processes, do not also use the --pin flag of the submit command. It may have some unwanted interferences. Below you can find an example of a script file that pins the executed process manually using taskset and numactl : taskset numactl #!/bin/bash taskset -c $HQ_CPUS <your-program> <args...> #!/bin/bash numactl -C $HQ_CPUS <your-program> <args...> If you submit this script with hq submit --cpus=4 script.sh , it will pin your program to 4 CPUs allocated by HQ.","title":"Manual pinning"},{"location":"jobs/cresources/#numa-allocation-strategy","text":"Workers automatically detect the number of CPUs and on Linux systems they also detect their partitioning into sockets. When a NUMA architecture is automatically detected, indexed resource with groups is used for resource \"cpus\". You can then use allocation strategies for groups to specify how sockets are allocated. They follow the same rules as normal allocation strategies; for clarity we are rephrasing the group allocation strategies in terms of cores and sockets: Compact ( compact ) - Tries to allocate cores on as few sockets as possible in the current worker state. $ hq submit --cpus = \"8 compact\" ... Strict Compact ( compact! ) - Always allocates cores on as few sockets as possible for a target node. The task will not be executed until the requirement could be fully fulfilled. For example, if your worker has 4 cores per socket, and you ask for 4 CPUs, it will always be executed on a single socket. If you ask for 8 CPUs, it will always be executed on two sockets. $ hq submit --cpus = \"8 compact!\" ... Tip You might encounter a problem in your shell when you try to specify the strict compact policy, because the definition contains an exclamation mark ( ! ). In that case, try to wrap the policy in single quotes, like this: $ hq submit --cpus = '8 compact!' ... Scatter ( scatter ) - Allocate cores across as many sockets possible, based on the currently available cores of a worker. If your worker has 4 sockets with 8 cores per socket, and you ask for 8 CPUs, then HQ will try to run the process with 2 CPUs on each socket, if possible given the currently available worker cores. $ hq submit --cpus = \"8 scatter\" ... The default policy is the compact policy, i.e. --cpus=<X> is equivalent to --cpus=\"<X> compact\" . Note Specifying a policy only has an effect if you have more than one socket (physical CPUs). In case of a single socket, policies are indistinguishable.","title":"NUMA allocation strategy"},{"location":"jobs/cresources/#cpu-configuration","text":"Each worker will automatically detect the number of CPUs available. On Linux systems, it will also detect the partitioning into sockets (NUMA configuration). In most cases, it should work out of the box. If you want to see how will a HQ worker see your CPU configuration without actually starting the worker, you can use the hq worker hwdetect command, which will print the detected CPU configuration.","title":"CPU configuration"},{"location":"jobs/cresources/#manual-specification-of-cpu-configuration","text":"If the automatic detection fails for some reason, or you want to manually configure the CPU configuration, you can use the --cpus flag when starting a worker. It is an alias for --resource cpus=... (More in Resource Management ), except it also allow to define --cpus=N where N is an integer; it is then interpreted as 1xN in the resource definition. Below there are some examples of configuration that you can specify: Worker with 8 CPUs and a single socket. $ hq worker start --cpus = 8 Worker with 2 sockets with 12 cores per socket. $ hq worker start --cpus = 2x12 Manually specify that the worker should use the following core ids and how they are organized into sockets. In this example, two sockets are defined, one with 3 cores and one with 2 cores. $ hq worker start --cpus =[[ 2 , 3 , 4 ] , [ 10 , 14 ]]","title":"Manual specification of CPU configuration"},{"location":"jobs/cresources/#disable-hyper-threading","text":"If you want to detect CPUs but ignore HyperThreading then --no-hyper-threading flag can be used. It will detect only the first virtual core of each physical core. Example: $ hq worker start --no-hyper-threading","title":"Disable Hyper Threading"},{"location":"jobs/directives/","text":"Directives # You can specify job parameters using special comments ( directives ) specified in a submitted shell script. Directives are lines that begin with the #HQ prefix. Any text following this prefix will be interpreted as a command line argument for hq submit . Example directive file # Suppose that script.sh has the following content: #!/bin/bash #HQ --name=Example #HQ --cpus=\"2 compact\" --pin taskset ./my-program If you execute $ hq submit script.sh it will behave as if you have executed $ hq submit --name = Example --cpus = \"2 compact\" --pin taskset script.sh Directives mode # You can select three modes using the --directives flag of hq submit . The mode will determine when should HyperQueue attempt to parse directives from the provided command. auto (default) - Directives will be parsed if the first command passed to hq submit has the .sh extension. file - Directives will be parsed from the first command passed to hq submit . stdin - Directives will be parsed from stdin (see --stdin ) off - Directives will not be parsed. Tip When HQ parses directives from a file, it will also try to parse a shebang line from the script and use it to select an interpreter for running the script. Notes # Directives have to be defined at the beginning of the file. Only comments or empty lines are allowed to precede the directives. Directives have to be defined in the first 32KiB of the file, the rest of the file is ignored. Parameters set via CLI have precedence over parameters set via direectives: Parameters that cannot occur multiple times (like --name ) will be overriden by values set from CLI. Parameters that can occur multiple times (like --resource ) will be combined from CLI and from directives. A script may contain more lines with the #HQ prefix, such lines are combined and evaluated as a continuous list of parameters.","title":"Directives"},{"location":"jobs/directives/#directives","text":"You can specify job parameters using special comments ( directives ) specified in a submitted shell script. Directives are lines that begin with the #HQ prefix. Any text following this prefix will be interpreted as a command line argument for hq submit .","title":"Directives"},{"location":"jobs/directives/#example-directive-file","text":"Suppose that script.sh has the following content: #!/bin/bash #HQ --name=Example #HQ --cpus=\"2 compact\" --pin taskset ./my-program If you execute $ hq submit script.sh it will behave as if you have executed $ hq submit --name = Example --cpus = \"2 compact\" --pin taskset script.sh","title":"Example directive file"},{"location":"jobs/directives/#directives-mode","text":"You can select three modes using the --directives flag of hq submit . The mode will determine when should HyperQueue attempt to parse directives from the provided command. auto (default) - Directives will be parsed if the first command passed to hq submit has the .sh extension. file - Directives will be parsed from the first command passed to hq submit . stdin - Directives will be parsed from stdin (see --stdin ) off - Directives will not be parsed. Tip When HQ parses directives from a file, it will also try to parse a shebang line from the script and use it to select an interpreter for running the script.","title":"Directives mode"},{"location":"jobs/directives/#notes","text":"Directives have to be defined at the beginning of the file. Only comments or empty lines are allowed to precede the directives. Directives have to be defined in the first 32KiB of the file, the rest of the file is ignored. Parameters set via CLI have precedence over parameters set via direectives: Parameters that cannot occur multiple times (like --name ) will be overriden by values set from CLI. Parameters that can occur multiple times (like --resource ) will be combined from CLI and from directives. A script may contain more lines with the #HQ prefix, such lines are combined and evaluated as a continuous list of parameters.","title":"Notes"},{"location":"jobs/failure/","text":"In distributed systems, failure is inevitable. This sections describes how HyperQueue handles various types of failures and how can you affect its behavior. Resubmitting jobs # When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq job resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq job resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array. Task restart # Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id. Task array failures # By default, when a single task of a task array fails, the computation of the job will continue. You can change this behavior with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Handling Failure"},{"location":"jobs/failure/#resubmitting-jobs","text":"When a job fails or is canceled, you might want to submit it again, without the need to pass all the original parameters. You can achieve this using resubmit : $ hq job resubmit <job-id> It wil create a new job that has the same configuration as the job with the entered job id. This is especially useful for task arrays . By default, resubmit will submit all tasks of the original job; however, you can specify only a subset of tasks based on their state : $ hq job resubmit <job-id> --status = failed,canceled Using this command you can resubmit e.g. only the tasks that have failed, without the need to recompute all tasks of a large task array.","title":"Resubmitting jobs"},{"location":"jobs/failure/#task-restart","text":"Sometimes a worker might crash while it is executing some task. In that case the server will reschedule that task to a different worker and the task will begin executing from the beginning. In order to let the executed application know that the same task is being executed repeatedly, HyperQueue assigns each execution a separate Instance id . It is a 32b non-negative number that identifies each (re-)execution of a task. It is guaranteed that a newer execution of a task will have a larger instance id, however HyperQueue explicitly does not guarantee any specific values or differences between two ids. Each instance id is valid only for a particular task. Two different tasks may have the same instance id.","title":"Task restart"},{"location":"jobs/failure/#task-array-failures","text":"By default, when a single task of a task array fails, the computation of the job will continue. You can change this behavior with the --max-fails=<X> option of the submit command, where X is non-negative integer. If specified, once more tasks than X tasks fail, the rest of the job's tasks that were not completed yet will be canceled. For example: $ hq submit --array 1 -1000 --max-fails 5 ... This will create a task array with 1000 tasks. Once 5 or more tasks fail, the remaining uncompleted tasks of the job will be canceled.","title":"Task array failures"},{"location":"jobs/jobfile/","text":"Job Definition File # Job Definition File (JDF) a way how to submit a complex pipeline into a HyperQueue. It is a TOML file that describes tasks of a job. JDF provides all functionalities as command line interface of HyperQueue and also adds access to additional features: Heterogeneous tasks -- Job may be composed of different tasks Dependencies -- Tasks may have dependencies Resource request alternatives -- Task may have alternative resource requests, e.g.: 4 cpus OR 1 cpus and 1 gpu Note that these features are also available through Python interface. Minimal example # First, we create file with the following content: [[task]] command = [ \"sleep\" , \"1\" ] Let us assume that we have named this file as myfile.toml , then we can run the following command to submit a job: $ hq job submit-file myfile.toml The effect will be same as running: $ hq submit sleep 1 Task configuration # The following shows how job and task may be configured in more detail. All options except command are optional. If not said otherwise, an option in format xxx = ... is an equivalent of --xxx = ... in hq submit command. The default are the same as CLI interface. name = \"test-job\" stream_log = \"output.log\" # Stdout/Stderr streaming (see --log) max_fails = 11 [[task]] stdout = \"testout-%{TASK_ID} stderr = { path = \" testerr- % { TASK_ID } \", mode = \" rm-if-finished \" } task_dir = true time_limit = \" 1 m 10 s \" priority = -1 crash_limit = 12 command = [\" / bin / bash \", \" - c \", \" echo $ ABC \"] # Environment variables env = {\" ABC \" = \" 123 \", \" XYZ \" = \" aaaa \"} # Content that will be written on stdin stdin = \" Hello world ! \" [[task.request]] resources = { \" cpus \" = \" 4 compact ! \", \" gpus \" = 2 } time_request = \" 10 s \" More tasks # More tasks with different configuration may be defined as follows: [[task]] command = [ \"sleep\" , \"1\" ] [[task]] command = [ \"sleep\" , \"2\" ] [[task]] command = [ \"sleep\" , \"3\" ] In the case above, tasks are given automatic task ids from id 0. You can also specify IDs manually: [[task]] id = 10 command = [ \"sleep\" , \"1\" ] [[task]] id = 11 command = [ \"sleep\" , \"2\" ] [[task]] id = 2 command = [ \"sleep\" , \"3\" ] Task arrays # If you want to create uniform tasks you can define task array (similar to --array ): [[array]] ids = \"1,2,50-100\" command = [ \"sleep\" , \"1\" ] You can also specify array with content of HQ_ENTRIES : [[array]] entries = [ \"One\" , \"Two\" , \"Three\" ] command = [ \"sleep\" , \"1\" ] Note Options entries and ids can be used together. Task dependencies # Job Definition File allows to define a dependencies between tasks. In other words, it means that the task may be executed only if the previous tasks are already finished. The task's option deps defines on which tasks the given task dependents. The task is addressed by their IDs. The following example creates three tasks where the third task depends on the first two tasks. [[task]] id = 1 command = [...] [[task]] id = 3 command = [...] [[task]] id = 5 command = [...] deps = [ 1 , 3 ] # <---- Dependancy on tasks 1 and 3 Resource variants # More resource configurations may be defined for a task. In this case, HyperQueue will take into account all these configurations during scheduling. When a task is started exactly one configuration is chosen. If in a given moment more configuration are possible for a given task, the configuration first defined has a higher priority. The following configuration defines that a task may be executed on 1 cpus and 1 gpu OR on 4 cpus. [[task]] command = [...] [[task.request]] resources = { \"cpus\" = 1 , \"gpus\" = 1 } [[task.request]] resources = { \"cpus\" = 4 } In the case that many tasks with such a configuration are submitted to a worker with 16 cpus and 4 gpus then HyperQueue will run simultaneously 4 tasks in the first configuration and 3 tasks in the second one. For a task with resource variants, HyperQueue sets variable HQ_RESOURCE_VARIANT to an index of chosen variant (counted from 0) when a task is started.","title":"Job Definition File"},{"location":"jobs/jobfile/#job-definition-file","text":"Job Definition File (JDF) a way how to submit a complex pipeline into a HyperQueue. It is a TOML file that describes tasks of a job. JDF provides all functionalities as command line interface of HyperQueue and also adds access to additional features: Heterogeneous tasks -- Job may be composed of different tasks Dependencies -- Tasks may have dependencies Resource request alternatives -- Task may have alternative resource requests, e.g.: 4 cpus OR 1 cpus and 1 gpu Note that these features are also available through Python interface.","title":"Job Definition File"},{"location":"jobs/jobfile/#minimal-example","text":"First, we create file with the following content: [[task]] command = [ \"sleep\" , \"1\" ] Let us assume that we have named this file as myfile.toml , then we can run the following command to submit a job: $ hq job submit-file myfile.toml The effect will be same as running: $ hq submit sleep 1","title":"Minimal example"},{"location":"jobs/jobfile/#task-configuration","text":"The following shows how job and task may be configured in more detail. All options except command are optional. If not said otherwise, an option in format xxx = ... is an equivalent of --xxx = ... in hq submit command. The default are the same as CLI interface. name = \"test-job\" stream_log = \"output.log\" # Stdout/Stderr streaming (see --log) max_fails = 11 [[task]] stdout = \"testout-%{TASK_ID} stderr = { path = \" testerr- % { TASK_ID } \", mode = \" rm-if-finished \" } task_dir = true time_limit = \" 1 m 10 s \" priority = -1 crash_limit = 12 command = [\" / bin / bash \", \" - c \", \" echo $ ABC \"] # Environment variables env = {\" ABC \" = \" 123 \", \" XYZ \" = \" aaaa \"} # Content that will be written on stdin stdin = \" Hello world ! \" [[task.request]] resources = { \" cpus \" = \" 4 compact ! \", \" gpus \" = 2 } time_request = \" 10 s \"","title":"Task configuration"},{"location":"jobs/jobfile/#more-tasks","text":"More tasks with different configuration may be defined as follows: [[task]] command = [ \"sleep\" , \"1\" ] [[task]] command = [ \"sleep\" , \"2\" ] [[task]] command = [ \"sleep\" , \"3\" ] In the case above, tasks are given automatic task ids from id 0. You can also specify IDs manually: [[task]] id = 10 command = [ \"sleep\" , \"1\" ] [[task]] id = 11 command = [ \"sleep\" , \"2\" ] [[task]] id = 2 command = [ \"sleep\" , \"3\" ]","title":"More tasks"},{"location":"jobs/jobfile/#task-arrays","text":"If you want to create uniform tasks you can define task array (similar to --array ): [[array]] ids = \"1,2,50-100\" command = [ \"sleep\" , \"1\" ] You can also specify array with content of HQ_ENTRIES : [[array]] entries = [ \"One\" , \"Two\" , \"Three\" ] command = [ \"sleep\" , \"1\" ] Note Options entries and ids can be used together.","title":"Task arrays"},{"location":"jobs/jobfile/#task-dependencies","text":"Job Definition File allows to define a dependencies between tasks. In other words, it means that the task may be executed only if the previous tasks are already finished. The task's option deps defines on which tasks the given task dependents. The task is addressed by their IDs. The following example creates three tasks where the third task depends on the first two tasks. [[task]] id = 1 command = [...] [[task]] id = 3 command = [...] [[task]] id = 5 command = [...] deps = [ 1 , 3 ] # <---- Dependancy on tasks 1 and 3","title":"Task dependencies"},{"location":"jobs/jobfile/#resource-variants","text":"More resource configurations may be defined for a task. In this case, HyperQueue will take into account all these configurations during scheduling. When a task is started exactly one configuration is chosen. If in a given moment more configuration are possible for a given task, the configuration first defined has a higher priority. The following configuration defines that a task may be executed on 1 cpus and 1 gpu OR on 4 cpus. [[task]] command = [...] [[task.request]] resources = { \"cpus\" = 1 , \"gpus\" = 1 } [[task.request]] resources = { \"cpus\" = 4 } In the case that many tasks with such a configuration are submitted to a worker with 16 cpus and 4 gpus then HyperQueue will run simultaneously 4 tasks in the first configuration and 3 tasks in the second one. For a task with resource variants, HyperQueue sets variable HQ_RESOURCE_VARIANT to an index of chosen variant (counted from 0) when a task is started.","title":"Resource variants"},{"location":"jobs/jobs/","text":"The main unit of computation within HyperQueue is called a Task . It represents a single computation (currently, a single execution of some program) that is scheduled and executed on a worker. To actually compute something, you have to create a Job , which is a collection of tasks (a task graph). Jobs are units of computation management - you can submit, query or cancel jobs using the CLI. Note This section focuses on simple jobs , where each job contains exactly one task. See Task arrays to find out how to create jobs with multiple tasks. Identification numbers # Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 . Submitting jobs # To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below. Name # Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ... Working directory # By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path. Output # By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are %{CWD}/job-%{JOB_ID}/%{TASK_ID}.stdout for stdout %{CWD}/job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder . Environment variables # You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id HQ_RESOURCE_... A set of variables related to allocated resources Time management # You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes. Priority # You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 . Placeholders # You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by Available for %{JOB_ID} Job ID stdout , stderr , cwd , log %{TASK_ID} Task ID stdout , stderr , cwd %{INSTANCE_ID} Instance ID stdout , stderr , cwd %{SUBMIT_DIR} Directory from which the job was submitted. stdout , stderr , cwd , log %{CWD} Working directory of the task. stdout , stderr %{SERVER_UID} Server unique ID (a string of length 6)[^uid] stdout , stderr , cwd , log [^uid] Server generates a random SERVER_UID string every time a new server is started ( hq server start ). State # At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following command 2 : $ hq job info <job-id> Task state # Each task starts in the Waiting state and can end up in one of the terminal states: Finished , Failed or Canceled . Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Waiting The task was submitted and is now waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed . Job state # The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Failed , then job state is Failed . If at least one task is Canceled , then job state is Canceled . All tasks have to be Finished , therefore the job state will also be Finished . Cancelling jobs # You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq job cancel command 2 : $ hq job cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed. Forgetting jobs # If you want to completely forget a job, and thus free up its associated memory, you can do that using the hq job forget command 2 : $ hq job forget <job-selector> By default, all completed jobs (finished/failed/canceled) will be forgotten. You can use the --status parameter to only forget jobs in certain statuses: $ hq job forget all --status finished,canceled However, only jobs that are completed, i.e. that have been finished successfully, failed or have been canceled, can be forgotten. If you want to forget a waiting or a running job, cancel it first. Waiting for jobs # There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq job wait command that can be used to wait until an existing job completes 2 : $ hq job wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq job progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq job progress <selector> Attaching standard input # When --stdin flag is used, HQ captures standard input and attaches it to each task of a job. When a task is started then the attached data is written into the standard input of the task. This can be used to submitting scripts without creating file. The following command will capture stdin and executes it in Bash $ hq submit --stdin bash If you want to parse #HQ directives from standard input, you can use --directives=stdin . Task directory # When a job is submitted with --task-dir then a temporary directory is created for each task and passed via environment variable HQ_TASK_DIR . This directory is automatically deleted when the task is completed (for any reason). Providing own error message # A task may pass its own error message into the HyperQueue. HyperQueue provides a filename via environment variable HQ_ERROR_FILENAME , if a task creates this file and terminates with a non-zero return code, then the content of this file is taken as an error message. HQ_ERROR_FILENAME is provided only if task directory is set on. The filename is always placed inside the task directory. If the message is longer than 2KiB, then it is truncated to 2KiB. If task terminates with zero return code, then the error file is ignored. Automatic file cleanup # If you create a lot of tasks and do not use output streaming , a lot of stdout / stderr files can be created on the disk. In certain cases, you might not be interested in the contents of these files, especially if the task has finished successfully, and you instead want to remove them as soon as they are not needed. For that, you can use a file cleanup mode when specifying stdout and/or stderr to choose what should happen with the file when its task finishes. The mode is specified as a name following a colon ( : ) after the file path. Currently, one cleanup mode is implemented: Remove the file if the task has finished successfully: $ hq submit --stdout = \"out.txt:rm-if-finished\" /my-program The file will not be deleted if the task fails or is cancelled. Note If you want to use the default stdout / stderr file path (and you don't want to look it up), you can also specify just the cleanup mode without the file path: $ hq submit --stdout = \":rm-if-finished\" /my-program Useful job commands # Here is a list of useful job commands: Display job table # List queued and running jobs List all jobs List jobs by status $ hq job list $ hq job list --all You can display only jobs having the selected states by using the --filter flag: $ hq job list --filter running,waiting Valid filter values are: waiting running finished failed canceled Display a summary table of all jobs # $ hq job summary Display information about a specific job # $ hq job info <job-selector> Display information about individual tasks (potentially across multiple jobs) # $ hq task list <job-selector> [--task-status <status>] [--tasks <task-selector>] Display job stdout / stderr # $ hq job cat <job-id> [--tasks <task-selector>] <stdout/stderr> Crashing limit # When a worker is lost then all running tasks on the worker are suspicious that they may cause the crash of the worker. HyperQueue server remembers how many times were a task running while a worker is lost. If the count reaches the limit, then the task is set to the failed state. By default, this limit is 5 but it can be changed as follows: $ hq submit --crash-limit=<NEWLIMIT> ... If the limit is set to 0, then the limit is disabled. You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Jobs and Tasks"},{"location":"jobs/jobs/#identification-numbers","text":"Each job is identified by a positive integer that is assigned by the HyperQueue server when the job is submitted. We refer to it as Job id . Each task within a job is identified by an unsigned 32b integer called Task id . Task id is either generated by the server or assigned by the user. Task ids are always relative to a specific job, two tasks inside different jobs can thus have the same task id. In simple jobs, task id is always set to 0 .","title":"Identification numbers"},{"location":"jobs/jobs/#submitting-jobs","text":"To submit a simple job that will execute some executable with the provided arguments, use the hq submit command: $ hq submit <program> <arg1> <arg2> ... When you submit a job, the server will assign it a unique job id and print it. You can use this ID in following commands to refer to the submitted job. After the job is submitted, HyperQueue will distribute it to a connected worker that will then execute the provided command. Warning The provided command will be executed on a worker that might be running on a different machine. You should thus make sure that the binary will be available there and that you provide an absolute path to it. Note When your command contains its own command line flags, you must put the command and its flags after -- : $ hq submit -- /bin/bash -c 'echo $PPID' There are many parameters that you can set for the executed program, they are listed below.","title":"Submitting jobs"},{"location":"jobs/jobs/#name","text":"Each job has an assigned name. It has only an informative character for the user. By default, the name is derived from the job's program name. You can also set the job name explicitly with the --name option: $ hq submit --name = <NAME> ...","title":"Name"},{"location":"jobs/jobs/#working-directory","text":"By default, the working directory of the job will be set to the directory from which the job was submitted. You can change this using the --cwd option: $ hq submit --cwd = <path> ... Warning Make sure that the provided path exists on all worker nodes. Hint You can use placeholders in the working directory path.","title":"Working directory"},{"location":"jobs/jobs/#output","text":"By default, each job will produce two files containing the standard output and standard error output, respectively. The default paths of these files are %{CWD}/job-%{JOB_ID}/%{TASK_ID}.stdout for stdout %{CWD}/job-%{JOB_ID}/%{TASK_ID}.stderr for stderr %{JOB_ID} and %{TASK_ID} are so-called placeholders, you can read about them below . You can change these paths with the --stdout and --stderr options. You can also avoid creating stdout / stderr files completely by setting the value to none : Change output paths Disable stdout $ hq submit --stdout = out.txt --stderr = err.txt ... $ hq submit --stdout = none ... Warning Make sure that the provided path(s) exist on all worker nodes. Also note that if you provide a relative path, it will be resolved relative to the directory from where you submit the job, not relative to the working directory of the job. If you want to change that, use the %{CWD} placeholder .","title":"Output"},{"location":"jobs/jobs/#environment-variables","text":"You can set environment variables which will be passed to the provided command when the job is executed using the --env <KEY>=<VAL> option. Multiple environment variables can be passed if you repeat the option. $ hq submit --env KEY1 = VAL1 --env KEY2 = VAL2 ... Each executed task will also automatically receive the following environment variables: Variable name Explanation HQ_JOB_ID Job id HQ_TASK_ID Task id HQ_INSTANCE_ID Instance id HQ_RESOURCE_... A set of variables related to allocated resources","title":"Environment variables"},{"location":"jobs/jobs/#time-management","text":"You can specify two time-related parameters when submitting a job. They will be applied to each task of the submitted job. Time Limit is the maximal running time of a task. If it is reached, the task will be terminated, and it will transition into the Failed state . This setting has no impact on scheduling. This can serve as a sanity check to make sure that some task will not run indefinitely. You can set it with the --time-limit option 1 : $ hq submit --time-limit = <duration> ... Note Time limit is counted separately for each task. If you set a time limit of 3 minutes and create two tasks, where each will run for two minutes, the time limit will not be hit. Time Request is the minimal remaining lifetime that a worker must have in order to start executing the task. Workers that do not have enough remaining lifetime will not be considered for running this task. Time requests are only used during scheduling, where the server decides which worker should execute which task. Once a task is scheduled and starts executing on a worker, the time request value will not have any effect. You can set the time request using the --time-request option 1 : $ hq submit --time-request = <duration> ... Note Workers with an unknown remaining lifetime will be able to execute any task, disregarding its time request. Here is an example situation where time limit and time request can be used: Let's assume that we have a collection of tasks where the vast majority of tasks usually finish within 10 minutes, but some of them run for (at most) 30 minutes. We do not know in advance which tasks will be \"slow\". In this case we may want to set the time limit to 35 minutes to protect us against an error (deadlock, endless loop, etc.). However, since we know that each task will usually take at least 10 minutes to execute, we don't want to start executing it on a worker if we know that the worker will definitely terminate in less than 10 minutes. It would only cause unnecessary lost computational resources. Therefore, we can set the time request to 10 minutes.","title":"Time management"},{"location":"jobs/jobs/#priority","text":"You can modify the order in which tasks are executed using Priority . Priority can be any 32b signed integer. A lower number signifies lower priority, e.g. when task A with priority 5 and task B with priority 3 are scheduled to the same worker and only one of them may be executed, then A will be executed first. You can set the priority using the --priority option: $hq submit --priority = <PRIORITY> If no priority is specified, then each task will have priority 0 .","title":"Priority"},{"location":"jobs/jobs/#placeholders","text":"You can use special variables when setting certain job parameters ( working directory , output paths, log path). These variables, called Placeholders , will be replaced by job or task-specific information before the job is executed. Placeholders are enclosed in curly braces ( {} ) and prefixed with a percent ( % ) sign. You can use the following placeholders: Placeholder Will be replaced by Available for %{JOB_ID} Job ID stdout , stderr , cwd , log %{TASK_ID} Task ID stdout , stderr , cwd %{INSTANCE_ID} Instance ID stdout , stderr , cwd %{SUBMIT_DIR} Directory from which the job was submitted. stdout , stderr , cwd , log %{CWD} Working directory of the task. stdout , stderr %{SERVER_UID} Server unique ID (a string of length 6)[^uid] stdout , stderr , cwd , log [^uid] Server generates a random SERVER_UID string every time a new server is started ( hq server start ).","title":"Placeholders"},{"location":"jobs/jobs/#state","text":"At any moment in time, each task and job has a specific state that represents what is currently happening to it. You can query the state of a job with the following command 2 : $ hq job info <job-id>","title":"State"},{"location":"jobs/jobs/#task-state","text":"Each task starts in the Waiting state and can end up in one of the terminal states: Finished , Failed or Canceled . Waiting-----------------\\ | ^ | | | | v | | Running-----------------| | | | | \\--------\\ | | | | v v v Finished Failed Canceled Waiting The task was submitted and is now waiting to be executed. Running The task is running on a worker. It may become Waiting again when the worker where the task is running crashes. Finished The task has successfully finished. Failed The task has failed. Canceled The task has been canceled . If a task is in the Finished , Failed or Canceled state, it is completed .","title":"Task state"},{"location":"jobs/jobs/#job-state","text":"The state of a job is derived from the states of its individual tasks. The state is determined by the first rule that matches from the following list of rules: If at least one task is Running , then job state is Running . If at least one task has not been completed yet, then job state is Waiting . If at least one task is Failed , then job state is Failed . If at least one task is Canceled , then job state is Canceled . All tasks have to be Finished , therefore the job state will also be Finished .","title":"Job state"},{"location":"jobs/jobs/#cancelling-jobs","text":"You can prematurely terminate a submitted job that haven't been completed yet by cancelling it using the hq job cancel command 2 : $ hq job cancel <job-selector> Cancelling a job will cancel all of its tasks that are not yet completed.","title":"Cancelling jobs"},{"location":"jobs/jobs/#forgetting-jobs","text":"If you want to completely forget a job, and thus free up its associated memory, you can do that using the hq job forget command 2 : $ hq job forget <job-selector> By default, all completed jobs (finished/failed/canceled) will be forgotten. You can use the --status parameter to only forget jobs in certain statuses: $ hq job forget all --status finished,canceled However, only jobs that are completed, i.e. that have been finished successfully, failed or have been canceled, can be forgotten. If you want to forget a waiting or a running job, cancel it first.","title":"Forgetting jobs"},{"location":"jobs/jobs/#waiting-for-jobs","text":"There are three ways of waiting until a job completes: Submit and wait You can use the --wait flag when submitting a job. This will cause the submission command to wait until the job becomes complete: $ hq submit --wait ... Tip This method can be used for benchmarking the job duration. Wait command There is a separate hq job wait command that can be used to wait until an existing job completes 2 : $ hq job wait <job-selector> Interactive wait If you want to interactively observe the status of a job (which is useful especially if it has multiple tasks ), you can use the hq job progress command: Submit and observe Observe an existing job 2 $ hq submit --progress ... $ hq job progress <selector>","title":"Waiting for jobs"},{"location":"jobs/jobs/#attaching-standard-input","text":"When --stdin flag is used, HQ captures standard input and attaches it to each task of a job. When a task is started then the attached data is written into the standard input of the task. This can be used to submitting scripts without creating file. The following command will capture stdin and executes it in Bash $ hq submit --stdin bash If you want to parse #HQ directives from standard input, you can use --directives=stdin .","title":"Attaching standard input"},{"location":"jobs/jobs/#task-directory","text":"When a job is submitted with --task-dir then a temporary directory is created for each task and passed via environment variable HQ_TASK_DIR . This directory is automatically deleted when the task is completed (for any reason).","title":"Task directory"},{"location":"jobs/jobs/#providing-own-error-message","text":"A task may pass its own error message into the HyperQueue. HyperQueue provides a filename via environment variable HQ_ERROR_FILENAME , if a task creates this file and terminates with a non-zero return code, then the content of this file is taken as an error message. HQ_ERROR_FILENAME is provided only if task directory is set on. The filename is always placed inside the task directory. If the message is longer than 2KiB, then it is truncated to 2KiB. If task terminates with zero return code, then the error file is ignored.","title":"Providing own error message"},{"location":"jobs/jobs/#automatic-file-cleanup","text":"If you create a lot of tasks and do not use output streaming , a lot of stdout / stderr files can be created on the disk. In certain cases, you might not be interested in the contents of these files, especially if the task has finished successfully, and you instead want to remove them as soon as they are not needed. For that, you can use a file cleanup mode when specifying stdout and/or stderr to choose what should happen with the file when its task finishes. The mode is specified as a name following a colon ( : ) after the file path. Currently, one cleanup mode is implemented: Remove the file if the task has finished successfully: $ hq submit --stdout = \"out.txt:rm-if-finished\" /my-program The file will not be deleted if the task fails or is cancelled. Note If you want to use the default stdout / stderr file path (and you don't want to look it up), you can also specify just the cleanup mode without the file path: $ hq submit --stdout = \":rm-if-finished\" /my-program","title":"Automatic file cleanup"},{"location":"jobs/jobs/#useful-job-commands","text":"Here is a list of useful job commands:","title":"Useful job commands"},{"location":"jobs/jobs/#display-job-table","text":"List queued and running jobs List all jobs List jobs by status $ hq job list $ hq job list --all You can display only jobs having the selected states by using the --filter flag: $ hq job list --filter running,waiting Valid filter values are: waiting running finished failed canceled","title":"Display job table"},{"location":"jobs/jobs/#display-a-summary-table-of-all-jobs","text":"$ hq job summary","title":"Display a summary table of all jobs"},{"location":"jobs/jobs/#display-information-about-a-specific-job","text":"$ hq job info <job-selector>","title":"Display information about a specific job"},{"location":"jobs/jobs/#display-information-about-individual-tasks-potentially-across-multiple-jobs","text":"$ hq task list <job-selector> [--task-status <status>] [--tasks <task-selector>]","title":"Display information about individual tasks (potentially across multiple jobs)"},{"location":"jobs/jobs/#display-job-stdoutstderr","text":"$ hq job cat <job-id> [--tasks <task-selector>] <stdout/stderr>","title":"Display job stdout/stderr"},{"location":"jobs/jobs/#crashing-limit","text":"When a worker is lost then all running tasks on the worker are suspicious that they may cause the crash of the worker. HyperQueue server remembers how many times were a task running while a worker is lost. If the count reaches the limit, then the task is set to the failed state. By default, this limit is 5 but it can be changed as follows: $ hq submit --crash-limit=<NEWLIMIT> ... If the limit is set to 0, then the limit is disabled. You can use various shortcuts for the duration value. \u21a9 \u21a9 You can use various shortcuts to select multiple jobs at once. \u21a9 \u21a9 \u21a9 \u21a9 \u21a9","title":"Crashing limit"},{"location":"jobs/multinode/","text":"Warning Multi-node support is now in the experimental stage. The core functionality is working, but some features may be limited and quality of scheduling may vary. Also auto allocation feature is not yet fully prepared for multi-node tasks. Multi-node tasks are tasks that spreads across multiple nodes. Each node reserved for such task is exclusively reserved, i.e. no other tasks may run on such nodes. A job with multi-node task can be specified by --nodes=X option. An example of a job with multi-node task asking for 4 nodes: $ hq submit --nodes 4 test.sh When the task is started, four nodes are assigned to this task. One of them is chosen as \"root\" node where test.sh is started. Hostnames of all assigned nodes can be found in file which path is in environmental variable HQ_NODE_FILE . Each line is now host name. The first line is always the root node. Note: Multi-node tasks always enables task directory ( --task-dir ). Groups # A multi-node task is started only on workers that belong to the same group. By default, workers are grouped by PBS/Slurm allocations and workers outside any allocation are put in \"default\" group. A group of a worker can be specified at the start of the worker and it may be any string. Example: $ hq worker start --group my_group Running MPI tasks # A script that starts an MPI program in multi-node task may look like as follows: #!/bin/sh mpirun --node-list = $HQ_NODE_FILE ./a-program","title":"Multinode Tasks"},{"location":"jobs/multinode/#groups","text":"A multi-node task is started only on workers that belong to the same group. By default, workers are grouped by PBS/Slurm allocations and workers outside any allocation are put in \"default\" group. A group of a worker can be specified at the start of the worker and it may be any string. Example: $ hq worker start --group my_group","title":"Groups"},{"location":"jobs/multinode/#running-mpi-tasks","text":"A script that starts an MPI program in multi-node task may look like as follows: #!/bin/sh mpirun --node-list = $HQ_NODE_FILE ./a-program","title":"Running MPI tasks"},{"location":"jobs/resources/","text":"Resource management # Resource management serves for defining arbitrary resources provided by workers and also corresponding resource requests required by tasks. HyperQueue will take care of matching task resource requests so that only workers that can fulfill them will be able to execute such tasks. Some generic resources are automatically detected ; however, users may also define their own resources. From version 0.13.0, CPUs are also managed as other resources, but they have still some extra functionality; therefore, there is a special section about CPU resources . Important Resources in HyperQueue exist on a purely logical level. They can correspond to physical things (like GPUs), but it is the responsibility of the user to make sure that this correspondence makes sense. With exception of CPUs, HyperQueue by itself does not attach any semantics to resources, they are just numbers used for scheduling. Worker resources # Each worker has one or mores resources attached. Each resource is a resource pool identified by a name. A resource pool represents some resources provided by a worker; each task can then ask for a part of the resources contained in that pool. There are two kinds of resource pools: Indexed pool : This pool represents an enumerated set of resources represented by strings. Each resource has its own identity. Tasks do not ask for specific values from the set, they just specify how many resources they require and HyperQueue will allocate the specified amount of resources from the pool for each task. This pool is useful for resources that have their own identity, for example individual GPU or FPGA accelerators. HyperQueue guarantees that no individual resource from the indexed pool is allocated to more than a single task at any given time and that a task will not be executed on a worker if it does not currently have enough individual resources to fulfill the resource request of the task. Indexed pool can be defined with groups where indices live in separated groups. Task may then ask for different allocation policies (e.g. use resources from the same or different groups). The main purpose of this is to capture NUMA architectures, each group then represents a socket with cores. Sum pool : This pool represents a resource that has a certain size which is split into individual tasks. A typical example is memory; if a worker has 2000 bytes of memory, it can serve e.g. four tasks, if each task asks for 500 bytes of memory. HyperQueue guarantees that the sum of resource request sizes of running tasks on a worker does not exceed the total size of the sum pool. Specifying worker resources # You can specify the resource pools of a worker when you start it: $ hq worker start --resource \"<NAME1>=<DEF1>\" --resource \"<NAME2>=<DEF2>\" ... where NAMEi is a name (string ) of the i -th resource pool and DEFi is a definition of the i-th resource pool. You can define resource pools using one of the following formats: [<VALUE>, <VALUE>, ..., <VALUE>] where VALUE is a string. This defines a an indexed pool with the given values. If you need to enter a string resource that contains special characters ( [ , ] , , , whitespace), you can wrap the value in quotes: [\"foo [,]\", bar, \"my resource\"] . range(<START>-<END>) where START and END are non-negative integers. This defines an indexed pool with numbers in the inclusive range [START, END] . [[<VALUE>, ..., <VALUE>], [<VALUE>, ..., <VALUE>], ...] where VALUE is a string. This defines an indexed pool where indices are grouped. <N>x<M> Creates indexed pool with N groups of size M, indices are indexed from 0, (e.g. \"2x3\" is equivalent to [[0, 1, 2], [3, 4, 5] ) sum(<SIZE>) where SIZE is a positive integer. This defines a sum pool with the given size. Tip You might encounter a problem in your shell when you try to specify worker resources, because the definition contains parentheses ( () ). In that case just wrap the resource definition in quotes, like this: $ hq worker start --resource \"foo=sum(5)\" Resource names # Resource names are restricted by the following rules: They can only contain ASCII letters and digits ( a-z , A-Z , 0-9 ) and the slash ( / ) symbol. They need to begin with an ASCII letter. These restrictions exist because the resource names are passed as environment variable names to tasks, which often execute shell scripts. However, shells typically do not support environment variables containing anything else than ASCII letters, digits and the underscore symbol. Therefore, HQ limits resource naming to align with the behaviour of the shell. Important HQ will normalize the resource name when passing environment variables to a task (see below ). Automatically detected resources # The following resources are detected automatically if a resource of a given name is not explicitly defined. CPUs are automatically detected as resource named \"cpus\" (more in CPU resources ). GPUs that are available when a worker is started are automatically detected under the following resource names: NVIDIA GPUs are stored the under resource name gpus/nvidia . These GPUs are detected from the environment variable CUDA_VISIBLE_DEVICES or from the procfs filesystem. AMD GPUs are stored under the resource name gpus/amd . These GPUs are detected from the environment variable ROCR_VISIBLE_DEVICES . You can set these environment variables when starting a worker to override the list of available GPUs: $ CUDA_VISIBLE_DEVICES = 2 ,3 hq worker start # The worker will have resource gpus/nvidia=[2,3] RAM of the node is detected as resource \"mem\" in bytes. If you want to see how is your system seen by a worker without actually starting it, you can start: $ hq worker hwdetect The automatic detection of resources can be disabled by argument --no-detect-resources in hq worker start ... . It disables detection of resources other than \"cpus\"; if resource \"cpus\" are not explicitly defined, it will always be detected. Resource request # When you submit a job, you can define a resource requests with the --resource flag: $ hq submit --resource <NAME1> = <AMOUNT1> --resource <NAME2> = <AMOUNT2> ... Where NAME is a name of the requested resource and the AMOUNT is a positive integer defining the size of the request. Tasks with such resource requests will only be executed on workers that fulfill all the specified task requests. Important Notice that task resource requests always ask for an amount of resources required by a task, regardless whether that resource corresponds to an indexed or a sum pool on workers. For example, let's say that a worker has an indexed pool of GPUs: $ hq worker start --resource \"gpus/nvidia=range(1-3)\" And we create two jobs, each with a single task. The first job wants 1 GPU, the second one wants two GPUs. $ hq submit --resource gpus/nvidia = 1 ... $ hq submit --resource gpus/nvidia = 2 ... Then the first job can be allocated e.g. the GPU 2 and the second job can be allocated the GPUs 1 and 3 . Requesting all resources # A task may ask for all given resources of that type by specifying --resource <NAME>=all . Such a task will be scheduled only on a worker that has at least 1 of such resource and when a task is executed all resources of that type will be given to this task. Resource request strategies # When resource request is defined, after the amount you can define allocation strategy: --resource <NAME>=\"<AMOUNT> <STRATEGY>\" . Specifying strategy has effect only if worker provides indexed resource in groups. If resource is other type, then strategy is ignored. When strategy is not defined then compact is used as default. Compact ( compact ) - Tries to allocate indices in few groups as possible in the current worker state. Example: $ hq submit --resource cpus = \"8 compact\" ... Strict Compact ( compact! ) - Always allocate indices on as few groups as possible for a target node. The task is not executed until the requirement could not be fully fulfilled. E.g. If a worker has 4 indices per a group and you ask for 4 indices in the strict compact mode, it will always be executed with indices from a single group. If you ask for 8 cpus in the same way, it will always be executed with indices from two groups. Example: $ hq submit --resource cpus = \"8 compact!\" ... ` Scatter ( scatter ) - Allocate indices across as many groups as possible in the current worker state. E.g. Let us assume that a worker has 4 groups with 8 indices per group, and you ask for 8 cpus in the scatter mode. If possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: $ hq submit --resource cpus = \"8 scatter\" ... Resource environment variables # When a task that has resource requests is executed, the following variables are passed to it for each resource request named <NAME> : HQ_RESOURCE_REQUEST_<NAME> contains the amount of requested resources. HQ_RESOURCE_VALUES_<NAME> contains the specific resource values allocated for the task as a comma-separated list. This variable is only filled for indexed resource pool. The slash symbol ( / ) in resource name is normalized to underscore ( _ ) when being used in the environment variable name. HQ also sets additional environment variables for various resources with special names: For the resource gpus/nvidia , HQ will set: CUDA_VISIBLE_DEVICES to the same value as HQ_RESOURCE_VALUES_gpus_nvidia CUDA_DEVICE_ORDER to PCI_BUS_ID For the resource gpus/amd , HQ will set: ROCR_VISIBLE_DEVICES to the same value as HQ_RESOURCE_VALUES_gpus_amd Resource requests and job arrays # Resource requests are applied to each task of job. For example, if you submit the following: $ hq submit --cpus = 2 --array = 1 -10 then each task will require two cores. Resource variants # A task may have attached more resource requests. There is no command line interface for this feature, but it can be configured through a Job Definition File .","title":"Resources"},{"location":"jobs/resources/#resource-management","text":"Resource management serves for defining arbitrary resources provided by workers and also corresponding resource requests required by tasks. HyperQueue will take care of matching task resource requests so that only workers that can fulfill them will be able to execute such tasks. Some generic resources are automatically detected ; however, users may also define their own resources. From version 0.13.0, CPUs are also managed as other resources, but they have still some extra functionality; therefore, there is a special section about CPU resources . Important Resources in HyperQueue exist on a purely logical level. They can correspond to physical things (like GPUs), but it is the responsibility of the user to make sure that this correspondence makes sense. With exception of CPUs, HyperQueue by itself does not attach any semantics to resources, they are just numbers used for scheduling.","title":"Resource management"},{"location":"jobs/resources/#worker-resources","text":"Each worker has one or mores resources attached. Each resource is a resource pool identified by a name. A resource pool represents some resources provided by a worker; each task can then ask for a part of the resources contained in that pool. There are two kinds of resource pools: Indexed pool : This pool represents an enumerated set of resources represented by strings. Each resource has its own identity. Tasks do not ask for specific values from the set, they just specify how many resources they require and HyperQueue will allocate the specified amount of resources from the pool for each task. This pool is useful for resources that have their own identity, for example individual GPU or FPGA accelerators. HyperQueue guarantees that no individual resource from the indexed pool is allocated to more than a single task at any given time and that a task will not be executed on a worker if it does not currently have enough individual resources to fulfill the resource request of the task. Indexed pool can be defined with groups where indices live in separated groups. Task may then ask for different allocation policies (e.g. use resources from the same or different groups). The main purpose of this is to capture NUMA architectures, each group then represents a socket with cores. Sum pool : This pool represents a resource that has a certain size which is split into individual tasks. A typical example is memory; if a worker has 2000 bytes of memory, it can serve e.g. four tasks, if each task asks for 500 bytes of memory. HyperQueue guarantees that the sum of resource request sizes of running tasks on a worker does not exceed the total size of the sum pool.","title":"Worker resources"},{"location":"jobs/resources/#specifying-worker-resources","text":"You can specify the resource pools of a worker when you start it: $ hq worker start --resource \"<NAME1>=<DEF1>\" --resource \"<NAME2>=<DEF2>\" ... where NAMEi is a name (string ) of the i -th resource pool and DEFi is a definition of the i-th resource pool. You can define resource pools using one of the following formats: [<VALUE>, <VALUE>, ..., <VALUE>] where VALUE is a string. This defines a an indexed pool with the given values. If you need to enter a string resource that contains special characters ( [ , ] , , , whitespace), you can wrap the value in quotes: [\"foo [,]\", bar, \"my resource\"] . range(<START>-<END>) where START and END are non-negative integers. This defines an indexed pool with numbers in the inclusive range [START, END] . [[<VALUE>, ..., <VALUE>], [<VALUE>, ..., <VALUE>], ...] where VALUE is a string. This defines an indexed pool where indices are grouped. <N>x<M> Creates indexed pool with N groups of size M, indices are indexed from 0, (e.g. \"2x3\" is equivalent to [[0, 1, 2], [3, 4, 5] ) sum(<SIZE>) where SIZE is a positive integer. This defines a sum pool with the given size. Tip You might encounter a problem in your shell when you try to specify worker resources, because the definition contains parentheses ( () ). In that case just wrap the resource definition in quotes, like this: $ hq worker start --resource \"foo=sum(5)\"","title":"Specifying worker resources"},{"location":"jobs/resources/#resource-names","text":"Resource names are restricted by the following rules: They can only contain ASCII letters and digits ( a-z , A-Z , 0-9 ) and the slash ( / ) symbol. They need to begin with an ASCII letter. These restrictions exist because the resource names are passed as environment variable names to tasks, which often execute shell scripts. However, shells typically do not support environment variables containing anything else than ASCII letters, digits and the underscore symbol. Therefore, HQ limits resource naming to align with the behaviour of the shell. Important HQ will normalize the resource name when passing environment variables to a task (see below ).","title":"Resource names"},{"location":"jobs/resources/#automatically-detected-resources","text":"The following resources are detected automatically if a resource of a given name is not explicitly defined. CPUs are automatically detected as resource named \"cpus\" (more in CPU resources ). GPUs that are available when a worker is started are automatically detected under the following resource names: NVIDIA GPUs are stored the under resource name gpus/nvidia . These GPUs are detected from the environment variable CUDA_VISIBLE_DEVICES or from the procfs filesystem. AMD GPUs are stored under the resource name gpus/amd . These GPUs are detected from the environment variable ROCR_VISIBLE_DEVICES . You can set these environment variables when starting a worker to override the list of available GPUs: $ CUDA_VISIBLE_DEVICES = 2 ,3 hq worker start # The worker will have resource gpus/nvidia=[2,3] RAM of the node is detected as resource \"mem\" in bytes. If you want to see how is your system seen by a worker without actually starting it, you can start: $ hq worker hwdetect The automatic detection of resources can be disabled by argument --no-detect-resources in hq worker start ... . It disables detection of resources other than \"cpus\"; if resource \"cpus\" are not explicitly defined, it will always be detected.","title":"Automatically detected resources"},{"location":"jobs/resources/#resource-request","text":"When you submit a job, you can define a resource requests with the --resource flag: $ hq submit --resource <NAME1> = <AMOUNT1> --resource <NAME2> = <AMOUNT2> ... Where NAME is a name of the requested resource and the AMOUNT is a positive integer defining the size of the request. Tasks with such resource requests will only be executed on workers that fulfill all the specified task requests. Important Notice that task resource requests always ask for an amount of resources required by a task, regardless whether that resource corresponds to an indexed or a sum pool on workers. For example, let's say that a worker has an indexed pool of GPUs: $ hq worker start --resource \"gpus/nvidia=range(1-3)\" And we create two jobs, each with a single task. The first job wants 1 GPU, the second one wants two GPUs. $ hq submit --resource gpus/nvidia = 1 ... $ hq submit --resource gpus/nvidia = 2 ... Then the first job can be allocated e.g. the GPU 2 and the second job can be allocated the GPUs 1 and 3 .","title":"Resource request"},{"location":"jobs/resources/#requesting-all-resources","text":"A task may ask for all given resources of that type by specifying --resource <NAME>=all . Such a task will be scheduled only on a worker that has at least 1 of such resource and when a task is executed all resources of that type will be given to this task.","title":"Requesting all resources"},{"location":"jobs/resources/#resource-request-strategies","text":"When resource request is defined, after the amount you can define allocation strategy: --resource <NAME>=\"<AMOUNT> <STRATEGY>\" . Specifying strategy has effect only if worker provides indexed resource in groups. If resource is other type, then strategy is ignored. When strategy is not defined then compact is used as default. Compact ( compact ) - Tries to allocate indices in few groups as possible in the current worker state. Example: $ hq submit --resource cpus = \"8 compact\" ... Strict Compact ( compact! ) - Always allocate indices on as few groups as possible for a target node. The task is not executed until the requirement could not be fully fulfilled. E.g. If a worker has 4 indices per a group and you ask for 4 indices in the strict compact mode, it will always be executed with indices from a single group. If you ask for 8 cpus in the same way, it will always be executed with indices from two groups. Example: $ hq submit --resource cpus = \"8 compact!\" ... ` Scatter ( scatter ) - Allocate indices across as many groups as possible in the current worker state. E.g. Let us assume that a worker has 4 groups with 8 indices per group, and you ask for 8 cpus in the scatter mode. If possible in the current situation, HQ tries to run process with 2 cpus on each socket. Example: $ hq submit --resource cpus = \"8 scatter\" ...","title":"Resource request strategies"},{"location":"jobs/resources/#resource-environment-variables","text":"When a task that has resource requests is executed, the following variables are passed to it for each resource request named <NAME> : HQ_RESOURCE_REQUEST_<NAME> contains the amount of requested resources. HQ_RESOURCE_VALUES_<NAME> contains the specific resource values allocated for the task as a comma-separated list. This variable is only filled for indexed resource pool. The slash symbol ( / ) in resource name is normalized to underscore ( _ ) when being used in the environment variable name. HQ also sets additional environment variables for various resources with special names: For the resource gpus/nvidia , HQ will set: CUDA_VISIBLE_DEVICES to the same value as HQ_RESOURCE_VALUES_gpus_nvidia CUDA_DEVICE_ORDER to PCI_BUS_ID For the resource gpus/amd , HQ will set: ROCR_VISIBLE_DEVICES to the same value as HQ_RESOURCE_VALUES_gpus_amd","title":"Resource environment variables"},{"location":"jobs/resources/#resource-requests-and-job-arrays","text":"Resource requests are applied to each task of job. For example, if you submit the following: $ hq submit --cpus = 2 --array = 1 -10 then each task will require two cores.","title":"Resource requests and job arrays"},{"location":"jobs/resources/#resource-variants","text":"A task may have attached more resource requests. There is no command line interface for this feature, but it can be configured through a Job Definition File .","title":"Resource variants"},{"location":"jobs/streaming/","text":"Jobs containing many tasks will generate a large amount of stdout and stderr files, which can be problematic, especially on network-based shared filesystems, such as Lustre. For example, when you submit the following task array: $ hq submit --array = 1 -10000 my-computation.sh 20000 files ( 10000 for stdout and 10000 for stderr) will be created on the disk. To avoid this situation, HyperQueue can optionally stream the stdout and stderr output of all tasks of a job over a network to the server, which will continuously append it to a single file called the Log . Note In this section, we refer to stdout and stderr as channels . Redirecting output to the log # You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path. Partial redirection # By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ... Guarantees # HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld. Superseded streams # When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default. Current limitations # The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten. Inspecting the log file # HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args> Log summary # You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary Printing log content # If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand. Log metadata # If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand. Exporting log # Log can be exported into JSON by the following command: $ hq log <log-file-path> export This prints the log file into a JSON format on standard output.","title":"Output Streaming"},{"location":"jobs/streaming/#redirecting-output-to-the-log","text":"You can redirect the output of stdout and stderr to a log file and thus enable output streaming by passing a path to a filename where the log will be stored with the --log option: $ hq submit --log=<log-path> --array=1-10000 ... This command would cause the stdout and stderr of all 10000 tasks to be streamed into the server, which will write them to a single file specified in <log-path> . The streamed data is stored with additional metadata, which allows the resulting file to be filtered/sorted by tasks or output type ( stdout / stderr ). Tip You can use selected placeholders inside the log path.","title":"Redirecting output to the log"},{"location":"jobs/streaming/#partial-redirection","text":"By default, both stdout and stderr will be streamed if you specify --log and do not specify an explicit path for stdout and stderr . To stream only one of the channels, you can use the --stdout / --stderr options to redirect one of them to a file or to disable it completely. For example: # Redirecting stdout into a file, streaming stderr into `my-log` $ hq submit --log = my-log --stdout = \"stdout-%{TASK_ID}\" ... # Streaming stdout into `my-log`, disabling stderr $ hq submit --log = my-log --stderr = none ...","title":"Partial redirection"},{"location":"jobs/streaming/#guarantees","text":"HyperQueue provides the following guarantees regarding output streaming: When a task is Finished or Failed , then it is guaranteed* that its streamed output is fully flushed into the log file. When a task is Canceled , then its stream is not necessarily fully written into the log file at the moment it becomes canceled. Some parts of its output may be written later, but the stream will be eventually closed. When a task is Canceled or its time limit is reached, then the part of its stream that was buffered in the worker is dropped to avoid spending additional resources for this task. In practice, only output produced immediately before a task is canceled could be dropped, since output data is streamed to the server as soon as possible. * If the streaming itself failed (e.g. because there was insufficient disk space for the log file), then the task will fail with an error prefixed with \"Streamer:\" and no further streaming guarantees will be upheld.","title":"Guarantees"},{"location":"jobs/streaming/#superseded-streams","text":"When a worker crashes while executing a task, the task will be restarted . If output streaming is enabled and the task has already streamed some output data before it was restarted, invalid or duplicate output could appear in the log. To avoid mixing outputs from different executions of the same task, when a task is restarted, HyperQueue automatically marks all output streamed from previous runs of the task as superseded and ignores this output by default.","title":"Superseded streams"},{"location":"jobs/streaming/#current-limitations","text":"The current version does not support streaming the output of multiple jobs into the same file. In other words, if you submit multiple jobs with the same log filename, like this: $ hq submit --log = my-log ... $ hq submit --log = my-log ... Then the log will contain data from a single job only, the other data will be overwritten.","title":"Current limitations"},{"location":"jobs/streaming/#inspecting-the-log-file","text":"HyperQueue lets you inspect the data stored inside the log file using various subcommands. All log subcommands have the following structure: $ hq log <log-file-path> <subcommand> <subcommand-args>","title":"Inspecting the log file"},{"location":"jobs/streaming/#log-summary","text":"You can display a summary of a log file using the summary subcommand: $ hq log <log-file-path> summary","title":"Log summary"},{"location":"jobs/streaming/#printing-log-content","text":"If you want to simply print the (textual) content of the log file, without any associating metadata, you can use the cat subcommand: $ hq log <log-file-path> cat <stdout/stderr> It will print the raw content of either stdout or stderr , ordered by task id. All outputs will be concatenated one after another. You can use this to process the streamed data e.g. by a postprocessing script. By default, this command will fail if there is an unfinished stream (i.e. when some task is still running and streaming data into the log). If you want to use cat even when the log is not finished yet, use the --allow-unfinished option. If you want to see the output of a specific task, you can use the --task=<task-id> option. Note Superseded streams are completely ignored by the cat subcommand.","title":"Printing log content"},{"location":"jobs/streaming/#log-metadata","text":"If you want to inspect the contents of the log, along with its inner metadata that shows which task and which channel has produced which part of the data, you can use the show subcommand: $ hq log <log-file-path> show The output will have the form X:Y> DATA where X is task id and Y is 0 for stdout channel and 1 for stderr channel. You can filter a specific channel with the --channel=stdout/stderr flag. By default, HQ does not show stream close metadata from streams that are empty (e.g. when a task did not produce any output on some channel). You can change that with the flag --show-empty . Note Superseded streams are completely ignored by the show subcommand.","title":"Log metadata"},{"location":"jobs/streaming/#exporting-log","text":"Log can be exported into JSON by the following command: $ hq log <log-file-path> export This prints the log file into a JSON format on standard output.","title":"Exporting log"},{"location":"python/","text":"Python API # To provide greater flexibility and support use-cases that are difficult to express using the CLI such as dynamically submitting tasks when some part is finished. Python API covers all task definition including all options available through Job Definition File (dependencies between tasks, resource variants, etc) You can find the HyperQueue Python API reference here . Requirements # To use the Python API, you will need at least Python 3.6 and some dependencies that will be installed automatically using pip. Installation # You can install the HyperQueue Python API from PyPi with the following command: $ python3 -m pip install hyperqueue The Python package contains a pre-compiled version of HyperQueue, so you do not have to download hq manually if you just want to use the Python API. Warning The Python API is currently distributed only for the x86-x64 architecture. If you need a build for another architecture, please contact us on GitHub. You can also build the Python package manually from our GitHub repository, but you will need to install a Rust toolchain for that. Quick start # Here is a minimal code example that spawns a local HyperQueue cluster and uses it to submit a simple job: from hyperqueue import Job , LocalCluster # Spawn a HQ server with LocalCluster () as cluster : # Add a single HyperQueue worker to the server cluster . start_worker () # Create a client and a job client = cluster . client () job = Job () # Add a task that executes `ls` to the job job . program ([ \"ls\" ]) # Submit the job submitted = client . submit ( job ) # Wait until the job completes client . wait_for_jobs ([ submitted ])","title":"Getting started"},{"location":"python/#python-api","text":"To provide greater flexibility and support use-cases that are difficult to express using the CLI such as dynamically submitting tasks when some part is finished. Python API covers all task definition including all options available through Job Definition File (dependencies between tasks, resource variants, etc) You can find the HyperQueue Python API reference here .","title":"Python API"},{"location":"python/#requirements","text":"To use the Python API, you will need at least Python 3.6 and some dependencies that will be installed automatically using pip.","title":"Requirements"},{"location":"python/#installation","text":"You can install the HyperQueue Python API from PyPi with the following command: $ python3 -m pip install hyperqueue The Python package contains a pre-compiled version of HyperQueue, so you do not have to download hq manually if you just want to use the Python API. Warning The Python API is currently distributed only for the x86-x64 architecture. If you need a build for another architecture, please contact us on GitHub. You can also build the Python package manually from our GitHub repository, but you will need to install a Rust toolchain for that.","title":"Installation"},{"location":"python/#quick-start","text":"Here is a minimal code example that spawns a local HyperQueue cluster and uses it to submit a simple job: from hyperqueue import Job , LocalCluster # Spawn a HQ server with LocalCluster () as cluster : # Add a single HyperQueue worker to the server cluster . start_worker () # Create a client and a job client = cluster . client () job = Job () # Add a task that executes `ls` to the job job . program ([ \"ls\" ]) # Submit the job submitted = client . submit ( job ) # Wait until the job completes client . wait_for_jobs ([ submitted ])","title":"Quick start"},{"location":"python/client/","text":"Client # To submit jobs using the Python API, you first need to create a Client that connects to a running HyperQueue cluster. You have two options of deploying the cluster. Once you have an instance of a Client , you can use it to submit a job. Using external deployment # If you want to run the HyperQueue infrastructure on a distributed cluster or you want to use automatic allocation , then deploy HyperQueue in any of the supported ways and then pass the server directory to the Client : from hyperqueue import Client client = Client ( \"/home/user/.hq-server/hq-current\" ) If you have used the default server directory and the server is deployed on a file-system shared by the node that executes the Python code, you can simply create an instance of a Client without passing any parameters. Using a local cluster # You can use the LocalCluster class to spawn a HyperQueue server and a set of workers directly on your local machine. This functionality is primarily intended for local prototyping and debugging, but it can also be used for actual computations for simple use-cases that do not require a distributed deployment of HyperQueue. When you create the cluster, it will initially only start the HyperQueue server. To connect workers to it, use the start_worker method. from hyperqueue import LocalCluster from hyperqueue.cluster import WorkerConfig with LocalCluster () as cluster : # Add a worker with 4 cores to the cluster cluster . start_worker ( WorkerConfig ( cores = 4 )) # Create a client connected to the cluster client = cluster . client () Tip You can use LocalCluster instances as context managers to make sure that the cluster is properly cleaned up at the end of the with block.","title":"Client"},{"location":"python/client/#client","text":"To submit jobs using the Python API, you first need to create a Client that connects to a running HyperQueue cluster. You have two options of deploying the cluster. Once you have an instance of a Client , you can use it to submit a job.","title":"Client"},{"location":"python/client/#using-external-deployment","text":"If you want to run the HyperQueue infrastructure on a distributed cluster or you want to use automatic allocation , then deploy HyperQueue in any of the supported ways and then pass the server directory to the Client : from hyperqueue import Client client = Client ( \"/home/user/.hq-server/hq-current\" ) If you have used the default server directory and the server is deployed on a file-system shared by the node that executes the Python code, you can simply create an instance of a Client without passing any parameters.","title":"Using external deployment"},{"location":"python/client/#using-a-local-cluster","text":"You can use the LocalCluster class to spawn a HyperQueue server and a set of workers directly on your local machine. This functionality is primarily intended for local prototyping and debugging, but it can also be used for actual computations for simple use-cases that do not require a distributed deployment of HyperQueue. When you create the cluster, it will initially only start the HyperQueue server. To connect workers to it, use the start_worker method. from hyperqueue import LocalCluster from hyperqueue.cluster import WorkerConfig with LocalCluster () as cluster : # Add a worker with 4 cores to the cluster cluster . start_worker ( WorkerConfig ( cores = 4 )) # Create a client connected to the cluster client = cluster . client () Tip You can use LocalCluster instances as context managers to make sure that the cluster is properly cleaned up at the end of the with block.","title":"Using a local cluster"},{"location":"python/dependencies/","text":"Task dependencies # One of the most useful features of the HyperQueue Python API is that it allows you to define dependencies between individual tasks of a job. If a task B depends on task A , then B will not be executed until A has (successfully) finished. Using dependencies, you can describe arbitrarily complex DAG (directed acyclic graph) workflows. Notice HyperQueue jobs are independent of each other, so dependencies can only be specified between tasks within a single job. Defining dependencies # To define a dependency between tasks, you will first need to store the Task instances that you get when you create a task . You can then use the deps parameter when creating a new task and pass an existing task instance to define a dependency: from hyperqueue import Job job = Job () # Create a first task that generates data task_a = job . program ([ \"generate-data\" , \"--file\" , \"out.txt\" ]) # Create a dependent task that consumes the data job . program ([ \"consume-data\" , \"--file\" , \"out.txt\" ], deps = [ task_a ]) The second task will not be started until the first one successfully finishes. You can also depend on multiple tasks at once: # Create several tasks that generate data tasks = [ job . program ([ \"generate-data\" , \"--file\" , f \"out- { i } .txt\" ]) for i in range ( 5 )] # Create a dependent task that consumes the data job . program ([ \"consume-data\" , \"--file\" , \"out- %d .txt\" ], deps = [ tasks ]) Dependencies are transitive, so you can build an arbitrary graph: task_a = job . program ([ \"generate\" , \"1\" ]) task_b = job . program ([ \"generate\" , \"2\" ]) task_c = job . program ([ \"compute\" ], deps = [ task_a , task_b ]) task_d = job . program ([ \"postprocess\" ], deps = [ task_c ]) In this case, task D will not start until all the three previous tasks are successfully finished.","title":"Dependencies"},{"location":"python/dependencies/#task-dependencies","text":"One of the most useful features of the HyperQueue Python API is that it allows you to define dependencies between individual tasks of a job. If a task B depends on task A , then B will not be executed until A has (successfully) finished. Using dependencies, you can describe arbitrarily complex DAG (directed acyclic graph) workflows. Notice HyperQueue jobs are independent of each other, so dependencies can only be specified between tasks within a single job.","title":"Task dependencies"},{"location":"python/dependencies/#defining-dependencies","text":"To define a dependency between tasks, you will first need to store the Task instances that you get when you create a task . You can then use the deps parameter when creating a new task and pass an existing task instance to define a dependency: from hyperqueue import Job job = Job () # Create a first task that generates data task_a = job . program ([ \"generate-data\" , \"--file\" , \"out.txt\" ]) # Create a dependent task that consumes the data job . program ([ \"consume-data\" , \"--file\" , \"out.txt\" ], deps = [ task_a ]) The second task will not be started until the first one successfully finishes. You can also depend on multiple tasks at once: # Create several tasks that generate data tasks = [ job . program ([ \"generate-data\" , \"--file\" , f \"out- { i } .txt\" ]) for i in range ( 5 )] # Create a dependent task that consumes the data job . program ([ \"consume-data\" , \"--file\" , \"out- %d .txt\" ], deps = [ tasks ]) Dependencies are transitive, so you can build an arbitrary graph: task_a = job . program ([ \"generate\" , \"1\" ]) task_b = job . program ([ \"generate\" , \"2\" ]) task_c = job . program ([ \"compute\" ], deps = [ task_a , task_b ]) task_d = job . program ([ \"postprocess\" ], deps = [ task_c ]) In this case, task D will not start until all the three previous tasks are successfully finished.","title":"Defining dependencies"},{"location":"python/submit/","text":"Submitting jobs # You can use the Python API to submit jobs (directed acyclic graphs of tasks) through a Client . In addition to the functionality offered by the HyperQueue CLI, you can use the Python API to add dependencies between jobs, configure each task individually and create tasks out of Python functions . Job # To build a job, you first have to create an instance of the Job class. from hyperqueue import Job job = Job () Tasks # Once you have created a job, you can add tasks to it. Currently, each task can represent either the execution of an external program or the execution of a Python function . To create complex workflows, you can also specify dependencies between tasks. External programs # To create a task that will execute an external program, you can use the program method of a Job : job . program ([ \"/bin/my-program\" , \"foo\" , \"bar\" , \"--arg\" , \"42\" ]) You can pass the program arguments or various other parameters to the task. The program method will return a Task object that represents the created task. This object can be used further e.g. for defining dependencies . Python functions # If you want to execute a Python function as a task, you can use the function method of a Job : def preprocess_data ( fast , path ): with open ( path ) as f : data = f . read () if fast : preprocess_fast ( data ) else : preprocess ( data ) job . function ( preprocess_data , args = ( True , \"/data/a.txt\" )) job . function ( preprocess_data , args = ( False , \"/data/b.txt\" )) You can pass both positional and keyword arguments to the function. The arguments will be serialized using cloudpickle . Python tasks can be useful to perform e.g. various data preprocessing and organization tasks. You can co-locate the logic of Python tasks together with the code that defines the submitted workflow (job), without the need to write an additional external script. Same as with the program method, function will return a Task that can used to define dependencies . Notice Currently, a new Python interpreter will be started for each Python task. Python environment # When you use a Python function as a task, the task will attempt to import the hyperqueue package when it executes (to perform some bookkeeping on the background). This function will be executed on a worker - this means that it needs to have access to the correct Python version (and virtual environment) that contains the hyperqueue package! To make sure that the function will be executed in the correct Python environment, you can use PythonEnv and its prologue argument. It lets you specify a (shell) command that will be executed before the Python interpreter that executes your function is spawned. from hyperqueue.task.function import PythonEnv from hyperqueue import Client env = PythonEnv ( prologue = \"ml Python/XYZ && source /<my-path-to-venv>/bin/activate\" ) client = Client ( python_env = env ) If you use Python functions as tasks, it is pretty much required to use PythonEnv , unless your workers are already spawned in an environment that has the correct Python loaded (e.g. using .bashrc or a similar mechanism). Parametrizing tasks # You can parametrize both external or Python tasks by setting their working directory, standard output paths, environment variables or HyperQueue specific parameters like resources or time limits . In contrast to the CLI, where you can only use a single set of parameters for all tasks of a job, with the Python API you can specify these parameters individually for each task. You can find more details in the documentation of the program or function methods. Submitting a job # Once you have added some tasks to the job, you can submit it using the Client 's submit method: client = Client () submitted = client . submit ( job ) To wait until the job has finished executing, use the wait_for_jobs method: client . wait_for_jobs ([ submitted ])","title":"Submitting jobs"},{"location":"python/submit/#submitting-jobs","text":"You can use the Python API to submit jobs (directed acyclic graphs of tasks) through a Client . In addition to the functionality offered by the HyperQueue CLI, you can use the Python API to add dependencies between jobs, configure each task individually and create tasks out of Python functions .","title":"Submitting jobs"},{"location":"python/submit/#job","text":"To build a job, you first have to create an instance of the Job class. from hyperqueue import Job job = Job ()","title":"Job"},{"location":"python/submit/#tasks","text":"Once you have created a job, you can add tasks to it. Currently, each task can represent either the execution of an external program or the execution of a Python function . To create complex workflows, you can also specify dependencies between tasks.","title":"Tasks"},{"location":"python/submit/#external-programs","text":"To create a task that will execute an external program, you can use the program method of a Job : job . program ([ \"/bin/my-program\" , \"foo\" , \"bar\" , \"--arg\" , \"42\" ]) You can pass the program arguments or various other parameters to the task. The program method will return a Task object that represents the created task. This object can be used further e.g. for defining dependencies .","title":"External programs"},{"location":"python/submit/#python-functions","text":"If you want to execute a Python function as a task, you can use the function method of a Job : def preprocess_data ( fast , path ): with open ( path ) as f : data = f . read () if fast : preprocess_fast ( data ) else : preprocess ( data ) job . function ( preprocess_data , args = ( True , \"/data/a.txt\" )) job . function ( preprocess_data , args = ( False , \"/data/b.txt\" )) You can pass both positional and keyword arguments to the function. The arguments will be serialized using cloudpickle . Python tasks can be useful to perform e.g. various data preprocessing and organization tasks. You can co-locate the logic of Python tasks together with the code that defines the submitted workflow (job), without the need to write an additional external script. Same as with the program method, function will return a Task that can used to define dependencies . Notice Currently, a new Python interpreter will be started for each Python task.","title":"Python functions"},{"location":"python/submit/#python-environment","text":"When you use a Python function as a task, the task will attempt to import the hyperqueue package when it executes (to perform some bookkeeping on the background). This function will be executed on a worker - this means that it needs to have access to the correct Python version (and virtual environment) that contains the hyperqueue package! To make sure that the function will be executed in the correct Python environment, you can use PythonEnv and its prologue argument. It lets you specify a (shell) command that will be executed before the Python interpreter that executes your function is spawned. from hyperqueue.task.function import PythonEnv from hyperqueue import Client env = PythonEnv ( prologue = \"ml Python/XYZ && source /<my-path-to-venv>/bin/activate\" ) client = Client ( python_env = env ) If you use Python functions as tasks, it is pretty much required to use PythonEnv , unless your workers are already spawned in an environment that has the correct Python loaded (e.g. using .bashrc or a similar mechanism).","title":"Python environment"},{"location":"python/submit/#parametrizing-tasks","text":"You can parametrize both external or Python tasks by setting their working directory, standard output paths, environment variables or HyperQueue specific parameters like resources or time limits . In contrast to the CLI, where you can only use a single set of parameters for all tasks of a job, with the Python API you can specify these parameters individually for each task. You can find more details in the documentation of the program or function methods.","title":"Parametrizing tasks"},{"location":"python/submit/#submitting-a-job","text":"Once you have added some tasks to the job, you can submit it using the Client 's submit method: client = Client () submitted = client . submit ( job ) To wait until the job has finished executing, use the wait_for_jobs method: client . wait_for_jobs ([ submitted ])","title":"Submitting a job"}]}